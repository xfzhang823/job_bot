{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 years of experience in management consulting, product management and strategy, or analytics in a technology company.\n",
      "Experience working with and analyzing data, and managing multiple cross-functional programs or projects.\n",
      "Experience with performing market analysis and developing competitive intelligence.\n",
      "Ability to manage executive stakeholders and communicate with a highly technical management team.\n",
      "Ability to form and refine hypotheses, gather supporting data, and make recommendations.\n",
      "Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\",\n",
    "    \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\",\n",
    "    \"Experience with performing market analysis and developing competitive intelligence\",\n",
    "    \"Ability to manage executive stakeholders and communicate with a highly technical management team\",\n",
    "    \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\",\n",
    "    \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\",\n",
    "]\n",
    "\n",
    "\n",
    "texts = \".\\n\".join(text for text in texts)\n",
    "\n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    bert_score_precision  soft_similarity  word_movers_distance  \\\n",
      "0               0.833088         0.294945             15.905974   \n",
      "1               0.833400         0.315879             15.811388   \n",
      "2               0.836929         0.377884             15.874508   \n",
      "3               0.845272         0.264874             15.524175   \n",
      "4               0.831945         0.243683             15.716234   \n",
      "5               0.832256         0.294245             15.524175   \n",
      "6               0.830909         0.337065             15.905974   \n",
      "7               0.838344         0.215926             15.099669   \n",
      "8               0.832782         0.277579             15.524175   \n",
      "9               0.856216         0.354124             15.620499   \n",
      "10              0.827047         0.433190             15.459625   \n",
      "11              0.835676         0.253837             15.748016   \n",
      "12              0.846321         0.363054             15.491933   \n",
      "13              0.852224         0.191506             15.524175   \n",
      "14              0.816647         0.157734             15.620499   \n",
      "15              0.869484         0.262018             15.459625   \n",
      "16              0.823150         0.330459             15.716234   \n",
      "17              0.837198         0.360644             15.524175   \n",
      "18              0.820165         0.344930             15.716234   \n",
      "19              0.822814         0.287575             15.556349   \n",
      "20              0.821459         0.282235             15.620499   \n",
      "21              0.829722         0.210947             15.652476   \n",
      "22              0.842820         0.352504             15.748016   \n",
      "23              0.824793         0.309893             15.684387   \n",
      "24              0.865685         0.395633             15.427249   \n",
      "25              0.828627         0.275297             15.524175   \n",
      "26              0.850405         0.346306             15.132746   \n",
      "\n",
      "    nli_entailment_score  jaccard_similarity  \n",
      "0               0.049976            0.042424  \n",
      "1               0.042624            0.036364  \n",
      "2               0.004728            0.058140  \n",
      "3               0.048242            0.043478  \n",
      "4               0.003041            0.037500  \n",
      "5               0.002506            0.036810  \n",
      "6               0.014546            0.080247  \n",
      "7               0.009681            0.061350  \n",
      "8               0.005850            0.057325  \n",
      "9               0.030971            0.038710  \n",
      "10              0.026829            0.056604  \n",
      "11              0.001266            0.037500  \n",
      "12              0.039242            0.052980  \n",
      "13              0.070042            0.026667  \n",
      "14              0.051734            0.032680  \n",
      "15              0.171467            0.026667  \n",
      "16              0.087002            0.019231  \n",
      "17              0.019296            0.026316  \n",
      "18              0.008592            0.019231  \n",
      "19              0.010191            0.032051  \n",
      "20              0.009039            0.048485  \n",
      "21              0.010838            0.025316  \n",
      "22              0.022875            0.049689  \n",
      "23              0.005869            0.025157  \n",
      "24              0.001645            0.038462  \n",
      "25              0.004120            0.025806  \n",
      "26              0.055380            0.045455  \n",
      "MBA or graduate degree in a management, technical, or engineering field Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling 11 years of experience in management consulting, product management and strategy, or analytics in a technology company Experience working with and analyzing data, and managing multiple cross-functional programs or projects Experience with performing market analysis and developing competitive intelligence Ability to manage executive stakeholders and communicate with a highly technical management team Ability to form and refine hypotheses, gather supporting data, and make recommendations Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems English proficiency is a requirement for all roles unless stated otherwise in the job posting Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "\n",
    "f_path = r\"C:\\github\\job_bot\\data\\output_sim_matrix.csv\"\n",
    "df = pd.read_csv(f_path)\n",
    "df[\"Similarity Metrices\"] = df[\"Similarity Metrices\"].apply(ast.literal_eval)\n",
    "\n",
    "metrics_df = pd.json_normalize(df[\"Similarity Metrices\"])\n",
    "print(metrics_df)\n",
    "combined_df = pd.concat([df[[\"Responsibility\"]], metrics_df], axis=1)\n",
    "combined_df[\"Responsibility\"] = combined_df[\"Responsibility\"].str.replace(\"\\n\", \" \")\n",
    "\n",
    "# Ensure full display of DataFrame content\n",
    "pd.set_option(\"display.max_colwidth\", 100)  # Display full column content\n",
    "pd.set_option(\"display.max_rows\", None)  # Display all rows\n",
    "\n",
    "# Export the combined DataFrame to a CSV file\n",
    "# combined_df.to_csv(\"output_sim_matrix_cleaned.csv\", index=False)\n",
    "print(df.Requirements[1])\n",
    "# This will create a CSV file that you can open directly in Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\xzhan\\.cache\\huggingface\\hub\\models--roberta-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "nli_model_name = \"roberta-large-mnli\"\n",
    "\n",
    "# Re-download the model and tokenizer\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    nli_model_name, force_download=True\n",
    ")\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name, force_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2819904685020447\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "premise = \"The candidate should have experience in developing machine learning models using Python.\"\n",
    "\n",
    "hypothesis = (\n",
    "    \"I have developed machine learning models using Python for data analysis projects.\"\n",
    ")\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "nli_score = textsimilarity.nli_entailment_score(premise, hypothesis)\n",
    "\n",
    "print(nli_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Entailment Score: 0.614780604839325\n"
     ]
    }
   ],
   "source": [
    "premise = \"The company has launched a new machine learning model.\"\n",
    "hypothesis = \"The company is working on AI projects.\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "nli_score = textsimilarity.nli_entailment_score(hypothesis, premise)\n",
    "print(f\"NLI Entailment Score: {nli_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Entailment Score: 0.0010548133868724108\n"
     ]
    }
   ],
   "source": [
    "hypothesis = \"The company has launched a new machine learning model.\"\n",
    "premise = \"The company is working on AI projects.\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "nli_score = textsimilarity.nli_entailment_score(hypothesis, premise)\n",
    "print(f\"NLI Entailment Score: {nli_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Entailment Score: 0.11766134947538376\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "premise = \"She graduated with a degree in computer science and has 5 years of experience in software development.\"\n",
    "hypothesis = \"She is prepared for a software engineering role.\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "nli_score = textsimilarity.deberta_entailment_score(hypothesis, premise)\n",
    "print(f\"NLI Entailment Score: {nli_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment Score: 0.0016027865931391716\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "hypothesis = \"Enhanced data quality and consistency by integrating thorough financial analysis, standardizing methodologies, and conducting in-depth vendor engagements.\"\n",
    "premise = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "deberta_score = textsimilarity.deberta_entailment_score(hypothesis, premise)\n",
    "print(f\"Entailment Score: {deberta_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment Score: 0.4864349961280823\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "premise = \"Enhanced data quality and consistency by integrating thorough financial analysis, standardizing methodologies, and conducting in-depth vendor engagements.\"\n",
    "hypothesis = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "deberta_score = textsimilarity.deberta_entailment_score(hypothesis, premise)\n",
    "print(f\"Entailment Score: {deberta_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment Score: 0.025084542110562325\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "premise = \"Collaborated with the engineering services research team to pioneer the engineering services tracker, authored influential publications on market forecasts, the impact of COVID-19 on services, and trends in M&A within the engineering services industry.\"\n",
    "hypothesis = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "deberta_score = textsimilarity.deberta_entailment_score(hypothesis, premise)\n",
    "print(f\"Entailment Score: {deberta_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Categorized Scores:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'bert_score_precision': 'High',\n",
       " 'soft_similarity': 'Medium',\n",
       " 'word_movers_distance': 'High',\n",
       " 'nli_entailment_score': 'Low',\n",
       " 'jaccard_similarity': 'High',\n",
       " 'deberta_entailment_score': 'Medium'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# metrics_evaluator.py\n",
    "\n",
    "# Define the metric criteria in a dictionary\n",
    "METRIC_CRITERIA = {\n",
    "    # \"BERTScore Precision\"\n",
    "    \"bert_score_precision\": {\n",
    "        \"range\": (0, 1),\n",
    "        \"high_threshold\": 0.85,\n",
    "        \"low_threshold\": 0.70,\n",
    "    },\n",
    "    # \"Soft Similarity (SBERT)\"\n",
    "    \"soft_similarity\": {\n",
    "        \"range\": (-1, 1),\n",
    "        \"high_threshold\": 0.7,\n",
    "        \"low_threshold\": 0.4,\n",
    "    },\n",
    "    # \"Word Mover's Distance\"\n",
    "    \"word_movers_distance\": {\n",
    "        \"range\": (0, float(\"inf\")),\n",
    "        \"high_threshold\": 5,  # High score is considered \"Low\" for this metric\n",
    "        \"low_threshold\": 15,  # Low score is considered \"High\" for this metric\n",
    "        \"reverse\": True,  # Indicates smaller scores are better\n",
    "    },\n",
    "    # \"NLI Entailment Score\"\n",
    "    \"nli_entailment_score\": {\n",
    "        \"range\": (0, 1),\n",
    "        \"high_threshold\": 0.7,\n",
    "        \"low_threshold\": 0.3,\n",
    "    },\n",
    "    # \"Jaccard Similarity\"\n",
    "    \"jaccard_similarity\": {\n",
    "        \"range\": (0, 1),\n",
    "        \"high_threshold\": 0.5,\n",
    "        \"low_threshold\": 0.2,\n",
    "    },\n",
    "    # \"DeBERTa Entailment Score\"\n",
    "    \"deberta_entailment_score\": {\n",
    "        \"range\": (0, 1),\n",
    "        \"high_threshold\": 0.8,\n",
    "        \"low_threshold\": 0.4,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def evaluate_score(metric_name, score):\n",
    "    \"\"\"\n",
    "    Evaluate the score for a given metric name.\n",
    "\n",
    "    Args:\n",
    "        metric_name (str): The name of the metric.\n",
    "        score (float): The score to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        str: The category of the score (\"High\", \"Medium\", \"Low\").\n",
    "    \"\"\"\n",
    "    if metric_name not in METRIC_CRITERIA:\n",
    "        raise ValueError(f\"Metric '{metric_name}' is not defined in the criteria.\")\n",
    "\n",
    "    criteria = METRIC_CRITERIA[metric_name]\n",
    "    high_threshold = criteria[\"high_threshold\"]\n",
    "    low_threshold = criteria[\"low_threshold\"]\n",
    "    reverse = criteria.get(\"reverse\", False)\n",
    "\n",
    "    # For metrics where a lower score is better (like Word Mover's Distance)\n",
    "    if reverse:\n",
    "        if score <= high_threshold:\n",
    "            return \"High\"\n",
    "        elif score >= low_threshold:\n",
    "            return \"Low\"\n",
    "    else:\n",
    "        if score >= high_threshold:\n",
    "            return \"High\"\n",
    "        elif score <= low_threshold:\n",
    "            return \"Low\"\n",
    "\n",
    "    return \"Medium\"\n",
    "\n",
    "\n",
    "def categorize_scores(scores):\n",
    "    \"\"\"\n",
    "    Categorize a dictionary of scores based on their metric names.\n",
    "\n",
    "    Args:\n",
    "        scores (dict): A dictionary where keys are metric names and values are scores.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the same keys but with \"High\", \"Medium\", or \"Low\" as values.\n",
    "    \"\"\"\n",
    "    categorized_scores = {}\n",
    "    for metric_name, score in scores.items():\n",
    "        try:\n",
    "            categorized_scores[metric_name] = evaluate_score(metric_name, score)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            categorized_scores[metric_name] = \"Unknown\"\n",
    "    return categorized_scores\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    example_scores = {\n",
    "        \"bert_score_precision\": 0.88,\n",
    "        \"soft_similarity\": 0.45,\n",
    "        \"word_movers_distance\": 4.2,\n",
    "        \"nli_entailment_score\": 0.2,\n",
    "        \"jaccard_similarity\": 0.6,\n",
    "        \"deberta_entailment_score\": 0.75,\n",
    "    }\n",
    "\n",
    "    categorized = categorize_scores(example_scores)\n",
    "    display(\"Categorized Scores:\", categorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing OpenAI / Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, here's one for you:\\n\\nWhy couldn't the bicycle find its way home?\\n\\nBecause it lost its bearings!\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "opeanai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = opeanai_api_key\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"tell me a joke.\"},\n",
    "    ],\n",
    "    temperature=0.2,\n",
    "    max_tokens=200,  # Ensure sufficient tokens for response\n",
    ")\n",
    "\n",
    "response_text = response.choices[0].message.content\n",
    "response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Responsibilities Edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:47:48,608 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:47:50,700 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:47:50,704 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:47:50,706 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': 1, 'optimized_text': 'Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.'}\n",
      "Results updated: \n",
      "{'resp_id': 1, 'optimized_text': 'Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:47:50,709 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': 1, 'optimized_text': 'Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'resp_id': 1,\n",
       " 'optimized_text': 'Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editing import (\n",
    "    edit_text_for_dp,\n",
    "    edit_text_for_semantic_entailment,\n",
    ")\n",
    "\n",
    "resp_text = \"Provided strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\"\n",
    "\n",
    "reqs_text1 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "reqs_text2 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "reqs_text3 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "api_key = get_openai_api_key()\n",
    "client = OpenAI(api_key=api_key)  # Instantiate openai api chat completion class\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "gpt4t = \"gpt-4-turbo\"\n",
    "\n",
    "\n",
    "revised_text = edit_text_for_semantic_entailment(\n",
    "    client=client,\n",
    "    text_id=1,\n",
    "    candidate_text=resp_text,\n",
    "    model_id=gpt4t,\n",
    "    reference_text=reqs_text3,\n",
    ")\n",
    "\n",
    "revised_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 15:35:47,560 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Formatted Prompt in edit_text_for_dp:\n",
      "\n",
      "You are a skilled professional at writing resumes. Please perform the following tasks:\n",
      "\n",
      "1. Analyze the **source text** at a high level.\n",
      "\n",
      "2. Apply the \"\"source text's** dependency structure to the **target text**. Ensure that the original meaning of the **target text** is mostly preserved.\n",
      "\n",
      "**Source Text:**\n",
      "\"Provided strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\"\n",
      "\n",
      "**Target Text:**\n",
      "\"Leveraged strategic insights to optimize the service partner ecosystem in Asia Pacific for a major global IT vendor, enhancing local implementation outcomes.\"\n",
      "\n",
      "**Return Format:**\n",
      "Please return the result in JSON format as follows:\n",
      "\n",
      "{\n",
      "  \"optimized_text\": \"Edited version of the target text\"\n",
      "}\n",
      "\n",
      "Do not include any additional text or explanations.\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 15:35:49,943 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 15:35:49,947 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 15:35:49,949 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': '1', 'optimized_text': 'Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.'}\n",
      "Results updated: \n",
      "{'resp_id': '1', 'optimized_text': 'Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 15:35:49,950 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': '1', 'optimized_text': 'Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "source text:\n",
      "Provided strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\n",
      "\n",
      "target text:\n",
      "Leveraged strategic insights to optimize the service partner ecosystem in Asia Pacific for a major global IT vendor, enhancing local implementation outcomes.\n",
      "\n",
      "Optimized Text Result: \n",
      "Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editing import (\n",
    "    edit_text_for_dp,\n",
    "    edit_text_for_semantic_entailment,\n",
    ")\n",
    "\n",
    "# Sample Source and Target Texts\n",
    "source_text = \"Provided strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\"\n",
    "target_text = \"Leveraged strategic insights to optimize the service partner ecosystem in Asia Pacific for a major global IT vendor, enhancing local implementation outcomes.\"\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "api_key = get_openai_api_key()\n",
    "client = OpenAI(api_key=api_key)  # Instantiate openai api chat completion class\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4\"\n",
    "gpt4t = \"gpt-4-turbo\"\n",
    "\n",
    "# Call the edit_text_for_dp function with the sample texts\n",
    "try:\n",
    "    result = edit_text_for_dp(\n",
    "        client=client,  # Your OpenAI client instance\n",
    "        text_id=\"1\",  # Just an example ID\n",
    "        target_text=target_text,\n",
    "        source_text=source_text,\n",
    "        model_id=\"gpt-4\",  # Example model ID\n",
    "        max_tokens=1056,  # Example max token limit\n",
    "    )\n",
    "    print(f\"\\nsource text:\\n{source_text}\")\n",
    "    print(f\"\\ntarget text:\\n{target_text}\")\n",
    "    print(f\"\\nOptimized Text Result: \\n{result['optimized_text']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during testing: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longer sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just Semantic & Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:11:38,282 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:11:44,505 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:11:44,508 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:11:44,511 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': 'ad95243c-4c69-4bf1-97cb-065b8672f538', 'optimized_text': 'Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.'}\n",
      "Results updated: \n",
      "{'resp_id': 'ad95243c-4c69-4bf1-97cb-065b8672f538', 'optimized_text': 'Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:11:44,512 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': 'ad95243c-4c69-4bf1-97cb-065b8672f538', 'optimized_text': 'Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidate Text:\n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n",
      "\n",
      "Reference Text:\n",
      "Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editing import (\n",
    "    edit_text_for_dp,\n",
    "    edit_text_for_semantic_entailment,\n",
    ")\n",
    "\n",
    "\n",
    "text_1 = \"MBA or graduate degree in a management, technical, or engineering field\"\n",
    "text_2 = \"Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling\"\n",
    "text_3 = \"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\"\n",
    "text_4 = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "text_5 = (\n",
    "    \"Experience with performing market analysis and developing competitive intelligence\"\n",
    ")\n",
    "text_6 = \"Ability to manage executive stakeholders and communicate with a highly technical management team\"\n",
    "text_7 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "text_8 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "text_9 = \"English proficiency is a requirement for all roles unless stated otherwise in the job posting\"\n",
    "text_10 = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "text_11 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "text_12 = \"Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\"\n",
    "\n",
    "original_text = \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
    "\n",
    "\n",
    "candidate = original_text\n",
    "reference = text_12\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "api_key = get_openai_api_key()\n",
    "client = OpenAI(api_key=api_key)  # Instantiate openai api chat completion class\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "\n",
    "revised = edit_text_for_semantic_entailment(\n",
    "    client=client,\n",
    "    candidate_text=candidate,\n",
    "    reference_text=reference,\n",
    "    model_id=gpt4t,\n",
    "    temperature=0.6,\n",
    ")\n",
    "\n",
    "revised_text = revised[\"optimized_text\"]\n",
    "\n",
    "print(f\"\\nCandidate Text:\\n{candidate}\")\n",
    "print(f\"\\nReference Text:\\n{reference}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:03,008 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:07,445 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:07,448 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:07,451 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': '5d15e5aa-5394-4ec8-9610-242f1f147be9', 'optimized_text': 'Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.'}\n",
      "Results updated: \n",
      "{'resp_id': '5d15e5aa-5394-4ec8-9610-242f1f147be9', 'optimized_text': 'Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:07,452 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': '5d15e5aa-5394-4ec8-9610-242f1f147be9', 'optimized_text': 'Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Formatted Prompt in edit_text_for_dp:\n",
      "\n",
      "You are a skilled professional at writing resumes. Please perform the following tasks:\n",
      "\n",
      "1. Analyze the **source text** at a high level.\n",
      "\n",
      "2. Apply the \"\"source text's** dependency structure to the **target text**. Ensure that the original meaning of the **target text** is mostly preserved.\n",
      "\n",
      "**Source Text:**\n",
      "\"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
      "\n",
      "**Target Text:**\n",
      "\"Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "\n",
      "**Return Format:**\n",
      "Please return the result in JSON format as follows:\n",
      "\n",
      "{\n",
      "  \"optimized_text\": \"Edited version of the target text\"\n",
      "}\n",
      "\n",
      "Do not include any additional text or explanations.\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:11,194 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:11,196 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:11,199 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': '8e0249d3-37f4-4818-aa8d-97751838b6f3', 'optimized_text': 'Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.'}\n",
      "Results updated: \n",
      "{'resp_id': '8e0249d3-37f4-4818-aa8d-97751838b6f3', 'optimized_text': 'Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:11,200 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': '8e0249d3-37f4-4818-aa8d-97751838b6f3', 'optimized_text': 'Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidate Text:\n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n",
      "\n",
      "Reference Text:\n",
      "Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editing import (\n",
    "    edit_text_for_dp,\n",
    "    edit_text_for_semantic_entailment,\n",
    ")\n",
    "\n",
    "# Texts\n",
    "text_1 = \"MBA or graduate degree in a management, technical, or engineering field\"\n",
    "text_2 = \"Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling\"\n",
    "text_3 = \"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\"\n",
    "text_4 = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "text_5 = (\n",
    "    \"Experience with performing market analysis and developing competitive intelligence\"\n",
    ")\n",
    "text_6 = \"Ability to manage executive stakeholders and communicate with a highly technical management team\"\n",
    "text_7 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "text_8 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "text_9 = \"English proficiency is a requirement for all roles unless stated otherwise in the job posting\"\n",
    "text_10 = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "text_11 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "text_12 = \"Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\"\n",
    "\n",
    "original_text = \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "api_key = get_openai_api_key()\n",
    "client = OpenAI(api_key=api_key)  # Instantiate openai api chat completion class\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "\n",
    "\n",
    "# Step 1: Align Semantic & Entailment\n",
    "candidate = original_text\n",
    "reference = text_12\n",
    "\n",
    "revised = edit_text_for_semantic_entailment(\n",
    "    client=client,\n",
    "    candidate_text=candidate,\n",
    "    reference_text=reference,\n",
    "    model_id=gpt4t,\n",
    "    temperature=0.6,\n",
    ")\n",
    "\n",
    "revised_text = revised[\"optimized_text\"]\n",
    "\n",
    "# Step 2: Align Original Sentence's DP\n",
    "target = revised_text\n",
    "source = original_text\n",
    "\n",
    "final = edit_text_for_dp(\n",
    "    client=client,\n",
    "    target_text=target,\n",
    "    source_text=source,\n",
    "    model_id=gpt4t,\n",
    "    temperature=0.8,\n",
    ")\n",
    "\n",
    "final_text = final[\"optimized_text\"]\n",
    "\n",
    "\n",
    "# Print\n",
    "print(f\"\\nCandidate Text:\\n{candidate}\")\n",
    "print(f\"\\nReference Text:\\n{reference}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{final_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Dependency Parsing Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:02,631 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:05,966 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:05,968 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:05,971 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': '4e77b7d4-2e89-418d-9bd1-725b2fb51f3f', 'optimized_text': 'Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.'}\n",
      "Results updated: \n",
      "{'resp_id': '4e77b7d4-2e89-418d-9bd1-725b2fb51f3f', 'optimized_text': 'Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:05,972 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': '4e77b7d4-2e89-418d-9bd1-725b2fb51f3f', 'optimized_text': 'Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Formatted Prompt in edit_text_for_dp:\n",
      "\n",
      "You are a skilled professional at writing resumes. Please perform the following tasks:\n",
      "\n",
      "1. Analyze the **source text** at a high level.\n",
      "\n",
      "2. Apply the \"\"source text's** dependency structure to the **target text**. Ensure that the original meaning of the **target text** is mostly preserved.\n",
      "\n",
      "**Source Text:**\n",
      "\"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
      "\n",
      "**Target Text:**\n",
      "\"Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.\"\n",
      "\n",
      "**Return Format:**\n",
      "Please return the result in JSON format as follows:\n",
      "\n",
      "{\n",
      "  \"optimized_text\": \"Edited version of the target text\"\n",
      "}\n",
      "\n",
      "Do not include any additional text or explanations.\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:08,998 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:09,000 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:09,002 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': '5bce9c3d-ff64-44b6-9ddd-52c2a8769499', 'optimized_text': 'Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.'}\n",
      "Results updated: \n",
      "{'resp_id': '5bce9c3d-ff64-44b6-9ddd-52c2a8769499', 'optimized_text': 'Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:09,003 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': '5bce9c3d-ff64-44b6-9ddd-52c2a8769499', 'optimized_text': 'Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidate Text:\n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n",
      "\n",
      "Reference Text:\n",
      "Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editing import (\n",
    "    edit_text_for_semantic_entailment_llama3,\n",
    ")\n",
    "\n",
    "# Texts\n",
    "text_1 = \"MBA or graduate degree in a management, technical, or engineering field\"\n",
    "text_2 = \"Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling\"\n",
    "text_3 = \"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\"\n",
    "text_4 = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "text_5 = (\n",
    "    \"Experience with performing market analysis and developing competitive intelligence\"\n",
    ")\n",
    "text_6 = \"Ability to manage executive stakeholders and communicate with a highly technical management team\"\n",
    "text_7 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "text_8 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "text_9 = \"English proficiency is a requirement for all roles unless stated otherwise in the job posting\"\n",
    "text_10 = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "text_11 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "text_12 = \"Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\"\n",
    "\n",
    "original_text = \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "api_key = get_openai_api_key()\n",
    "client = OpenAI(api_key=api_key)  # Instantiate openai api chat completion class\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "\n",
    "\n",
    "# Step 1: Align Semantic & Entailment\n",
    "candidate = original_text\n",
    "reference = text_12\n",
    "\n",
    "revised = edit_text_for_semantic_entailment(\n",
    "    client=client,\n",
    "    candidate_text=candidate,\n",
    "    reference_text=reference,\n",
    "    model_id=gpt4t,\n",
    "    temperature=0.6,\n",
    ")\n",
    "\n",
    "revised_text = revised[\"optimized_text\"]\n",
    "\n",
    "# Step 2: Align Original Sentence's DP\n",
    "target = revised_text\n",
    "source = original_text\n",
    "\n",
    "final = edit_text_for_dp(\n",
    "    client=client,\n",
    "    target_text=target,\n",
    "    source_text=source,\n",
    "    model_id=gpt4t,\n",
    "    temperature=0.8,\n",
    ")\n",
    "\n",
    "final_text = final[\"optimized_text\"]\n",
    "\n",
    "\n",
    "# Print\n",
    "print(f\"\\nCandidate Text:\\n{candidate}\")\n",
    "print(f\"\\nReference Text:\\n{reference}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{final_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:30,270 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:33,487 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:33,490 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:33,492 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': '035476c0-63f1-4357-86c8-200f9aec746d', 'optimized_text': 'Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.'}\n",
      "Results updated: \n",
      "{'text_id': '035476c0-63f1-4357-86c8-200f9aec746d', 'optimized_text': 'Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:33,495 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': '035476c0-63f1-4357-86c8-200f9aec746d', 'optimized_text': 'Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:36,996 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:36,998 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:37,002 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': '70fa76ac-430b-4cd2-bfde-5ebb770d900f', 'optimized_text': 'Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.'}\n",
      "Results updated: \n",
      "{'text_id': '70fa76ac-430b-4cd2-bfde-5ebb770d900f', 'optimized_text': 'Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:37,003 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': '70fa76ac-430b-4cd2-bfde-5ebb770d900f', 'optimized_text': 'Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:40,340 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:40,343 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:40,346 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': 'a4ef3843-19d5-4667-9869-4d5c3850a206', 'optimized_text': 'Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.'}\n",
      "Results updated: \n",
      "{'text_id': 'a4ef3843-19d5-4667-9869-4d5c3850a206', 'optimized_text': 'Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:40,347 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': 'a4ef3843-19d5-4667-9869-4d5c3850a206', 'optimized_text': 'Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text:\n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n",
      "\n",
      "Compared to Text:\n",
      "Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n",
      "\n",
      "Optimized Text Result: \n",
      "Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.\n",
      "\n",
      "Optimized Text Result: \n",
      "Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editor import TextEditor\n",
    "\n",
    "# Texts\n",
    "text_1 = \"MBA or graduate degree in a management, technical, or engineering field\"\n",
    "text_2 = \"Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling\"\n",
    "text_3 = \"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\"\n",
    "text_4 = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "text_5 = (\n",
    "    \"Experience with performing market analysis and developing competitive intelligence\"\n",
    ")\n",
    "text_6 = \"Ability to manage executive stakeholders and communicate with a highly technical management team\"\n",
    "text_7 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "text_8 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "text_9 = \"English proficiency is a requirement for all roles unless stated otherwise in the job posting\"\n",
    "text_10 = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "text_11 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "text_12 = \"Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\"\n",
    "\n",
    "original = \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
    "requirement = text_12\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "\n",
    "# Instantiate TextEditor class\n",
    "text_editor = TextEditor(model=\"openai\", model_id=gpt4, max_tokens=512)\n",
    "\n",
    "\n",
    "# Step 1: Align Semantic\n",
    "candidate = original\n",
    "reference = requirement\n",
    "\n",
    "revised = text_editor.edit_for_semantics(\n",
    "    candidate_text=candidate, reference_text=reference, temperature=0.4\n",
    ")\n",
    "\n",
    "revised_text_1 = revised[\"optimized_text\"]\n",
    "\n",
    "# Step 2: Allign Entailment\n",
    "premise = revised_text_1\n",
    "hypothesis = requirement\n",
    "\n",
    "revised = text_editor.edit_for_entailment(\n",
    "    premise_text=premise, hypothesis_text=hypothesis, temperature=0.6\n",
    ")\n",
    "\n",
    "revised_text_2 = revised[\"optimized_text\"]\n",
    "\n",
    "# Step 3: Align Original Sentence's DP\n",
    "target = revised_text_2\n",
    "source = original\n",
    "\n",
    "revised = text_editor.edit_for_dp(\n",
    "    target_text=target, source_text=source, temperature=0.8\n",
    ")\n",
    "\n",
    "revised_text_3 = revised[\"optimized_text\"]\n",
    "\n",
    "\n",
    "# Print\n",
    "print(f\"\\nOriginal Text:\\n{original}\")\n",
    "print(f\"\\nCompared to Text:\\n{requirement}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_1}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_2}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:27,773 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:29,101 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:29,104 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:29,105 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': '0d79593a-af95-4a68-99f6-2d309ae3cc2f', 'optimized_text': 'Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.'}\n",
      "Results updated: \n",
      "{'text_id': '0d79593a-af95-4a68-99f6-2d309ae3cc2f', 'optimized_text': 'Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:29,106 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': '0d79593a-af95-4a68-99f6-2d309ae3cc2f', 'optimized_text': 'Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:30,206 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:30,209 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:30,212 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': '5c78fdb4-854a-40d2-bc17-f62213610763', 'optimized_text': 'Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.'}\n",
      "Results updated: \n",
      "{'text_id': '5c78fdb4-854a-40d2-bc17-f62213610763', 'optimized_text': 'Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:30,213 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': '5c78fdb4-854a-40d2-bc17-f62213610763', 'optimized_text': 'Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:31,466 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:31,469 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:31,471 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': 'ebaba7f3-a403-4566-8ac7-f1f4dc4e31c6', 'optimized_text': 'Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.'}\n",
      "Results updated: \n",
      "{'text_id': 'ebaba7f3-a403-4566-8ac7-f1f4dc4e31c6', 'optimized_text': 'Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:31,473 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': 'ebaba7f3-a403-4566-8ac7-f1f4dc4e31c6', 'optimized_text': 'Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text:\n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n",
      "\n",
      "Compared to Text:\n",
      "Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n",
      "\n",
      "Optimized Text Result: \n",
      "Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.\n",
      "\n",
      "Optimized Text Result: \n",
      "Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editor import TextEditor\n",
    "\n",
    "# Texts\n",
    "text_1 = \"MBA or graduate degree in a management, technical, or engineering field\"\n",
    "text_2 = \"Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling\"\n",
    "text_3 = \"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\"\n",
    "text_4 = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "text_5 = (\n",
    "    \"Experience with performing market analysis and developing competitive intelligence\"\n",
    ")\n",
    "text_6 = \"Ability to manage executive stakeholders and communicate with a highly technical management team\"\n",
    "text_7 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "text_8 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "text_9 = \"English proficiency is a requirement for all roles unless stated otherwise in the job posting\"\n",
    "text_10 = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "text_11 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "text_12 = \"Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\"\n",
    "\n",
    "original = \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
    "requirement = text_12\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "\n",
    "# Instantiate TextEditor class\n",
    "text_editor = TextEditor(model=\"openai\", model_id=gpt3, max_tokens=512)\n",
    "\n",
    "\n",
    "# Step 1: Align Semantic\n",
    "candidate = original\n",
    "reference = requirement\n",
    "\n",
    "revised = text_editor.edit_for_semantics(\n",
    "    candidate_text=candidate, reference_text=reference, temperature=0.5\n",
    ")\n",
    "\n",
    "revised_text_1 = revised[\"optimized_text\"]\n",
    "\n",
    "# Step 2: Allign Entailment\n",
    "premise = revised_text_1\n",
    "hypothesis = requirement\n",
    "\n",
    "revised = text_editor.edit_for_entailment(\n",
    "    premise_text=premise, hypothesis_text=hypothesis, temperature=0.6\n",
    ")\n",
    "\n",
    "revised_text_2 = revised[\"optimized_text\"]\n",
    "\n",
    "# Step 3: Align Original Sentence's DP\n",
    "target = revised_text_2\n",
    "source = original\n",
    "\n",
    "revised = text_editor.edit_for_dp(\n",
    "    target_text=target, source_text=source, temperature=0.9\n",
    ")\n",
    "\n",
    "revised_text_3 = revised[\"optimized_text\"]\n",
    "\n",
    "\n",
    "# Print\n",
    "print(f\"\\nOriginal Text:\\n{original}\")\n",
    "print(f\"\\nCompared to Text:\\n{requirement}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_1}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_2}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:00,148 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:19,460 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:19,462 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': 'cfeae29e-475e-42d0-92ca-8817b5869f3f', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), and industry trends. Developed actionable recommendations through reports, blogs, presentations, and custom studies.'}\n",
      "Results updated: \n",
      "{'text_id': 'cfeae29e-475e-42d0-92ca-8817b5869f3f', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), and industry trends. Developed actionable recommendations through reports, blogs, presentations, and custom studies.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:19,463 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': 'cfeae29e-475e-42d0-92ca-8817b5869f3f', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), and industry trends. Developed actionable recommendations through reports, blogs, presentations, and custom studies.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:35,382 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:35,383 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': '50978914-33be-4817-98c4-c06fae43c0eb', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies.'}\n",
      "Results updated: \n",
      "{'text_id': '50978914-33be-4817-98c4-c06fae43c0eb', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:35,385 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': '50978914-33be-4817-98c4-c06fae43c0eb', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:59,118 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:59,120 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': '0cd881f0-9b4d-4925-ac27-fdf9cd19550a', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies, authored in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.'}\n",
      "Results updated: \n",
      "{'text_id': '0cd881f0-9b4d-4925-ac27-fdf9cd19550a', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies, authored in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:59,121 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': '0cd881f0-9b4d-4925-ac27-fdf9cd19550a', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies, authored in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text:\n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n",
      "\n",
      "Compared to Text:\n",
      "Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n",
      "\n",
      "Optimized Text Result: \n",
      "Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), and industry trends. Developed actionable recommendations through reports, blogs, presentations, and custom studies.\n",
      "\n",
      "Optimized Text Result: \n",
      "Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies.\n",
      "\n",
      "Optimized Text Result: \n",
      "Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies, authored in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editor import TextEditor\n",
    "\n",
    "# Texts\n",
    "text_1 = \"MBA or graduate degree in a management, technical, or engineering field\"\n",
    "text_2 = \"Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling\"\n",
    "text_3 = \"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\"\n",
    "text_4 = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "text_5 = (\n",
    "    \"Experience with performing market analysis and developing competitive intelligence\"\n",
    ")\n",
    "text_6 = \"Ability to manage executive stakeholders and communicate with a highly technical management team\"\n",
    "text_7 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "text_8 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "text_9 = \"English proficiency is a requirement for all roles unless stated otherwise in the job posting\"\n",
    "text_10 = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "text_11 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "text_12 = \"Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\"\n",
    "\n",
    "original = \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
    "requirement = text_12\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "\n",
    "# Instantiate TextEditor class\n",
    "text_editor = TextEditor(model=\"llama3\")\n",
    "\n",
    "\n",
    "# Step 1: Align Semantic\n",
    "candidate = original\n",
    "reference = requirement\n",
    "\n",
    "revised = text_editor.edit_for_semantics(\n",
    "    candidate_text=candidate, reference_text=reference, temperature=0.8\n",
    ")\n",
    "\n",
    "revised_text_1 = revised[\"optimized_text\"]\n",
    "\n",
    "# Step 2: Allign Entailment\n",
    "premise = revised_text_1\n",
    "hypothesis = requirement\n",
    "\n",
    "revised = text_editor.edit_for_entailment(\n",
    "    premise_text=premise, hypothesis_text=hypothesis, temperature=0.6\n",
    ")\n",
    "\n",
    "revised_text_2 = revised[\"optimized_text\"]\n",
    "\n",
    "# Step 3: Align Original Sentence's DP\n",
    "target = revised_text_2\n",
    "source = original\n",
    "\n",
    "revised = text_editor.edit_for_dp(\n",
    "    target_text=target, source_text=source, temperature=0.9\n",
    ")\n",
    "\n",
    "revised_text_3 = revised[\"optimized_text\"]\n",
    "\n",
    "\n",
    "# Print\n",
    "print(f\"\\nOriginal Text:\\n{original}\")\n",
    "print(f\"\\nCompared to Text:\\n{requirement}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_1}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_2}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEb Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     39\u001b[0m urls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://jobs.careers.microsoft.com/us/en/job/1771714/Head-of-Partner-Intelligence-and-Strategy?jobsource=linkedin\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 40\u001b[0m page_contents \u001b[38;5;241m=\u001b[39m \u001b[43mload_text_from_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43murls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(page_contents)\n",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m, in \u001b[0;36mload_text_from_pages\u001b[1;34m(urls)\u001b[0m\n\u001b[0;32m      5\u001b[0m content_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Start Playwright and launch the browser\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msync_playwright\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchromium\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheadless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Launch headless browser\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Create a new browser page\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\sync_api\\_context_manager.py:47\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_own_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m---> 47\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Error(\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"It looks like you are using Playwright Sync API inside the asyncio loop.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03mPlease use the Async API instead.\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m             )\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;66;03m# Create a new fiber for the protocol dispatcher. It will be pumping events\u001b[39;00m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;66;03m# until the end of times. We will pass control to that fiber every time we\u001b[39;00m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m# block while waiting for a response.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgreenlet_main\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mError\u001b[0m: It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead."
     ]
    }
   ],
   "source": [
    "from playwright.sync_api import sync_playwright\n",
    "\n",
    "\n",
    "def load_text_from_pages(urls):\n",
    "\n",
    "    # Initialize an empty dictionary to store the content\n",
    "\n",
    "    content_dict = {}\n",
    "\n",
    "    # Start Playwright and launch the browser\n",
    "\n",
    "    with sync_playwright() as p:\n",
    "\n",
    "        browser = p.chromium.launch(headless=True)  # Launch headless browser\n",
    "\n",
    "        page = browser.new_page()  # Create a new browser page\n",
    "\n",
    "        # Loop through the list of URLs\n",
    "\n",
    "        for url in urls:\n",
    "\n",
    "            try:\n",
    "\n",
    "                # Navigate to the URL\n",
    "\n",
    "                page.goto(url)\n",
    "\n",
    "                # Wait for the page to fully load\n",
    "\n",
    "                page.wait_for_load_state(\"networkidle\")\n",
    "\n",
    "                # Get the text content of the entire page\n",
    "\n",
    "                content = page.inner_text(\"body\")  # Extract text from <body>\n",
    "\n",
    "                # Store the content in the dictionary with the URL as the key\n",
    "\n",
    "                content_dict[url] = content\n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "                # Handle errors (e.g., failed to load page)\n",
    "\n",
    "                print(f\"Failed to load {url}: {e}\")\n",
    "\n",
    "                content_dict[url] = None  # Store None for failed pages\n",
    "\n",
    "        # Close the browser after processing all URLs\n",
    "\n",
    "        browser.close()\n",
    "\n",
    "    # Return the dictionary with URL and content\n",
    "    return content_dict\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "\n",
    "urls = [\n",
    "    \"https://jobs.careers.microsoft.com/us/en/job/1771714/Head-of-Partner-Intelligence-and-Strategy?jobsource=linkedin\"\n",
    "]\n",
    "\n",
    "\n",
    "page_contents = load_text_from_pages(urls)\n",
    "\n",
    "\n",
    "print(page_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Install Playwright using pip: pip install playwright\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Run the following command to install the browsers: playwright install\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplaywright\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msync_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sync_playwright\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msync_playwright\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Choose the browser (chromium, firefox, or webkit)\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchromium\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheadless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Create a new browser context\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\sync_api\\_context_manager.py:47\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_own_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m---> 47\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Error(\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"It looks like you are using Playwright Sync API inside the asyncio loop.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03mPlease use the Async API instead.\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m             )\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;66;03m# Create a new fiber for the protocol dispatcher. It will be pumping events\u001b[39;00m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;66;03m# until the end of times. We will pass control to that fiber every time we\u001b[39;00m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m# block while waiting for a response.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgreenlet_main\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mError\u001b[0m: It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead."
     ]
    }
   ],
   "source": [
    "# Install Playwright using pip: pip install playwright\n",
    "# Run the following command to install the browsers: playwright install\n",
    "\n",
    "from playwright.sync_api import sync_playwright\n",
    "\n",
    "with sync_playwright() as p:\n",
    "    # Choose the browser (chromium, firefox, or webkit)\n",
    "    browser = p.chromium.launch(headless=False)\n",
    "\n",
    "    # Create a new browser context\n",
    "    context = browser.new_context()\n",
    "\n",
    "    # Open a new page\n",
    "    page = context.new_page()\n",
    "\n",
    "    # Navigate to a webpage\n",
    "    urls = \"https://jobs.careers.microsoft.com/us/en/job/1771714/Head-of-Partner-Intelligence-and-Strategy?jobsource=linkedin\"\n",
    "    page.goto(urls)\n",
    "\n",
    "    # Extract the page title\n",
    "    title = page.title()\n",
    "    print(title)\n",
    "\n",
    "    # Close the browser\n",
    "    browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 14:09:26,927 - asyncio - ERROR - Task exception was never retrieved\n",
      "future: <Task finished name='Task-2' coro=<Connection.run() done, defined at c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_connection.py:265> exception=NotImplementedError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_connection.py\", line 272, in run\n",
      "    await self._transport.connect()\n",
      "  File \"c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 133, in connect\n",
      "    raise exc\n",
      "  File \"c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 120, in connect\n",
      "    self._proc = await asyncio.create_subprocess_exec(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\subprocess.py\", line 223, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1708, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 503, in _make_subprocess_transport\n",
      "    raise NotImplementedError\n",
      "NotImplementedError\n",
      "2024-09-30 14:09:26,931 - asyncio - ERROR - Task exception was never retrieved\n",
      "future: <Task finished name='Task-4' coro=<Connection.run() done, defined at c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_connection.py:265> exception=NotImplementedError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Temp\\ipykernel_20304\\1154074729.py\", line 91, in <module>\n",
      "    asyncio.get_running_loop().run_until_complete(main())\n",
      "  File \"c:\\github\\job_bot\\env\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Temp\\ipykernel_20304\\1154074729.py\", line 85, in main\n",
      "    content_dict, failed_urls = await load_webpages_with_playwright(urls)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Temp\\ipykernel_20304\\1154074729.py\", line 50, in load_webpages_with_playwright\n",
      "    async with async_playwright() as p:\n",
      "  File \"c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py\", line 46, in __aenter__\n",
      "    playwright = AsyncPlaywright(next(iter(done)).result())\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 120, in connect\n",
      "    self._proc = await asyncio.create_subprocess_exec(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\subprocess.py\", line 223, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1708, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 503, in _make_subprocess_transport\n",
      "    raise NotImplementedError\n",
      "NotImplementedError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_connection.py\", line 272, in run\n",
      "    await self._transport.connect()\n",
      "  File \"c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 133, in connect\n",
      "    raise exc\n",
      "  File \"c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 120, in connect\n",
      "    self._proc = await asyncio.create_subprocess_exec(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\subprocess.py\", line 223, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1708, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 503, in _make_subprocess_transport\n",
      "    raise NotImplementedError\n",
      "NotImplementedError\n",
      "C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:50: RuntimeWarning: coroutine 'load_webpages_with_playwright' was never awaited\n",
      "  return compile(source, filename, mode, flags,\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 91\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_running_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:  \u001b[38;5;66;03m# In case no loop is running (standard Python env)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[8], line 85\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m urls \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.metacareers.com/jobs/522232286825036/?rx_campaign=Linkedin1&rx_ch=connector&rx_group=126320&rx_job=a1KDp00000E28eGMAR&rx_medium=post&rx_r=none&rx_source=Linkedin&rx_ts=20240927T121201Z&rx_vp=slots&utm_campaign=Job\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m2Bboard&utm_medium=jobs&utm_source=LIpaid&rx_viewer=e3efacca649311ef917d17a1705b89ba0dc4e1e7a57f4231bbce94a604c83931\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m ]\n\u001b[1;32m---> 85\u001b[0m content_dict, failed_urls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m load_webpages_with_playwright(urls)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(content_dict, failed_urls)\n",
      "Cell \u001b[1;32mIn[8], line 50\u001b[0m, in \u001b[0;36mload_webpages_with_playwright\u001b[1;34m(urls)\u001b[0m\n\u001b[0;32m     48\u001b[0m failed_urls \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m async_playwright() \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m     51\u001b[0m     browser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m p\u001b[38;5;241m.\u001b[39mchromium\u001b[38;5;241m.\u001b[39mlaunch(headless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Headless False for debugging\u001b[39;00m\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:46\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__aenter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m     playwright_future\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m---> 46\u001b[0m playwright \u001b[38;5;241m=\u001b[39m AsyncPlaywright(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     47\u001b[0m playwright\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aexit__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_transport.py:120\u001b[0m, in \u001b[0;36mPipeTransport.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m     executable_path, entrypoint_path \u001b[38;5;241m=\u001b[39m compute_driver_executable()\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_subprocess_exec(\n\u001b[0;32m    121\u001b[0m         executable_path,\n\u001b[0;32m    122\u001b[0m         entrypoint_path,\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-driver\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    124\u001b[0m         stdin\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    125\u001b[0m         stdout\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    126\u001b[0m         stderr\u001b[38;5;241m=\u001b[39m_get_stderr_fileno(),\n\u001b[0;32m    127\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32768\u001b[39m,\n\u001b[0;32m    128\u001b[0m         env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m    129\u001b[0m         startupinfo\u001b[38;5;241m=\u001b[39mstartupinfo,\n\u001b[0;32m    130\u001b[0m     )\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\subprocess.py:223\u001b[0m, in \u001b[0;36mcreate_subprocess_exec\u001b[1;34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[0m\n\u001b[0;32m    221\u001b[0m protocol_factory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: SubprocessStreamProtocol(limit\u001b[38;5;241m=\u001b[39mlimit,\n\u001b[0;32m    222\u001b[0m                                                     loop\u001b[38;5;241m=\u001b[39mloop)\n\u001b[1;32m--> 223\u001b[0m transport, protocol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m loop\u001b[38;5;241m.\u001b[39msubprocess_exec(\n\u001b[0;32m    224\u001b[0m     protocol_factory,\n\u001b[0;32m    225\u001b[0m     program, \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    226\u001b[0m     stdin\u001b[38;5;241m=\u001b[39mstdin, stdout\u001b[38;5;241m=\u001b[39mstdout,\n\u001b[0;32m    227\u001b[0m     stderr\u001b[38;5;241m=\u001b[39mstderr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Process(transport, protocol, loop)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py:1708\u001b[0m, in \u001b[0;36mBaseEventLoop.subprocess_exec\u001b[1;34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_subprocess(debug_log, stdin, stdout, stderr)\n\u001b[1;32m-> 1708\u001b[0m transport \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_subprocess_transport(\n\u001b[0;32m   1709\u001b[0m     protocol, popen_args, \u001b[38;5;28;01mFalse\u001b[39;00m, stdin, stdout, stderr,\n\u001b[0;32m   1710\u001b[0m     bufsize, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debug \u001b[38;5;129;01mand\u001b[39;00m debug_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py:503\u001b[0m, in \u001b[0;36mBaseEventLoop._make_subprocess_transport\u001b[1;34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create subprocess transport.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m     asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_until_complete(main())\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:  \u001b[38;5;66;03m# In case no loop is running (standard Python env)\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[1;32mIn[8], line 85\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m     82\u001b[0m     urls \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.metacareers.com/jobs/522232286825036/?rx_campaign=Linkedin1&rx_ch=connector&rx_group=126320&rx_job=a1KDp00000E28eGMAR&rx_medium=post&rx_r=none&rx_source=Linkedin&rx_ts=20240927T121201Z&rx_vp=slots&utm_campaign=Job\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m2Bboard&utm_medium=jobs&utm_source=LIpaid&rx_viewer=e3efacca649311ef917d17a1705b89ba0dc4e1e7a57f4231bbce94a604c83931\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m     ]\n\u001b[1;32m---> 85\u001b[0m     content_dict, failed_urls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m load_webpages_with_playwright(urls)\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28mprint\u001b[39m(content_dict, failed_urls)\n",
      "Cell \u001b[1;32mIn[8], line 50\u001b[0m, in \u001b[0;36mload_webpages_with_playwright\u001b[1;34m(urls)\u001b[0m\n\u001b[0;32m     47\u001b[0m content_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     48\u001b[0m failed_urls \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m async_playwright() \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m     51\u001b[0m     browser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m p\u001b[38;5;241m.\u001b[39mchromium\u001b[38;5;241m.\u001b[39mlaunch(headless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Headless False for debugging\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m browser\u001b[38;5;241m.\u001b[39mnew_page()\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:46\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__aenter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m playwright_future\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     45\u001b[0m     playwright_future\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m---> 46\u001b[0m playwright \u001b[38;5;241m=\u001b[39m AsyncPlaywright(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     47\u001b[0m playwright\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aexit__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m playwright\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_transport.py:120\u001b[0m, in \u001b[0;36mPipeTransport.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m         startupinfo\u001b[38;5;241m.\u001b[39mwShowWindow \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mSW_HIDE\n\u001b[0;32m    119\u001b[0m     executable_path, entrypoint_path \u001b[38;5;241m=\u001b[39m compute_driver_executable()\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_subprocess_exec(\n\u001b[0;32m    121\u001b[0m         executable_path,\n\u001b[0;32m    122\u001b[0m         entrypoint_path,\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-driver\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    124\u001b[0m         stdin\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    125\u001b[0m         stdout\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    126\u001b[0m         stderr\u001b[38;5;241m=\u001b[39m_get_stderr_fileno(),\n\u001b[0;32m    127\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32768\u001b[39m,\n\u001b[0;32m    128\u001b[0m         env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m    129\u001b[0m         startupinfo\u001b[38;5;241m=\u001b[39mstartupinfo,\n\u001b[0;32m    130\u001b[0m     )\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_error_future\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\subprocess.py:223\u001b[0m, in \u001b[0;36mcreate_subprocess_exec\u001b[1;34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m loop \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[0;32m    221\u001b[0m protocol_factory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: SubprocessStreamProtocol(limit\u001b[38;5;241m=\u001b[39mlimit,\n\u001b[0;32m    222\u001b[0m                                                     loop\u001b[38;5;241m=\u001b[39mloop)\n\u001b[1;32m--> 223\u001b[0m transport, protocol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m loop\u001b[38;5;241m.\u001b[39msubprocess_exec(\n\u001b[0;32m    224\u001b[0m     protocol_factory,\n\u001b[0;32m    225\u001b[0m     program, \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    226\u001b[0m     stdin\u001b[38;5;241m=\u001b[39mstdin, stdout\u001b[38;5;241m=\u001b[39mstdout,\n\u001b[0;32m    227\u001b[0m     stderr\u001b[38;5;241m=\u001b[39mstderr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Process(transport, protocol, loop)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py:1708\u001b[0m, in \u001b[0;36mBaseEventLoop.subprocess_exec\u001b[1;34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1706\u001b[0m     debug_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecute program \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogram\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_subprocess(debug_log, stdin, stdout, stderr)\n\u001b[1;32m-> 1708\u001b[0m transport \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_subprocess_transport(\n\u001b[0;32m   1709\u001b[0m     protocol, popen_args, \u001b[38;5;28;01mFalse\u001b[39;00m, stdin, stdout, stderr,\n\u001b[0;32m   1710\u001b[0m     bufsize, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debug \u001b[38;5;129;01mand\u001b[39;00m debug_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1712\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, debug_log, transport)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py:503\u001b[0m, in \u001b[0;36mBaseEventLoop._make_subprocess_transport\u001b[1;34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_subprocess_transport\u001b[39m(\u001b[38;5;28mself\u001b[39m, protocol, args, shell,\n\u001b[0;32m    500\u001b[0m                                      stdin, stdout, stderr, bufsize,\n\u001b[0;32m    501\u001b[0m                                      extra\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    502\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create subprocess transport.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "import logging\n",
    "import logging_config\n",
    "import re\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Allow nested event loops (needed for Jupyter/IPython environments)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "def clean_webpage_text(content):\n",
    "    \"\"\"\n",
    "    Clean the extracted text by removing JavaScript, URLs, scripts, and excessive whitespace.\n",
    "\n",
    "    This function performs the following cleaning steps:\n",
    "    - Removes JavaScript function calls.\n",
    "    - Removes URLs (e.g., tracking or other unwanted URLs).\n",
    "    - Removes script tags and their content.\n",
    "    - Replaces multiple spaces or newline characters with a single space or newline.\n",
    "    - Strips leading and trailing whitespace.\n",
    "\n",
    "    Args:\n",
    "        content (str): The text content to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned and processed text.\n",
    "    \"\"\"\n",
    "    # Remove JavaScript function calls (e.g., requireLazy([...]))\n",
    "    content = re.sub(r\"requireLazy\\([^)]+\\)\", \"\", content)\n",
    "\n",
    "    # Remove URLs (e.g., http, https)\n",
    "    content = re.sub(r\"https?:\\/\\/\\S+\", \"\", content)\n",
    "\n",
    "    # Remove <script> tags and their contents\n",
    "    content = re.sub(r\"<script[^>]*>[\\s\\S]*?</script>\", \"\", content)\n",
    "\n",
    "    # Remove excessive whitespace (more than one space)\n",
    "    content = re.sub(r\"\\s+\", \" \", content).strip()\n",
    "\n",
    "    # Replace double newlines with single newlines\n",
    "    content = content.replace(\"\\n\\n\", \"\\n\")\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "async def load_webpages_with_playwright(urls):\n",
    "    content_dict = {}\n",
    "    failed_urls = []\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(\n",
    "            headless=False\n",
    "        )  # Headless False for debugging\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        for url in urls:\n",
    "            try:\n",
    "                logger.info(\n",
    "                    f\"Attempting to load content with Playwright for URL: {url}\"\n",
    "                )\n",
    "                await page.goto(url)\n",
    "\n",
    "                # Wait for network to be idle\n",
    "                await page.wait_for_load_state(\"networkidle\")\n",
    "\n",
    "                # Try using page.evaluate() to directly access the text content via JavaScript\n",
    "                content = await page.evaluate(\"document.body.innerText\")\n",
    "                logger.debug(f\"Extracted content: {content}\")\n",
    "\n",
    "                if content and content.strip():\n",
    "                    clean_content = clean_webpage_text(content)\n",
    "                    content_dict[url] = clean_content\n",
    "                    logger.info(f\"Successfully processed content for {url}\")\n",
    "                else:\n",
    "                    raise ValueError(\"No content extracted.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error occurred while fetching content for {url}: {e}\")\n",
    "                failed_urls.append(url)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    return content_dict, failed_urls\n",
    "\n",
    "\n",
    "async def main():\n",
    "    urls = [\n",
    "        \"https://www.metacareers.com/jobs/522232286825036/?rx_campaign=Linkedin1&rx_ch=connector&rx_group=126320&rx_job=a1KDp00000E28eGMAR&rx_medium=post&rx_r=none&rx_source=Linkedin&rx_ts=20240927T121201Z&rx_vp=slots&utm_campaign=Job%2Bboard&utm_medium=jobs&utm_source=LIpaid&rx_viewer=e3efacca649311ef917d17a1705b89ba0dc4e1e7a57f4231bbce94a604c83931\"\n",
    "    ]\n",
    "    content_dict, failed_urls = await load_webpages_with_playwright(urls)\n",
    "    print(content_dict, failed_urls)\n",
    "\n",
    "\n",
    "# For environments like Jupyter, use get_event_loop instead of asyncio.run\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        asyncio.get_running_loop().run_until_complete(main())\n",
    "    except RuntimeError:  # In case no loop is running (standard Python env)\n",
    "        asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics. You may view our Equal Employment Opportunity notice\n",
      "here. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. We may use your information to maintain the safety and security of Meta, its employees, and others as required or permitted by law. You may view\n",
      "Meta Pay Transparency Policy, Equal Employment Opportunity is the Law notice, and\n",
      "Notice to Applicants for Employment and Employees by clicking on their corresponding links. Additionally, Meta participates in the\n",
      "E-Verify program in certain locations, as required by law.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from llama_index.readers.web import TrafilaturaWebReader\n",
    "\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "urls = [\n",
    "    \"https://www.metacareers.com/jobs/522232286825036/?rx_campaign=Linkedin1&rx_ch=connector&rx_group=126320&rx_job=a1KDp00000E28eGMAR&rx_medium=post&rx_r=none&rx_source=Linkedin&rx_ts=20240927T121201Z&rx_vp=slots&utm_campaign=Job%2Bboard&utm_medium=jobs&utm_source=LIpaid&rx_viewer=e3efacca649311ef917d17a1705b89ba0dc4e1e7a57f4231bbce94a604c83931\"\n",
    "]\n",
    "\n",
    "reader = TrafilaturaWebReader()\n",
    "\n",
    "document = reader.load_data(urls=urls)\n",
    "\n",
    "for i, doc in enumerate(document):\n",
    "    # Check if the document has a 'text' attribute (as in Trafilatura Document object)\n",
    "    if hasattr(doc, \"text\"):\n",
    "        # If it's a Trafilatura Document, use its text attribute\n",
    "        page_text = doc.text\n",
    "        print(page_text)\n",
    "# display(document[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics. You may view our Equal Employment Opportunity notice\n",
      "here. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. We may use your information to maintain the safety and security of Meta, its employees, and others as required or permitted by law. You may view\n",
      "Meta Pay Transparency Policy, Equal Employment Opportunity is the Law notice, and\n",
      "Notice to Applicants for Employment and Employees by clicking on their corresponding links. Additionally, Meta participates in the\n",
      "E-Verify program in certain locations, as required by law.\n"
     ]
    }
   ],
   "source": [
    "import trafilatura\n",
    "\n",
    "\n",
    "def get_webpage_content(url):\n",
    "\n",
    "    download = trafilatura.fetch_url(url)\n",
    "\n",
    "    if download:\n",
    "\n",
    "        content = trafilatura.extract(download)\n",
    "        return content\n",
    "\n",
    "    else:\n",
    "\n",
    "        return \"failed\"\n",
    "\n",
    "\n",
    "urls = \"https://www.metacareers.com/jobs/522232286825036/?rx_campaign=Linkedin1&rx_ch=connector&rx_group=126320&rx_job=a1KDp00000E28eGMAR&rx_medium=post&rx_r=none&rx_source=Linkedin&rx_ts=20240927T121201Z&rx_vp=slots&utm_campaign=Job%2Bboard&utm_medium=jobs&utm_source=LIpaid&rx_viewer=e3efacca649311ef917d17a1705b89ba0dc4e1e7a57f4231bbce94a604c83931\"\n",
    "\n",
    "\n",
    "content = get_webpage_content(urls)\n",
    "\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m browser\n\u001b[0;32m     15\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.metacareers.com/jobs/522232286825036/?rx_campaign=Linkedin1&rx_ch=connector&rx_group=126320&rx_job=a1KDp00000E28eGMAR&rx_medium=post&rx_r=none&rx_source=Linkedin&rx_ts=20240927T121201Z&rx_vp=slots&utm_campaign=Job\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m2Bboard&utm_medium=jobs&utm_source=LIpaid&rx_viewer=e3efacca649311ef917d17a1705b89ba0dc4e1e7a57f4231bbce94a604c83931\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_synamic_webpage_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m content:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(content)\n",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m, in \u001b[0;36mfetch_synamic_webpage_content\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_synamic_webpage_content\u001b[39m(url):\n\u001b[1;32m----> 4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msync_playwright\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchromium\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheadless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\sync_api\\_context_manager.py:47\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_own_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m---> 47\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Error(\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"It looks like you are using Playwright Sync API inside the asyncio loop.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03mPlease use the Async API instead.\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m             )\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;66;03m# Create a new fiber for the protocol dispatcher. It will be pumping events\u001b[39;00m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;66;03m# until the end of times. We will pass control to that fiber every time we\u001b[39;00m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m# block while waiting for a response.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgreenlet_main\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mError\u001b[0m: It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead."
     ]
    }
   ],
   "source": [
    "from playwright.sync_api import sync_playwright\n",
    "\n",
    "\n",
    "def fetch_synamic_webpage_content(url):\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=False)\n",
    "        page = browser.new_page()\n",
    "\n",
    "        page.goto(url)\n",
    "\n",
    "        cotnent = page.content()\n",
    "\n",
    "        browser.close()\n",
    "        return browser\n",
    "\n",
    "\n",
    "url = \"https://www.metacareers.com/jobs/522232286825036/?rx_campaign=Linkedin1&rx_ch=connector&rx_group=126320&rx_job=a1KDp00000E28eGMAR&rx_medium=post&rx_r=none&rx_source=Linkedin&rx_ts=20240927T121201Z&rx_vp=slots&utm_campaign=Job%2Bboard&utm_medium=jobs&utm_source=LIpaid&rx_viewer=e3efacca649311ef917d17a1705b89ba0dc4e1e7a57f4231bbce94a604c83931\"\n",
    "content = fetch_synamic_webpage_content(url)\n",
    "\n",
    "if content:\n",
    "    print(content)\n",
    "else:\n",
    "    print(\"No content extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from playwright.sync_api import Page, expect\n",
    "\n",
    "\n",
    "def test_has_title(page: Page):\n",
    "    page.goto(\n",
    "        \"https://jobs.careers.microsoft.com/us/en/job/1771714/Head-of-Partner-Intelligence-and-Strategy?jobsource=linkedin\"\n",
    "    )\n",
    "\n",
    "    # Expect a title \"to contain\" a substring.\n",
    "    expect(page).to_have_title(re.compile(\"Playwright\"))\n",
    "\n",
    "\n",
    "def test_get_started_link(page: Page):\n",
    "    page.goto(\n",
    "        \"https://jobs.careers.microsoft.com/us/en/job/1771714/Head-of-Partner-Intelligence-and-Strategy?jobsource=linkedin\"\n",
    "    )\n",
    "\n",
    "    # Click the get started link.\n",
    "    page.get_by_role(\"link\", name=\"Get started\").click()\n",
    "\n",
    "    # Expects page to have a heading with the name of Installation.\n",
    "    expect(page.get_by_role(\"heading\", name=\"Installation\")).to_be_visible()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\github\\job_bot\\input_output\\evaluation_optimization\\iteration_0\\similarity_metrics\\Amazon_Product_Manager__Artificial_General_Intelligence_-_Data_Services_sim_metrics.csv done.\n",
      "C:\\github\\job_bot\\input_output\\evaluation_optimization\\iteration_0\\similarity_metrics\\Amazon_Research_Manager_-_Strategy_and_Insights_GCA_Marketing_sim_metrics.csv done.\n",
      "C:\\github\\job_bot\\input_output\\evaluation_optimization\\iteration_0\\similarity_metrics\\Amazon_Sr__Generative_AI_Strategist__Generative_AI_Innovation_Center_sim_metrics.csv done.\n",
      "C:\\github\\job_bot\\input_output\\evaluation_optimization\\iteration_0\\similarity_metrics\\Amplitude_Marketing_Strategy___Analytics_Manager_sim_metrics.csv done.\n",
      "C:\\github\\job_bot\\input_output\\evaluation_optimization\\iteration_0\\similarity_metrics\\Capital_One_Director__AI_Platforms_sim_metrics.csv done.\n",
      "C:\\github\\job_bot\\input_output\\evaluation_optimization\\iteration_0\\similarity_metrics\\Google_AI_Market_Intelligence_Principal_sim_metrics.csv done.\n",
      "C:\\github\\job_bot\\input_output\\evaluation_optimization\\iteration_0\\similarity_metrics\\Liberty_Mutual_Insurance_Senior_Manager_I_-_Corporate_Strategy___Research_sim_metrics.csv done.\n",
      "C:\\github\\job_bot\\input_output\\evaluation_optimization\\iteration_0\\similarity_metrics\\Meta_Product_Strategy_Lead_sim_metrics.csv done.\n",
      "C:\\github\\job_bot\\input_output\\evaluation_optimization\\iteration_0\\similarity_metrics\\Microsoft_Head_of_Partner_Intelligence_and_Strategy_sim_metrics.csv done.\n"
     ]
    }
   ],
   "source": [
    "from utils.get_file_names import get_file_names\n",
    "import pandas as pd\n",
    "\n",
    "DIR_PATH = r\"C:\\github\\job_bot\\input_output\\evaluation_optimization\\iteration_0\\similarity_metrics\"\n",
    "file_paths = get_file_names(DIR_PATH, True, file_types=\".csv\")\n",
    "for file in file_paths:\n",
    "    try:\n",
    "        # Read, rename columns, and save CSV\n",
    "        df = pd.read_csv(file)\n",
    "        df.columns = df.columns.str.lower().str.replace(\" \", \"_\")\n",
    "        df.to_csv(file, index=False)\n",
    "        print(f\"{file} done.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup codes (to be delated later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_file_mapping(\n",
    "    job_descriptions, output_dir, output_file, suffixes=None, append=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a mapping of URLs to file paths for requirements and responsibilities,\n",
    "    and save it to a file. If append is True, it will add to the existing mapping file.\n",
    "\n",
    "    Args:\n",
    "        -job_descriptions (dict): A dictionary containing job descriptions with URLs as keys.\n",
    "        -output_dir (Path): Directory where the files are stored.\n",
    "        -output_file (Path): Path to the file where the mapping will be saved.\n",
    "        -suffixes (list): List of suffixes for file names (e.g., 'reqs_flat', 'resps_flat').\n",
    "        -append (bool): Whether to append to the existing file or overwrite it.\n",
    "        Defaults to False (overwrite).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Safe default for suffixes\n",
    "    if suffixes is None:\n",
    "        suffixes = [\"reqs_flat\", \"resps_flat\", \"sim_metrics\"]\n",
    "\n",
    "    file_mapping = {}\n",
    "\n",
    "    # If appending, load the existing mapping if it exists\n",
    "    if append and output_file.exists():\n",
    "        with open(output_file, \"r\") as f:\n",
    "            file_mapping = json.load(f)\n",
    "        print(f\"Loaded existing mapping from {output_file}\")\n",
    "\n",
    "    # Generate new mappings and add to the existing file_mapping\n",
    "    for url, info in job_descriptions.items():\n",
    "        company = info.get(\"company\")\n",
    "        job_title = info.get(\"job_title\")\n",
    "\n",
    "        # Generate file names for requirements, responsibilities, and metrics\n",
    "        mapping_entry = {}\n",
    "        for suffix in suffixes:\n",
    "            ext = \".json\" if \"flat\" in suffix else \".csv\"\n",
    "            file_name = f\"{company}_{job_title}_{suffix}{ext}\"\n",
    "            file_path = output_dir / file_name\n",
    "            mapping_entry[suffix] = str(file_path)  # Store the file path as a string\n",
    "\n",
    "        # Add the new mapping to the existing file_mapping\n",
    "        file_mapping[url] = mapping_entry\n",
    "\n",
    "    # Save the updated mapping back to the file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(file_mapping, f, indent=4)\n",
    "\n",
    "    print(f\"Mapping saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back up just in case\n",
    "\n",
    "\n",
    "def call_llama3(self, prompt, temperature):\n",
    "\n",
    "    # def edit_text_for_semantic_entailment_llama3(\n",
    "\n",
    "    #     candidate_text,\n",
    "\n",
    "    #     reference_text,\n",
    "\n",
    "    #     text_id=None,\n",
    "\n",
    "    #     temperature=0.6,\n",
    "\n",
    "    # ):\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    Edits/optimizes a text using Llama3 model based on another text (source text).\n",
    "\n",
    "\n",
    "\n",
    "    Args:\n",
    "\n",
    "\n",
    "        - text_id (int): (Optional) Identifier for the responsibility text.\n",
    "\n",
    "\n",
    "        Default to None (unique ids to be generated with UUID function)\n",
    "\n",
    "\n",
    "        - candidate_text (str): Original text to be transformed by the model\n",
    "\n",
    "\n",
    "        (i.e., the riginal responsibility text to be revised.)\n",
    "\n",
    "\n",
    "        - reference_text (str): Text that the candidate text is being compared to\n",
    "\n",
    "\n",
    "        (i.e., requirement text to optimize against.)\n",
    "\n",
    "\n",
    "        - max_tokens: default set to 1056\n",
    "\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "\n",
    "        dict: Contains 'resp_id' and 'optimized_text' after revision.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate a unique text_id using UUID\n",
    "\n",
    "    if text_id is None:\n",
    "\n",
    "        text_id = str(uuid.uuid4())\n",
    "\n",
    "    # Create the prompt using a predefined template\n",
    "\n",
    "    try:\n",
    "\n",
    "        prompt = SEMANTIC_ALIGNMENT_PROMPT.format(\n",
    "            content_1=candidate_text, content_2=reference_text\n",
    "        )\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        logger.error(f\"Error formatting STRUCTURE_TRANSFER_PROMPT: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Call api function and get response; deserialize from pydantic obj to dict\n",
    "\n",
    "    response_pyd_obj = call_llama3(\n",
    "        prompt, expected_res_type=\"json\", temperature=temperature\n",
    "    )\n",
    "\n",
    "    # Ensure the response is in the correct format (Pydantic JSONResponse model)\n",
    "\n",
    "    if not isinstance(response_pyd_obj, JSONResponse):\n",
    "\n",
    "        logger.error(\"Received response is not in expected JSONResponse format.\")\n",
    "\n",
    "        raise ValueError(\"Received response is not in expected JSONResponse format.\")\n",
    "\n",
    "    # Deserialize pydantic obj. (expect a dictionary)\n",
    "\n",
    "    response_dict = response_pyd_obj.model_dump()\n",
    "\n",
    "    # print(f\"final text: {response_dict}\")\n",
    "\n",
    "    # Validate the JSON structure using JSON Schema\n",
    "\n",
    "    try:\n",
    "\n",
    "        jsonschema.validate(instance=response_dict, schema=LLM_RES_JSON_SCHEMA)\n",
    "\n",
    "        logger.info(\"JSON schema validation passed.\")\n",
    "\n",
    "    except jsonschema.exceptions.ValidationError as e:\n",
    "\n",
    "        logger.error(f\"JSON schema validation failed: {e}\")\n",
    "\n",
    "        raise ValueError(f\"Invalid JSON format: {e}\")\n",
    "\n",
    "    # Combine w/t id, then return the combined dictionary\n",
    "\n",
    "    result = {\"resp_id\": text_id, **response_dict}  # ** to unpack a dictionary\n",
    "\n",
    "    logger.info(f\"Results updated: \\n{result}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def validate_response(self, response_dict):\n",
    "    \"\"\"Validate the API response dictionary using JSON Schema.\"\"\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        jsonschema.validate(instance=response_dict, schema=LLM_RES_JSON_SCHEMA)\n",
    "\n",
    "        logger.info(\"JSON schema validation passed.\")\n",
    "\n",
    "    except jsonschema.exceptions.ValidationError as e:\n",
    "\n",
    "        logger.error(f\"JSON schema validation failed: {e}\")\n",
    "\n",
    "        raise ValueError(f\"Invalid JSON format: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_text_for_semantic_entailment(\n",
    "    client,\n",
    "    candidate_text,\n",
    "    reference_text,\n",
    "    text_id=None,\n",
    "    model_id=\"gpt-4-turbo\",\n",
    "    temperature=0.6,\n",
    "    max_tokens=1056,\n",
    "):\n",
    "\n",
    "    # Generate a unique text_id using UUID\n",
    "    if text_id is None:\n",
    "        text_id = str(uuid.uuid4())\n",
    "\n",
    "    # Create the prompt using a predefined template\n",
    "    try:\n",
    "        prompt = SEMANTIC_ALIGNMENT_PROMPT.format(\n",
    "            content_1=candidate_text, content_2=reference_text\n",
    "        )\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"Error formatting STRUCTURE_TRANSFER_PROMPT: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Call call openai api function and get response;\n",
    "    # deserialize from pydantic obj to dict\n",
    "    response_pyd_obj = call_openai_api(\n",
    "        client,\n",
    "        model_id,\n",
    "        prompt,\n",
    "        expected_res_type=\"json\",\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    # Ensure the response is a pyd  (Pydantic JSONResponse model)\n",
    "    if not isinstance(response_pyd_obj, JSONResponse):\n",
    "        logger.error(\"Received response is not in expected JSONResponse format.\")\n",
    "        raise ValueError(\"Received response is not in expected JSONResponse format.\")\n",
    "\n",
    "    # Deserialize pydantic obj. (expect a dictionary)\n",
    "    response_dict = response_pyd_obj.model_dump()\n",
    "    # print(f\"final text: {response_dict}\")\n",
    "\n",
    "    # Validate the JSON structure using JSON Schema\n",
    "    try:\n",
    "        jsonschema.validate(instance=response_dict, schema=LLM_RES_JSON_SCHEMA)\n",
    "        logger.info(\"JSON schema validation passed.\")\n",
    "    except jsonschema.exceptions.ValidationError as e:\n",
    "        logger.error(f\"JSON schema validation failed: {e}\")\n",
    "        raise ValueError(f\"Invalid JSON format: {e}\")\n",
    "\n",
    "    # Combine w/t id, then return the combined dictionary\n",
    "    result = {\"resp_id\": text_id, **response_dict}  # ** to unpack a dictionary\n",
    "\n",
    "    logger.info(f\"Results updated: \\n{result}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def edit_text_for_dp(\n",
    "    client,\n",
    "    target_text,\n",
    "    source_text,\n",
    "    text_id=None,\n",
    "    model_id=\"gpt-4-turbo\",\n",
    "    temperature=0.8,\n",
    "    max_tokens=1056,\n",
    "):\n",
    "    \"\"\"\n",
    "    Re-edit the target text to better align w/t source text's dependency parsing (DP),\n",
    "    leveraging the OpenAI API.\n",
    "\n",
    "    Example:\n",
    "    Re-edit revised responsibility to match with the original responsibility text's DP\n",
    "    to perserve the tone & style.\n",
    "\n",
    "    Args:\n",
    "        - client (OpenAI()): OpenAI API client instance.\n",
    "        - text_id (str): Identifier of the target text (defaulted to None - unique IDs\n",
    "        to be generated by UUID function)\n",
    "        (i.e., the responsibility bullet text.)\n",
    "        - target_text (str): The target text to be transformed (i.e.,\n",
    "        revised responsibility text).\n",
    "        - source_text (str): The source text from whose \"dependency parsing\"\n",
    "        to be modeled after (i.e., original responsibility text from resume).\n",
    "        - model_id (str): OpenAI model to use (default is 'gpt-4').\n",
    "        - temperature (float): defaulted to 0.8 (a higher temperature setting is\n",
    "        needed to give the model more flexibility/creativity).\n",
    "        - max_token: default to 1056\n",
    "\n",
    "        Note:\n",
    "        - resp is short for responsibility\n",
    "        - req is short for (job) requirement\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary in the format of {'resp_id': \"...\", 'optimized_text': \"...\"}.\n",
    "    \"\"\"\n",
    "    # Generate a unique text_id using UUID\n",
    "    if text_id is None:\n",
    "        text_id = str(uuid.uuid4())\n",
    "\n",
    "    # Define the JSON schema and instructions clearly in the prompt\n",
    "    try:\n",
    "        prompt = STRUCTURE_TRANSFER_PROMPT.format(\n",
    "            content_1=source_text, content_2=target_text\n",
    "        )\n",
    "        print(f\"DEBUG - Formatted Prompt in edit_text_for_dp:\\n{prompt}\")\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"Error formatting STRUCTURE_TRANSFER_PROMPT: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Call API function and get response; deserialize from pydantic obj to dict\n",
    "    response_pyd_obj = call_openai_api(\n",
    "        client,\n",
    "        model_id,\n",
    "        prompt,\n",
    "        expected_res_type=\"json\",\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    # Ensure the response is in the correct format (Pydantic JSONResponse model)\n",
    "    if not isinstance(response_pyd_obj, JSONResponse):\n",
    "        logger.error(\"Received response is not in expected JSONResponse format.\")\n",
    "        raise ValueError(\"Received response is not in expected JSONResponse format.\")\n",
    "\n",
    "    # Deserialize pydantic obj. (expect a dictionary)\n",
    "    response_dict = response_pyd_obj.model_dump()\n",
    "\n",
    "    # Validate the JSON structure using JSON Schema\n",
    "    try:\n",
    "        jsonschema.validate(instance=response_dict, schema=LLM_RES_JSON_SCHEMA)\n",
    "        logger.info(\"JSON schema validation passed.\")\n",
    "    except jsonschema.exceptions.ValidationError as e:\n",
    "        logger.error(f\"JSON schema validation failed: {e}\")\n",
    "        raise ValueError(f\"Invalid JSON format: {e}\")\n",
    "\n",
    "    # Combine w/t id, then return the combined dictionary\n",
    "    result = {\"resp_id\": text_id, **response_dict}  # ** to unpack a dictionary\n",
    "\n",
    "    logger.info(f\"Results updated: \\n{result}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_json_files_processing_mini_pipeline(\n",
    "    job_descriptions_file: Union[str, Path],\n",
    "    job_requirements_file: Union[str, Path],\n",
    "    resume_json_file: Union[str, Path],\n",
    "    flat_json_output_files_dir: Union[str, Path],\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocessing mini-pipeline for flattening and saving JSON files for responsibilities and requirements.\n",
    "\n",
    "    This pipeline processes the resume and job descriptions by flattening the JSON structure of responsibilities\n",
    "    and job requirements and saving the flattened data to separate JSON files.\n",
    "\n",
    "    The pipeline:\n",
    "    1. Finds new URLs (from job descriptions).\n",
    "    2. Creates and saves flattened responsibilities (resume) and requirements (job posting) JSON files.\n",
    "\n",
    "    Args:\n",
    "        job_descriptions_file (str or Path): Path to the JSON file containing job descriptions.\n",
    "        job_requirements_file (str or Path): Path to the extracted requirements (from job postings) JSON file.\n",
    "        resume_json_file (str or Path): Path to the resume JSON file.\n",
    "        flat_json_output_files_dir (str or Path): Directory where output files are stored.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Convert inputs to Path objects if they are not already\n",
    "    if not isinstance(job_descriptions_file, Path):\n",
    "        job_descriptions_file = Path(job_descriptions_file)\n",
    "    if not isinstance(job_requirements_file, Path):\n",
    "        job_requirements_file = Path(job_requirements_file)\n",
    "    if not isinstance(resume_json_file, Path):\n",
    "        resume_json_file = Path(resume_json_file)\n",
    "    if not isinstance(flat_json_output_files_dir, Path):\n",
    "        flat_json_output_files_dir = Path(flat_json_output_files_dir)\n",
    "\n",
    "    logger.info(f\"Job descriptions file path: {job_descriptions_file}\")\n",
    "    logger.info(f\"Job requirements file path: {job_requirements_file}\")\n",
    "    logger.info(f\"Resume JSON file path: {resume_json_file}\")\n",
    "    logger.info(f\"Flat JSON output files dir: {flat_json_output_files_dir}\")\n",
    "\n",
    "    # Step 1: Read job descriptions\n",
    "    try:\n",
    "        job_descriptions = read_from_json_file(job_descriptions_file)\n",
    "        logger.info(f\"job_descriptions:\\n{job_descriptions}\")\n",
    "        if not job_descriptions:\n",
    "            logger.error(\"No job descriptions loaded. Exiting.\")\n",
    "            return\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Job descriptions file not found: {e}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading job descriptions: {e}\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Find new URLs and corresponding file paths\n",
    "    try:\n",
    "        new_urls_and_file_paths = get_new_urls_and_flat_json_file_paths(\n",
    "            job_descriptions, flat_json_output_files_dir\n",
    "        )\n",
    "        if not new_urls_and_file_paths:\n",
    "            logger.info(\"No new flat JSON files to process. Exiting.\")\n",
    "            return\n",
    "        logger.info(\n",
    "            f\"Found {len(new_urls_and_file_paths)} new flat JSON files to process.\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving new URLs and file paths: {e}\")\n",
    "        return\n",
    "\n",
    "    # Step 3: Create and save flattened job requirements files for each URL\n",
    "    try:\n",
    "        for url, file_path in new_urls_and_file_paths.items():\n",
    "            process_and_save_requirements_by_url(\n",
    "                requirements_json_file=job_requirements_file,\n",
    "                url=url,\n",
    "                requirements_flat_json_file=file_path,\n",
    "            )\n",
    "            logger.info(f\"Requirements flat file saved: {file_path}\")\n",
    "        logger.info(\"All requirements flat files saved.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing requirements files: {e}\")\n",
    "        return\n",
    "\n",
    "    # Step 4: Create and save flattened responsibilities file from the resume\n",
    "    try:\n",
    "        responsibilities_flat_json_file = (\n",
    "            flat_json_output_files_dir / \"responsibilities_flat.json\"\n",
    "        )\n",
    "        process_and_save_responsibilities_from_resume(\n",
    "            resume_json_file=resume_json_file,\n",
    "            responsibilities_flat_json_file=responsibilities_flat_json_file,\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"Responsibilities flat file saved: {responsibilities_flat_json_file}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing responsibilities file: {e}\")\n",
    "        return\n",
    "\n",
    "    logger.info(\n",
    "        \"All responsibilities and requirements flattened files created and saved.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_preprocessing_mini_pipeline(job_descriptions_file, output_dir):\n",
    "    \"\"\"\n",
    "    Preprocess job descriptions for evaluation by identifying new URLs to process.\n",
    "\n",
    "    Args:\n",
    "        job_descriptions_file (str or Path): Path to the JSON file containing job descriptions.\n",
    "        output_dir (str or Path, optional): Directory where output files are stored.\n",
    "            Defaults to METRICS_OUTPUTS_CSV_FILES_DIR / \"iteration_0\".\n",
    "\n",
    "    Returns:\n",
    "        Dict: A dictionary of new URLs that need to be processed and their file paths to be saved.\n",
    "    \"\"\"\n",
    "    job_descriptions = read_from_json_file(job_descriptions_file)\n",
    "    if not job_descriptions:\n",
    "        logger.error(\"No job descriptions loaded. Exiting.\")\n",
    "        return {}\n",
    "\n",
    "    new_urls_and_f_names = get_new_urls_and_metrics_file_paths(\n",
    "        job_descriptions, output_dir\n",
    "    )\n",
    "    logger.info(f\"Found {len(new_urls_and_f_names)} new URLs to process.\")\n",
    "    return new_urls_and_f_names  # a dict\n",
    "\n",
    "\n",
    "def metrics_processing_pipeline(\n",
    "    url: str, requirements_json_file: str, resume_json_file: str, csv_file: str\n",
    "):\n",
    "\n",
    "    logger.info(\"Start running responsibility/requirement alignment scoring pipeline.\")\n",
    "\n",
    "    # Step 1: Parse and flatten responsibilities from resume (as a dict)\n",
    "    resume_parser = ResumeParser(resume_json_file)\n",
    "    resps_flat = resume_parser.extract_and_flatten_responsibilities()  # dict\n",
    "\n",
    "    # Step 2: Parse and flatten job requirements (as a dict) or\n",
    "    # parse/flatten/concatenate into a single string\n",
    "    job_reqs_parser = JobRequirementsParser(requirements_json_file, url)\n",
    "    reqs_flat = job_reqs_parser.extract_flatten_reqs()  # dict\n",
    "\n",
    "    # Check if either responsibilities or requirements are empty\n",
    "    if not resps_flat or not reqs_flat:\n",
    "        logger.error(\n",
    "            \"One of the required datasets (responsibilities or requirements) is empty.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # Step 3. Calculate and display similarity metrics - Segment by Segment\n",
    "    similarity_df = calculate_many_to_many_similarity_metrices(resps_flat, reqs_flat)\n",
    "    logger.info(\"Similarity metrics calculated.\")  # Changed to logger\n",
    "\n",
    "    # Step 4. Add score category values (high, mid, low)\n",
    "    # Translate DataFrame columns to match expected column names\n",
    "    similarity_df = categorize_scores_for_df(similarity_df)\n",
    "    logger.info(\"Similarity metrics score categories created.\")\n",
    "\n",
    "    # Step 5. Clean up the data by removing newline characters from the DataFrame\n",
    "    df = similarity_df.applymap(lambda x: str(x).replace(\"\\n\", \" \").strip())\n",
    "\n",
    "    # Step 6. Ensure the output directory exists and save the CSV file\n",
    "    df.to_csv(csv_file, index=False)  # Save to the correct path\n",
    "    logger.info(f\"Similarity metrics saved to file ({csv_file})\")\n",
    "\n",
    "    logger.info(\n",
    "        \"Finished running responsibility/requirement alignment scoring pipeline.\"\n",
    "    )\n",
    "\n",
    "    # Display the top rows of the DataFrame for verification\n",
    "    print(df.head(5))  # Updated display to print for non-interactive environments\n",
    "    logger.info(\n",
    "        f\"Finished running responsibility/requirement alignment scoring pipeline for {url}.\"\n",
    "    )\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "def get_metrics_file_paths(mapping_file, output_dir):\n",
    "    \"\"\"\n",
    "    Get a dictionary of URLs and their corresponding metrics file paths.\n",
    "\n",
    "    Args:\n",
    "        mapping_file_path (str or Path): Path to the JSON mapping file.\n",
    "        output_dir (str or Path): Directory where the new metrics files will be created.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are URLs and values are the paths to the metrics files.\n",
    "    \"\"\"\n",
    "\n",
    "    sim_metrics_mapping_dict = {\n",
    "        url: data[\"sim_metrics\"] for url, data in mapping_dict.items()\n",
    "    }\n",
    "    return sim_metrics_mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_response(response_content, context_type):\n",
    "    cleaned_content = clean_and_extract_json(response_content)\n",
    "    if not cleaned_content:\n",
    "        raise ValueError(\"Received an empty JSON response after cleaning.\")\n",
    "\n",
    "    response_dict = json.loads(cleaned_content)\n",
    "\n",
    "    if context_type == \"editing\":\n",
    "        return EditingResponseModel(optimized_text=response_dict.get(\"optimized_text\"))\n",
    "    elif context_type == \"job_site\":\n",
    "        return JobSiteResponseModel(\n",
    "            url=response_dict.get(\"url\"),\n",
    "            job_title=response_dict.get(\"job_title\"),\n",
    "            company=response_dict.get(\"company\"),\n",
    "            location=response_dict.get(\"location\"),\n",
    "            salary_info=response_dict.get(\"salary_info\"),\n",
    "            posted_date=response_dict.get(\"posted_date\"),\n",
    "            content=response_dict.get(\"content\"),\n",
    "        )\n",
    "    return JSONResponse(data=response_dict)\n",
    "\n",
    "\n",
    "def parse_tabular_response(response_content):\n",
    "    df = pd.read_csv(StringIO(response_content))\n",
    "    return TabularResponse(data=df)\n",
    "\n",
    "\n",
    "def parse_code_response(response_content):\n",
    "    return CodeResponse(code=response_content)\n",
    "\n",
    "\n",
    "def call_openai_api(\n",
    "    prompt,\n",
    "    client=None,\n",
    "    model_id=\"gpt-4-turbo\",\n",
    "    expected_res_type=\"str\",\n",
    "    context_type: str = \"\",  # Use this to determine which JSON model to use\n",
    "    temperature=0.4,\n",
    "    max_tokens=1056,\n",
    ") -> Union[\n",
    "    TextResponse,\n",
    "    JSONResponse,\n",
    "    TabularResponse,\n",
    "    CodeResponse,\n",
    "    EditingResponseModel,\n",
    "    JobSiteResponseModel,\n",
    "]:\n",
    "    \"\"\"\n",
    "    Handles API call to OpenAI to generate responses based on a given prompt and expected response type.\n",
    "\n",
    "    Args:\n",
    "        - client: OpenAI API client instance (optional)\n",
    "        - model_id (str): Model ID to use for the OpenAI API call.\n",
    "        - prompt (str): The prompt to send to the API.\n",
    "        - expected_res_type (str): The expected type of response from the API\n",
    "        ('str', 'json', 'tabular', or 'code').\n",
    "        - context_type (str): Specifies whether to use a job-related JSON model or\n",
    "        an editing model for JSON (\"editing\" or \"job_site\")\n",
    "        - temperature (str): creativity of the response (0 to 1.0)\n",
    "        - max_tokens: Maximum tokens to be generated in response.\n",
    "\n",
    "    Returns:\n",
    "        - Union[str, JSONResponse, pd.DataFrame, CodeResponse]:\n",
    "        Response formatted according to the specified expected_response_type ()\n",
    "\n",
    "        - Union: A utility from Python's typing module\n",
    "        - str: plain text response\n",
    "        - JSONResponse: a custom Pydantic model for JSON response (e.g., inherited from pydantic.BaseModel).\n",
    "        - pd.DataFrame: pandas object; for tabular response\n",
    "        - CodeResponse: a custom Pydantic model for code response (e.g., inherited from pydantic.BaseModel).\n",
    "    \"\"\"\n",
    "    if not client:\n",
    "        openai_api_key = get_openai_api_key()\n",
    "        client = OpenAI(api_key=openai_api_key)\n",
    "        logger.info(\"OpenaAI API instantiated.\")\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Making API call with expected response type: {expected_res_type}\")\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful assistant who strictly adheres to the provided instructions \"\n",
    "                    \"and returns responses in the specified format.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,  # Ensure sufficient tokens for response\n",
    "        )\n",
    "\n",
    "        # Extract the content from the response\n",
    "        response_content = response.choices[0].message.content.strip()\n",
    "        logger.info(f\"Raw LLM Response: {response_content}\")\n",
    "\n",
    "        # Check if the response is empty\n",
    "        if not response_content:\n",
    "            logger.error(\"Received an empty response from OpenAI API.\")\n",
    "            raise ValueError(\"Received an empty response from OpenAI API.\")\n",
    "\n",
    "        # Handle response based on expected type using Pydantic models\n",
    "        if expected_res_type == \"str\":\n",
    "            # Return plain string wrapped in a Pydantic model\n",
    "            parsed_response = TextResponse(content=response_content)\n",
    "            return parsed_response  # return as plain string instead the model\n",
    "\n",
    "        elif expected_res_type == \"json\":\n",
    "            try:\n",
    "                cleaned_response_content = clean_and_extract_json(response_content)\n",
    "                if not cleaned_response_content:\n",
    "                    logger.error(\"Received an empty response after cleaning.\")\n",
    "                    raise ValueError(\"Received an empty response after cleaning.\")\n",
    "\n",
    "                response_dict = json.loads(cleaned_response_content)\n",
    "\n",
    "                # Determine the correct model to use based on context_type\n",
    "                if context_type == \"editing\":\n",
    "                    return EditingResponseModel(\n",
    "                        optimized_text=response_dict.get(\"optimized_text\")\n",
    "                    )\n",
    "\n",
    "                elif context_type == \"job_site\":\n",
    "                    return JobSiteResponseModel(\n",
    "                        url=response_dict.get(\"url\"),\n",
    "                        job_title=response_dict.get(\"job_title\"),\n",
    "                        company=response_dict.get(\"company\"),\n",
    "                        location=response_dict.get(\"location\"),\n",
    "                        salary_info=response_dict.get(\"salary_info\"),\n",
    "                        posted_date=response_dict.get(\"posted_date\"),\n",
    "                        content=response_dict.get(\"content\"),\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    # Fallback to a more generic JSON response if no specific context is provided\n",
    "                    return JSONResponse(data=response_dict)\n",
    "\n",
    "            except (json.JSONDecodeError, ValidationError) as e:\n",
    "                logger.error(f\"Failed to parse JSON or validate with Pydantic: {e}\")\n",
    "                raise ValueError(\"Invalid JSON format received from OpenAI API.\")\n",
    "\n",
    "        elif expected_res_type == \"tabular\":\n",
    "            try:\n",
    "                # Parse tabular response using pandas\n",
    "                # Assumes the response is in a CSV or Markdown table format\n",
    "                df = pd.read_csv(StringIO(response_content))\n",
    "                parsed_response = TabularResponse(data=df)\n",
    "                return parsed_response\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error parsing tabular data: {e}\")\n",
    "                raise ValueError(\"Invalid tabular format received from OpenAI API.\")\n",
    "\n",
    "        elif expected_res_type == \"code\":\n",
    "            # Return the code as a Pydantic model\n",
    "            parsed_response = CodeResponse(code=response_content)\n",
    "            return parsed_response\n",
    "\n",
    "        else:\n",
    "            # Handle unsupported response types\n",
    "            logger.error(f\"Unsupported expected_response_type: {expected_res_type}\")\n",
    "            raise ValueError(f\"Unsupported expected_response_type: {expected_res_type}\")\n",
    "    except (json.JSONDecodeError, ValidationError) as e:\n",
    "        logger.error(f\"Validation or parsing error: {e}\")\n",
    "        raise ValueError(f\"Invalid format received from OpenAI API: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"OpenAI API call failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def call_claude_api(\n",
    "    prompt,\n",
    "    client=None,\n",
    "    model_id=\"claude-3-5-sonnet-20241022\",  # Changed default model\n",
    "    expected_res_type=\"str\",\n",
    "    context_type: str = \"\",\n",
    "    temperature=0.4,\n",
    "    max_tokens=1056,\n",
    ") -> Union[\n",
    "    TextResponse,\n",
    "    JSONResponse,\n",
    "    TabularResponse,\n",
    "    CodeResponse,\n",
    "    EditingResponseModel,\n",
    "    JobSiteResponseModel,\n",
    "]:\n",
    "    \"\"\"\n",
    "    Handles API call to Anthropic Claude to generate responses based on a given prompt and expected response type.\n",
    "\n",
    "    Args:\n",
    "        - client: Anthropic API client instance (optional)\n",
    "        - model_id (str): Model ID to use for the Claude API call.\n",
    "        - prompt (str): The prompt to send to the API.\n",
    "        - expected_res_type (str): The expected type of response from the API\n",
    "        ('str', 'json', 'tabular', or 'code').\n",
    "        - context_type (str): Specifies whether to use a job-related JSON model or\n",
    "        an editing model for JSON (\"editing\" or \"job_site\")\n",
    "        - temperature (float): creativity of the response (0 to 1.0)\n",
    "        - max_tokens (int): Maximum tokens to be generated in response.\n",
    "\n",
    "    Returns:\n",
    "        - Union[str, JSONResponse, pd.DataFrame, CodeResponse]:\n",
    "        Response formatted according to the specified expected_response_type ()\n",
    "        - Union: A utility from Python's typing module\n",
    "        - str: plain text response\n",
    "        - JSONResponse: a custom Pydantic model for JSON response\n",
    "        (e.g., inherited from pydantic.BaseModel).\n",
    "        - pd.DataFrame: pandas object; for tabular response\n",
    "        - CodeResponse: a custom Pydantic model for code response\n",
    "        (e.g., inherited from pydantic.BaseModel).\n",
    "    \"\"\"\n",
    "    if not client:\n",
    "        claude_api_key = get_claude_api_key()\n",
    "        client = Anthropic(api_key=claude_api_key)\n",
    "        logger.info(\"Anthropic API instantiated.\")\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Making API call with expected response type: {expected_res_type}\")\n",
    "\n",
    "        # Combine system messageand prompt into a single user message\n",
    "        # Claude doesn't have separate user/system setup - just one\n",
    "        system_instruction = \"You are a helpful assistant who strictly adheres to \\\n",
    "            the provided instructions.\"\n",
    "        full_prompt = system_instruction + prompt\n",
    "\n",
    "        # Single API call using messages endpoint\n",
    "        response = client.messages.create(\n",
    "            model=model_id,\n",
    "            max_tokens=max_tokens,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": full_prompt,\n",
    "                },\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        # Extract the content from the response\n",
    "        response_content = response.content[0]\n",
    "        logger.info(f\"Raw LLM Response: {response_content}\")\n",
    "\n",
    "        # Check if the response is empty\n",
    "        if not response_content:\n",
    "            logger.error(\"Received an empty response from Claude API.\")\n",
    "            raise ValueError(\"Received an empty response from Claude API.\")\n",
    "\n",
    "        # Rest of the function remains the same since it's handling the response parsing\n",
    "        if expected_res_type == \"str\":\n",
    "            parsed_response = TextResponse(content=response_content.text)\n",
    "            return parsed_response\n",
    "\n",
    "        elif expected_res_type == \"json\":\n",
    "            try:\n",
    "                cleaned_response_content = clean_and_extract_json(response_content)\n",
    "                if not cleaned_response_content:\n",
    "                    logger.error(\"Received an empty response after cleaning.\")\n",
    "                    raise ValueError(\"Received an empty response after cleaning.\")\n",
    "\n",
    "                response_dict = json.loads(cleaned_response_content)\n",
    "\n",
    "                if context_type == \"editing\":\n",
    "                    return EditingResponseModel(\n",
    "                        optimized_text=response_dict.get(\"optimized_text\")\n",
    "                    )\n",
    "                elif context_type == \"job_site\":\n",
    "                    return JobSiteResponseModel(\n",
    "                        url=response_dict.get(\"url\"),\n",
    "                        job_title=response_dict.get(\"job_title\"),\n",
    "                        company=response_dict.get(\"company\"),\n",
    "                        location=response_dict.get(\"location\"),\n",
    "                        salary_info=response_dict.get(\"salary_info\"),\n",
    "                        posted_date=response_dict.get(\"posted_date\"),\n",
    "                        content=response_dict.get(\"content\"),\n",
    "                    )\n",
    "                else:\n",
    "                    return JSONResponse(data=response_dict)\n",
    "\n",
    "            except (json.JSONDecodeError, ValidationError) as e:\n",
    "                logger.error(f\"Failed to parse JSON or validate with Pydantic: {e}\")\n",
    "                raise ValueError(\"Invalid JSON format received from Claude API.\")\n",
    "\n",
    "        elif expected_res_type == \"tabular\":\n",
    "            try:\n",
    "                df = pd.read_csv(StringIO(response_content))\n",
    "                parsed_response = TabularResponse(data=df)\n",
    "                return parsed_response\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error parsing tabular data: {e}\")\n",
    "                raise ValueError(\"Invalid tabular format received from Claude API.\")\n",
    "\n",
    "        elif expected_res_type == \"code\":\n",
    "            parsed_response = CodeResponse(code=response_content)\n",
    "            return parsed_response\n",
    "\n",
    "        else:\n",
    "            logger.error(f\"Unsupported expected_response_type: {expected_res_type}\")\n",
    "            raise ValueError(f\"Unsupported expected_response_type: {expected_res_type}\")\n",
    "\n",
    "    except (json.JSONDecodeError, ValidationError) as e:\n",
    "        logger.error(f\"Validation or parsing error: {e}\")\n",
    "        raise ValueError(f\"Invalid format received from Claude API: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Claude API call failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def call_llama3(\n",
    "    prompt: str,\n",
    "    expected_res_type: str = \"str\",\n",
    "    temperature: float = 0.4,\n",
    "    max_tokens: int = 1056,\n",
    "):\n",
    "    \"\"\"\n",
    "    Handles calls to LLaMA 3 (on-premise) to generate responses based on\n",
    "    a given prompt and expected response type.\n",
    "\n",
    "    Args:\n",
    "        - model_id (str): Model ID to use for the LLaMA 3 API call.\n",
    "        - prompt (str): The prompt to send to the API.\n",
    "        - expected_response_type (str):\n",
    "            * The expected type of response from the API.\n",
    "            * Options are 'str' (default), 'json', 'tabular', or 'code'.\n",
    "\n",
    "    Returns:\n",
    "        - Union[str, JSONResponse, pd.DataFrame, CodeResponse]:\n",
    "        Response formatted according to the specified expected_response_type\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate response\n",
    "        response = ollama.generate(\n",
    "            model=\"llama3\",\n",
    "            prompt=prompt,\n",
    "            options={\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"batch_size\": 10,\n",
    "                \"retry_enabled\": True,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Extract the content from the response\n",
    "        response_content = response[\"response\"]\n",
    "\n",
    "        # Log the raw response before attempting to parse it\n",
    "        logger.info(f\"Raw response content: {response_content}\")\n",
    "\n",
    "        # Check if the response is empty\n",
    "        if not response_content:\n",
    "            logger.error(\"Received an empty response from LLaMA 3.\")\n",
    "            raise ValueError(\"Received an empty response from LLaMA 3.\")\n",
    "\n",
    "        # Handle response based on expected type using Pydantic models\n",
    "        if expected_res_type == \"str\":\n",
    "            # Return plain string wrapped in a Pydantic model\n",
    "            parsed_response = TextResponse(content=response_content)\n",
    "            return parsed_response.content  # return as plain string instead the model\n",
    "\n",
    "        elif expected_res_type == \"json\":\n",
    "            # Extract JSON content using string manipulation\n",
    "            start_idx = response_content.find(\"{\")\n",
    "            end_idx = response_content.rfind(\"}\")\n",
    "            if start_idx != -1 and end_idx != -1:\n",
    "                cleaned_response_content = response_content[start_idx : end_idx + 1]\n",
    "            else:\n",
    "                logger.error(\"Failed to extract JSON content.\")\n",
    "                raise ValueError(\"Failed to extract JSON content.\")\n",
    "\n",
    "            # Validate extracted JSON\n",
    "            try:\n",
    "                response_dict = json.loads(cleaned_response_content)\n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.error(f\"Invalid JSON: {e}\")\n",
    "                raise ValueError(\"Invalid JSON format received from LLaMA 3.\")\n",
    "\n",
    "            # Verify JSON structure using Pydantic model\n",
    "            try:\n",
    "                parsed_response = JSONResponse(**response_dict)\n",
    "            except ValidationError as e:\n",
    "                logger.error(f\"JSON validation error: {e}\")\n",
    "                raise ValueError(\"Invalid JSON format received from LLaMA 3.\")\n",
    "\n",
    "            return parsed_response\n",
    "        elif expected_res_type == \"tabular\":\n",
    "            # Parse tabular response using pandas\n",
    "            # Assumes the response is in a CSV or Markdown table format\n",
    "            df = pd.read_csv(StringIO(response_content))\n",
    "            parsed_response = TabularResponse(data=df)\n",
    "            return parsed_response\n",
    "\n",
    "        elif expected_res_type == \"code\":\n",
    "            # Return the code as a Pydantic model\n",
    "            parsed_response = CodeResponse(code=response_content)\n",
    "            return parsed_response\n",
    "\n",
    "        else:\n",
    "            # Handle unsupported response types\n",
    "            logger.error(f\"Unsupported expected_response_type: {expected_res_type}\")\n",
    "            raise ValueError(f\"Unsupported expected_response_type: {expected_res_type}\")\n",
    "    except (json.JSONDecodeError, ValidationError) as e:\n",
    "        logger.error(f\"Validation or parsing error: {e}\")\n",
    "        raise ValueError(f\"Invalid format received from LLaMA 3: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"LLaMA 3 call failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# OpenAI parsing functions\n",
    "def parse_openai_text_response(response) -> OpenAITextResponse:\n",
    "    \"\"\"\n",
    "    Parses the raw JSON response from OpenAI's text generation API.\n",
    "\n",
    "    Args:\n",
    "        response (dict): JSON response from OpenAI API containing a \"choices\" field,\n",
    "                         where the first choice's \"text\" key holds the generated text.\n",
    "\n",
    "    Returns:\n",
    "        OpenAITextResponse: Parsed OpenAI text response model.\n",
    "    \"\"\"\n",
    "    return OpenAITextResponse(text=response[\"choices\"][0][\"text\"])\n",
    "\n",
    "\n",
    "def parse_openai_json_response(response) -> OpenAIJSONResponse:\n",
    "    \"\"\"\n",
    "    Parses the raw JSON response from OpenAI when expecting a JSON structure.\n",
    "\n",
    "    Args:\n",
    "        response (dict): JSON response directly from OpenAI API, expected to contain\n",
    "                         a dictionary structure with JSON data.\n",
    "\n",
    "    Returns:\n",
    "        OpenAIJSONResponse: Parsed OpenAI JSON response model.\n",
    "    \"\"\"\n",
    "    return OpenAIJSONResponse(data=response)\n",
    "\n",
    "\n",
    "def parse_openai_tabular_response(response) -> OpenAITabularResponse:\n",
    "    \"\"\"\n",
    "    Parses the raw JSON response from OpenAI when expecting a tabular structure.\n",
    "\n",
    "    Args:\n",
    "        response (dict): JSON response from OpenAI API, expected to contain \"rows\"\n",
    "                         in a list-of-dictionaries format (one dictionary per row).\n",
    "\n",
    "    Returns:\n",
    "        OpenAITabularResponse: Parsed OpenAI tabular response model, which includes\n",
    "                               a method to convert rows to a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    return OpenAITabularResponse(rows=response)\n",
    "\n",
    "\n",
    "def parse_openai_code_response(response) -> OpenAICodeResponse:\n",
    "    \"\"\"\n",
    "    Parses the raw JSON response from OpenAI when expecting a code snippet.\n",
    "\n",
    "    Args:\n",
    "        response (dict): JSON response from OpenAI API, expected to contain a \"code\" key\n",
    "                         with generated code content, and optionally \"language\" and\n",
    "                         \"explanation\" keys.\n",
    "\n",
    "    Returns:\n",
    "        OpenAICodeResponse: Parsed OpenAI code response model.\n",
    "    \"\"\"\n",
    "    return OpenAICodeResponse(\n",
    "        code=response.get(\"code\"),\n",
    "        language=response.get(\"language\"),\n",
    "        explanation=response.get(\"explanation\"),\n",
    "    )\n",
    "\n",
    "\n",
    "# Claude parsing functions\n",
    "def parse_claude_text_response(response) -> ClaudeTextResponse:\n",
    "    \"\"\"\n",
    "    Parses the raw JSON response from Claude's text generation API.\n",
    "\n",
    "    Args:\n",
    "        response (dict): JSON response from Claude API, expected to contain a \"completion\"\n",
    "                         field with generated text.\n",
    "\n",
    "    Returns:\n",
    "        ClaudeTextResponse: Parsed Claude text response model.\n",
    "    \"\"\"\n",
    "    return ClaudeTextResponse(content=response[\"completion\"])\n",
    "\n",
    "\n",
    "def parse_claude_json_response(response) -> ClaudeJSONResponse:\n",
    "    \"\"\"\n",
    "    Parses the raw JSON response from Claude when expecting a JSON structure.\n",
    "\n",
    "    Args:\n",
    "        response (dict): JSON response directly from Claude API, expected to contain\n",
    "                         a dictionary structure with JSON data.\n",
    "\n",
    "    Returns:\n",
    "        ClaudeJSONResponse: Parsed Claude JSON response model.\n",
    "    \"\"\"\n",
    "    return ClaudeJSONResponse(data=response)\n",
    "\n",
    "\n",
    "def parse_claude_tabular_response(response) -> ClaudeTabularResponse:\n",
    "    \"\"\"\n",
    "    Parses the raw JSON response from Claude when expecting a tabular structure.\n",
    "\n",
    "    Args:\n",
    "        response (dict): JSON response from Claude API, expected to contain \"rows\"\n",
    "                         in a list-of-dictionaries format (one dictionary per row).\n",
    "\n",
    "    Returns:\n",
    "        ClaudeTabularResponse: Parsed Claude tabular response model, which includes\n",
    "                               a method to convert rows to a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    return ClaudeTabularResponse(rows=response)\n",
    "\n",
    "\n",
    "def parse_claude_code_response(response) -> ClaudeCodeResponse:\n",
    "    \"\"\"\n",
    "    Parses the raw JSON response from Claude when expecting a code snippet.\n",
    "\n",
    "    Args:\n",
    "        response (dict): JSON response from Claude API, expected to contain a \"code\" key\n",
    "                         with generated code content, and optionally \"language\" and\n",
    "                         \"explanation\" keys.\n",
    "\n",
    "    Returns:\n",
    "        ClaudeCodeResponse: Parsed Claude code response model.\n",
    "    \"\"\"\n",
    "    return ClaudeCodeResponse(\n",
    "        code=response.get(\"code\"),\n",
    "        language=response.get(\"language\"),\n",
    "        explanation=response.get(\"explanation\"),\n",
    "    )\n",
    "\n",
    "\n",
    "# Llama parsing functions\n",
    "def parse_llama_text_response(response) -> LlamaTextResponse:\n",
    "    \"\"\"\n",
    "    Parses the raw JSON response from Llama's text generation API.\n",
    "\n",
    "    Args:\n",
    "        response (dict): JSON response from Llama API, expected to contain a \"result\"\n",
    "                         field with generated text.\n",
    "\n",
    "    Returns:\n",
    "        LlamaTextResponse: Parsed Llama text response model.\n",
    "    \"\"\"\n",
    "    return LlamaTextResponse(result=response[\"result\"])\n",
    "\n",
    "\n",
    "def parse_llama_json_response(response) -> LlamaJSONResponse:\n",
    "    \"\"\"\n",
    "    Parses the raw JSON response from Llama when expecting a JSON structure.\n",
    "\n",
    "    Args:\n",
    "        response (dict): JSON response directly from Llama API, expected to contain\n",
    "                         a dictionary structure with JSON data.\n",
    "\n",
    "    Returns:\n",
    "        LlamaJSONResponse: Parsed Llama JSON response model.\n",
    "    \"\"\"\n",
    "    return LlamaJSONResponse(data=response)\n",
    "\n",
    "\n",
    "def parse_llama_tabular_response(response) -> LlamaTabularResponse:\n",
    "    \"\"\"\n",
    "    Parses the raw JSON response from Llama when expecting a tabular structure.\n",
    "\n",
    "    Args:\n",
    "        response (dict): JSON response from Llama API, expected to contain \"rows\"\n",
    "                         in a list-of-dictionaries format (one dictionary per row).\n",
    "\n",
    "    Returns:\n",
    "        LlamaTabularResponse: Parsed Llama tabular response model, which includes\n",
    "                              a method to convert rows to a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    return LlamaTabularResponse(rows=response)\n",
    "\n",
    "\n",
    "def parse_llama_code_response(response) -> LlamaCodeResponse:\n",
    "    \"\"\"\n",
    "    Parses the raw JSON response from Llama when expecting a code snippet.\n",
    "\n",
    "    Args:\n",
    "        response (dict): JSON response from Llama API, expected to contain a \"code\" key\n",
    "                         with generated code content, and optionally \"language\" and\n",
    "                         \"explanation\" keys.\n",
    "\n",
    "    Returns:\n",
    "        LlamaCodeResponse: Parsed Llama code response model.\n",
    "    \"\"\"\n",
    "    return LlamaCodeResponse(\n",
    "        code=response.get(\"code\"),\n",
    "        language=response.get(\"language\"),\n",
    "        explanation=response.get(\"explanation\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_response(provider: str, response_type: str, response: dict):\n",
    "    \"\"\"\n",
    "    Parses the raw JSON response into the appropriate provider-specific model.\n",
    "    \"\"\"\n",
    "    # Mapping of providers and response types to specific parsing functions\n",
    "    parsers = {\n",
    "        \"openai\": {\n",
    "            \"text\": parse_openai_text_response,\n",
    "            \"json\": parse_openai_json_response,\n",
    "            \"tabular\": parse_openai_tabular_response,\n",
    "            \"code\": parse_openai_code_response,\n",
    "        },\n",
    "        \"claude\": {\n",
    "            \"text\": parse_claude_text_response,\n",
    "            \"json\": parse_claude_json_response,\n",
    "            \"tabular\": parse_claude_tabular_response,\n",
    "            \"code\": parse_claude_code_response,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Call the parser function specific to the provider and response type\n",
    "        return parsers[provider][response_type](response)\n",
    "    except KeyError:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported provider or response type: {provider}, {response_type}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_response(provider: str, prompt: str, model: str, **kwargs) -> dict:\n",
    "    \"\"\"\n",
    "    Sends a prompt to the specified LLM provider and retrieves the raw JSON response.\n",
    "    \"\"\"\n",
    "    # Logic to choose provider and send API request\n",
    "    if provider == \"openai\":\n",
    "        response = call_openai_api(prompt, model, **kwargs)\n",
    "    elif provider == \"claude\":\n",
    "        response = call_claude_api(prompt, model, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "\n",
    "    return response  # Raw JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing Functions\n",
    "def parse_response(\n",
    "    provider: str, response_type: str, response: Mapping[str, Any]\n",
    ") -> Union[\n",
    "    OpenAITextResponse,\n",
    "    OpenAIJSONResponse,\n",
    "    OpenAITabularResponse,\n",
    "    OpenAICodeResponse,\n",
    "    ClaudeTextResponse,\n",
    "    ClaudeJSONResponse,\n",
    "    ClaudeTabularResponse,\n",
    "    ClaudeCodeResponse,\n",
    "    LlamaTextResponse,\n",
    "    LlamaJSONResponse,\n",
    "    LlamaTabularResponse,\n",
    "    LlamaCodeResponse,\n",
    "]:\n",
    "    \"\"\"\n",
    "    Parses raw JSON response from an LLM API based on provider and response type.\n",
    "\n",
    "    Args:\n",
    "        provider (str): The LLM provider (\"openai\", \"claude\", \"llama\").\n",
    "        response_type (str): The type of response expected (\"text\", \"json\", \"tabular\", \"code\").\n",
    "        response (Mapping[str, Any]): The raw response dictionary from the LLM API.\n",
    "\n",
    "    Returns:\n",
    "        Union[OpenAITextResponse, OpenAIJSONResponse, ...]: Parsed response model specific to the provider and response type.\n",
    "    \"\"\"\n",
    "    parsers = {\n",
    "        \"openai\": {\n",
    "            \"text\": lambda r: OpenAITextResponse(text=r.choices[0].message.content),\n",
    "            \"json\": lambda r: OpenAIJSONResponse(data=r),\n",
    "            \"tabular\": lambda r: OpenAITabularResponse(rows=r),\n",
    "            \"code\": lambda r: OpenAICodeResponse(\n",
    "                code=r.get(\"code\"),\n",
    "                language=r.get(\"language\"),\n",
    "                explanation=r.get(\"explanation\"),\n",
    "            ),\n",
    "        },\n",
    "        \"claude\": {\n",
    "            \"text\": lambda r: ClaudeTextResponse(content=r[\"completion\"]),\n",
    "            \"json\": lambda r: ClaudeJSONResponse(data=r),\n",
    "            \"tabular\": lambda r: ClaudeTabularResponse(rows=r),\n",
    "            \"code\": lambda r: ClaudeCodeResponse(\n",
    "                code=r.get(\"code\"),\n",
    "                language=r.get(\"language\"),\n",
    "                explanation=r.get(\"explanation\"),\n",
    "            ),\n",
    "        },\n",
    "        \"llama\": {\n",
    "            \"text\": lambda r: LlamaTextResponse(result=r[\"result\"]),\n",
    "            \"json\": lambda r: LlamaJSONResponse(data=r),\n",
    "            \"tabular\": lambda r: LlamaTabularResponse(rows=r),\n",
    "            \"code\": lambda r: LlamaCodeResponse(\n",
    "                code=r.get(\"code\"),\n",
    "                language=r.get(\"language\"),\n",
    "                explanation=r.get(\"explanation\"),\n",
    "            ),\n",
    "        },\n",
    "    }\n",
    "    try:\n",
    "        return parsers[provider][response_type](response)\n",
    "    except KeyError:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported provider or response type: {provider}, {response_type}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_2f():  # for now do not run this - skip!!!!\n",
    "    \"\"\"Prune responsibilities\"\"\"\n",
    "    logger.info(\n",
    "        \"Running pipeline 2e: prune resume responsibilities based on its alignment scores \\\n",
    "            with requirements\"\n",
    "    )\n",
    "\n",
    "    # Run pipeline\n",
    "\n",
    "    # File location of the mapping file (url: dir name: file names...)\n",
    "    mapping_file = ITERATE_0_OPENAI_DIR / mapping_file_name\n",
    "\n",
    "    #\n",
    "    elbow_curve_plot_file = ITERATE_0_OPENAI_DIR / elbow_curve_plot_file_name\n",
    "    elbow_method_specific_params = {\n",
    "        \"max_k\": 15,\n",
    "        \"S\": 12.0,\n",
    "        \"elbow_curve_plot_file\": elbow_curve_plot_file,\n",
    "    }\n",
    "    run_resume_pruning_pipeline(\n",
    "        mapping_file=str(mapping_file),\n",
    "        pruning_method=\"elbow\",\n",
    "        group_by_responsibility=False,\n",
    "        **elbow_method_specific_params,\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        \"Finished running pipeline 2e: prune resume responsibilities based on its alignment scores \\\n",
    "        with requirements.\"\n",
    "    )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function my_function at 0x0000027D100CDEE0>\n",
      "(a, b, c=None)\n",
      "1 2 None\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "\n",
    "def my_function(a, b, c=None):\n",
    "    print(a, b, c)\n",
    "\n",
    "\n",
    "func_name = \"my_function\"\n",
    "\n",
    "\n",
    "io_config = {\"a\": 1, \"b\": 2}\n",
    "\n",
    "\n",
    "func = globals()[func_name]\n",
    "\n",
    "print(func)\n",
    "\n",
    "\n",
    "func_signature = inspect.signature(func)\n",
    "print(func_signature)\n",
    "\n",
    "\n",
    "kwargs = io_config.copy()\n",
    "\n",
    "\n",
    "# Bind arguments to the signature\n",
    "\n",
    "\n",
    "bound = func_signature.bind(**kwargs)\n",
    "\n",
    "\n",
    "# Call the function with bound arguments\n",
    "\n",
    "\n",
    "func(*bound.args, **bound.kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Original JSON content\n",
    "json_file = r\"C:\\github\\job_bot\\input_output\\evaluation_optimization\\evaluation_optimization_by_anthropic\\iteration_1\\url_to_file_mapping.json\"\n",
    "with open(json_file, \"r\") as f:\n",
    "    json_content = json.load(f)\n",
    "\n",
    "# Update file paths\n",
    "updated_json_content = json_content.copy()\n",
    "\n",
    "for key, value in updated_json_content.items():\n",
    "    value[\"reqs\"] = value[\"reqs\"].replace(\n",
    "        \"evaluation_optimization_by_claude\", \"evaluation_optimization_by_anthropic\"\n",
    "    )\n",
    "    value[\"resps\"] = value[\"resps\"].replace(\n",
    "        \"evaluation_optimization_by_claude\", \"evaluation_optimization_by_anthropic\"\n",
    "    )\n",
    "    value[\"sim_metrics\"] = value[\"sim_metrics\"].replace(\n",
    "        \"evaluation_optimization_by_claude\", \"evaluation_optimization_by_anthropic\"\n",
    "    )\n",
    "    value[\"pruned_resps\"] = value[\"pruned_resps\"].replace(\n",
    "        \"evaluation_optimization_by_claude\", \"evaluation_optimization_by_anthropic\"\n",
    "    )\n",
    "\n",
    "# Show the updated content\n",
    "\n",
    "updated_json = json.dumps(updated_json_content, indent=4)\n",
    "with open(json_file, \"w\") as f:\n",
    "    f.write(updated_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 14:26:42,894 - preprocessing.preprocessing_utils - INFO - Checking for new urls and semi_new urls...\n",
      "2025-02-12 14:26:42,896 - utils.generic_utils - INFO - Loaded data from C:\\github\\job_bot\\input_output\\input\\job_posting_urls.json\n",
      "2025-02-12 14:26:42,898 - utils.generic_utils - INFO - Loaded data from C:\\github\\job_bot\\input_output\\preprocessing\\jobpostings.json\n",
      "2025-02-12 14:26:42,899 - utils.generic_utils - INFO - Loaded data from C:\\github\\job_bot\\input_output\\preprocessing\\extracted_job_requirements.json\n",
      "2025-02-12 14:26:42,900 - preprocessing.preprocessing_utils - INFO - No. of job posting urls: 21\n",
      "2025-02-12 14:26:42,902 - preprocessing.preprocessing_utils - INFO - No. of job descriptions urls: 22\n",
      "2025-02-12 14:26:42,902 - preprocessing.preprocessing_utils - INFO - No. of job requirements urls: 22\n",
      "2025-02-12 14:26:42,903 - preprocessing.preprocessing_utils - INFO - Identified 0 new URLs and 0 semi-new URLs.\n",
      "2025-02-12 14:26:42,904 - preprocessing.preprocessing_utils - INFO - Finished checking for new and semi-new urls.\n",
      "2025-02-12 14:26:42,905 - utils.generic_utils - INFO - Loaded data from C:\\github\\job_bot\\input_output\\preprocessing\\extracted_job_requirements.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x0000013EEC0F02B0>\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from project_config import (\n",
    "    JOB_REQUIREMENTS_JSON_FILE,\n",
    "    JOB_DESCRIPTIONS_JSON_FILE,\n",
    "    JOB_POSTING_URLS_FILE,\n",
    ")\n",
    "from preprocessing.preprocessing_utils import find_new_and_semi_new_urls\n",
    "from utils.generic_utils import read_from_json_file\n",
    "\n",
    "requirements = JOB_REQUIREMENTS_JSON_FILE\n",
    "descriptions = JOB_DESCRIPTIONS_JSON_FILE\n",
    "all_urls = JOB_POSTING_URLS_FILE\n",
    "\n",
    "print(file for file in [requirements, descriptions, all_urls])\n",
    "\n",
    "\n",
    "new_urls, semi_new_urls = find_new_and_semi_new_urls(\n",
    "    job_posting_urls_file=all_urls,\n",
    "    job_descriptions_file=descriptions,\n",
    "    job_requirements_file=requirements,\n",
    ")\n",
    "\n",
    "print(new_urls)\n",
    "\n",
    "print(semi_new_urls)\n",
    "\n",
    "urls = read_from_json_file(requirements)\n",
    "len(urls.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'clean_and_fix_json' from 'utils.generic_utils' (c:\\github\\job_bot\\src\\utils\\generic_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mproject_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JOB_DESCRIPTIONS_JSON_FILE\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clean_and_fix_json\n\u001b[0;32m      4\u001b[0m clean_and_fix_json(JOB_DESCRIPTIONS_JSON_FILE)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'clean_and_fix_json' from 'utils.generic_utils' (c:\\github\\job_bot\\src\\utils\\generic_utils.py)"
     ]
    }
   ],
   "source": [
    "from project_config import JOB_DESCRIPTIONS_JSON_FILE\n",
    "from utils.generic_utils import clean_and_fix_json\n",
    "\n",
    "clean_and_fix_json(JOB_DESCRIPTIONS_JSON_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
