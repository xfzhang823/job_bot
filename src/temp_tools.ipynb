{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 years of experience in management consulting, product management and strategy, or analytics in a technology company.\n",
      "Experience working with and analyzing data, and managing multiple cross-functional programs or projects.\n",
      "Experience with performing market analysis and developing competitive intelligence.\n",
      "Ability to manage executive stakeholders and communicate with a highly technical management team.\n",
      "Ability to form and refine hypotheses, gather supporting data, and make recommendations.\n",
      "Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\n"
     ]
    }
   ],
   "source": [
    "texts = [\"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\", \n",
    "        \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\",\n",
    "        \"Experience with performing market analysis and developing competitive intelligence\",\n",
    "        \"Ability to manage executive stakeholders and communicate with a highly technical management team\",\n",
    "        \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\",\n",
    "        \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"]\n",
    "\n",
    "texts = \".\\n\".join(text for text in texts)\n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    bert_score_precision  soft_similarity  word_movers_distance  \\\n",
      "0               0.833088         0.294945             15.905974   \n",
      "1               0.833400         0.315879             15.811388   \n",
      "2               0.836929         0.377884             15.874508   \n",
      "3               0.845272         0.264874             15.524175   \n",
      "4               0.831945         0.243683             15.716234   \n",
      "5               0.832256         0.294245             15.524175   \n",
      "6               0.830909         0.337065             15.905974   \n",
      "7               0.838344         0.215926             15.099669   \n",
      "8               0.832782         0.277579             15.524175   \n",
      "9               0.856216         0.354124             15.620499   \n",
      "10              0.827047         0.433190             15.459625   \n",
      "11              0.835676         0.253837             15.748016   \n",
      "12              0.846321         0.363054             15.491933   \n",
      "13              0.852224         0.191506             15.524175   \n",
      "14              0.816647         0.157734             15.620499   \n",
      "15              0.869484         0.262018             15.459625   \n",
      "16              0.823150         0.330459             15.716234   \n",
      "17              0.837198         0.360644             15.524175   \n",
      "18              0.820165         0.344930             15.716234   \n",
      "19              0.822814         0.287575             15.556349   \n",
      "20              0.821459         0.282235             15.620499   \n",
      "21              0.829722         0.210947             15.652476   \n",
      "22              0.842820         0.352504             15.748016   \n",
      "23              0.824793         0.309893             15.684387   \n",
      "24              0.865685         0.395633             15.427249   \n",
      "25              0.828627         0.275297             15.524175   \n",
      "26              0.850405         0.346306             15.132746   \n",
      "\n",
      "    nli_entailment_score  jaccard_similarity  \n",
      "0               0.049976            0.042424  \n",
      "1               0.042624            0.036364  \n",
      "2               0.004728            0.058140  \n",
      "3               0.048242            0.043478  \n",
      "4               0.003041            0.037500  \n",
      "5               0.002506            0.036810  \n",
      "6               0.014546            0.080247  \n",
      "7               0.009681            0.061350  \n",
      "8               0.005850            0.057325  \n",
      "9               0.030971            0.038710  \n",
      "10              0.026829            0.056604  \n",
      "11              0.001266            0.037500  \n",
      "12              0.039242            0.052980  \n",
      "13              0.070042            0.026667  \n",
      "14              0.051734            0.032680  \n",
      "15              0.171467            0.026667  \n",
      "16              0.087002            0.019231  \n",
      "17              0.019296            0.026316  \n",
      "18              0.008592            0.019231  \n",
      "19              0.010191            0.032051  \n",
      "20              0.009039            0.048485  \n",
      "21              0.010838            0.025316  \n",
      "22              0.022875            0.049689  \n",
      "23              0.005869            0.025157  \n",
      "24              0.001645            0.038462  \n",
      "25              0.004120            0.025806  \n",
      "26              0.055380            0.045455  \n",
      "MBA or graduate degree in a management, technical, or engineering field Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling 11 years of experience in management consulting, product management and strategy, or analytics in a technology company Experience working with and analyzing data, and managing multiple cross-functional programs or projects Experience with performing market analysis and developing competitive intelligence Ability to manage executive stakeholders and communicate with a highly technical management team Ability to form and refine hypotheses, gather supporting data, and make recommendations Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems English proficiency is a requirement for all roles unless stated otherwise in the job posting Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "\n",
    "f_path = r\"C:\\github\\job_bot\\data\\output_sim_matrix.csv\"\n",
    "df = pd.read_csv(f_path)\n",
    "df[\"Similarity Metrices\"] = df['Similarity Metrices'].apply(ast.literal_eval)\n",
    "\n",
    "metrics_df = pd.json_normalize(df['Similarity Metrices'])\n",
    "print(metrics_df)\n",
    "combined_df = pd.concat([df[[\"Responsibility\"]], metrics_df], axis=1)\n",
    "combined_df['Responsibility'] = combined_df['Responsibility'].str.replace('\\n', ' ')\n",
    "\n",
    "    # Ensure full display of DataFrame content\n",
    "pd.set_option(\"display.max_colwidth\", 100)  # Display full column content\n",
    "pd.set_option(\"display.max_rows\", None)  # Display all rows\n",
    "\n",
    "# Export the combined DataFrame to a CSV file\n",
    "# combined_df.to_csv(\"output_sim_matrix_cleaned.csv\", index=False)\n",
    "print(df.Requirements[1])\n",
    "# This will create a CSV file that you can open directly in Excel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\xzhan\\.cache\\huggingface\\hub\\models--roberta-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "nli_model_name = \"roberta-large-mnli\"\n",
    "\n",
    "# Re-download the model and tokenizer\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name, force_download=True)\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name, force_download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2819904685020447\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "premise = \"The candidate should have experience in developing machine learning models using Python.\"\n",
    "\n",
    "hypothesis = \"I have developed machine learning models using Python for data analysis projects.\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "nli_score = textsimilarity.nli_entailment_score(premise, hypothesis)\n",
    "\n",
    "print(nli_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Entailment Score: 0.614780604839325\n"
     ]
    }
   ],
   "source": [
    "premise = \"The company has launched a new machine learning model.\"\n",
    "hypothesis = \"The company is working on AI projects.\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "nli_score = textsimilarity.nli_entailment_score(hypothesis, premise)\n",
    "print(f\"NLI Entailment Score: {nli_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Entailment Score: 0.0010548133868724108\n"
     ]
    }
   ],
   "source": [
    "hypothesis = \"The company has launched a new machine learning model.\"\n",
    "premise = \"The company is working on AI projects.\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "nli_score = textsimilarity.nli_entailment_score(hypothesis, premise)\n",
    "print(f\"NLI Entailment Score: {nli_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Entailment Score: 0.11766134947538376\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "premise = \"She graduated with a degree in computer science and has 5 years of experience in software development.\"\n",
    "hypothesis = \"She is prepared for a software engineering role.\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "nli_score = textsimilarity.deberta_entailment_score(hypothesis, premise)\n",
    "print(f\"NLI Entailment Score: {nli_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment Score: 0.0016027865931391716\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "hypothesis = \"Enhanced data quality and consistency by integrating thorough financial analysis, standardizing methodologies, and conducting in-depth vendor engagements.\"\n",
    "premise = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "deberta_score = textsimilarity.deberta_entailment_score(hypothesis, premise)\n",
    "print(f\"Entailment Score: {deberta_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment Score: 0.4864349961280823\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "premise = \"Enhanced data quality and consistency by integrating thorough financial analysis, standardizing methodologies, and conducting in-depth vendor engagements.\"\n",
    "hypothesis = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "deberta_score = textsimilarity.deberta_entailment_score(hypothesis, premise)\n",
    "print(f\"Entailment Score: {deberta_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment Score: 0.025084542110562325\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "premise = \"Collaborated with the engineering services research team to pioneer the engineering services tracker, authored influential publications on market forecasts, the impact of COVID-19 on services, and trends in M&A within the engineering services industry.\"\n",
    "hypothesis = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "deberta_score = textsimilarity.deberta_entailment_score(hypothesis, premise)\n",
    "print(f\"Entailment Score: {deberta_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Categorized Scores:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'bert_score_precision': 'High',\n",
       " 'soft_similarity': 'Medium',\n",
       " 'word_movers_distance': 'High',\n",
       " 'nli_entailment_score': 'Low',\n",
       " 'jaccard_similarity': 'High',\n",
       " 'deberta_entailment_score': 'Medium'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# metrics_evaluator.py\n",
    "\n",
    "# Define the metric criteria in a dictionary\n",
    "METRIC_CRITERIA = {\n",
    "    # \"BERTScore Precision\"\n",
    "    \"bert_score_precision\": {\n",
    "        \"range\": (0, 1),\n",
    "        \"high_threshold\": 0.85,\n",
    "        \"low_threshold\": 0.70,\n",
    "    },\n",
    "    # \"Soft Similarity (SBERT)\"\n",
    "    \"soft_similarity\": {\n",
    "        \"range\": (-1, 1),\n",
    "        \"high_threshold\": 0.7,\n",
    "        \"low_threshold\": 0.4,\n",
    "    },\n",
    "    # \"Word Mover's Distance\"\n",
    "    \"word_movers_distance\": {\n",
    "        \"range\": (0, float(\"inf\")),\n",
    "        \"high_threshold\": 5,  # High score is considered \"Low\" for this metric\n",
    "        \"low_threshold\": 15,  # Low score is considered \"High\" for this metric\n",
    "        \"reverse\": True,  # Indicates smaller scores are better\n",
    "    },\n",
    "    # \"NLI Entailment Score\"\n",
    "    \"nli_entailment_score\": {\n",
    "        \"range\": (0, 1),\n",
    "        \"high_threshold\": 0.7,\n",
    "        \"low_threshold\": 0.3,\n",
    "    },\n",
    "    # \"Jaccard Similarity\"\n",
    "    \"jaccard_similarity\": {\n",
    "        \"range\": (0, 1),\n",
    "        \"high_threshold\": 0.5,\n",
    "        \"low_threshold\": 0.2,\n",
    "    },\n",
    "    # \"DeBERTa Entailment Score\"\n",
    "    \"deberta_entailment_score\": {\n",
    "        \"range\": (0, 1),\n",
    "        \"high_threshold\": 0.8,\n",
    "        \"low_threshold\": 0.4,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def evaluate_score(metric_name, score):\n",
    "    \"\"\"\n",
    "    Evaluate the score for a given metric name.\n",
    "\n",
    "    Args:\n",
    "        metric_name (str): The name of the metric.\n",
    "        score (float): The score to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        str: The category of the score (\"High\", \"Medium\", \"Low\").\n",
    "    \"\"\"\n",
    "    if metric_name not in METRIC_CRITERIA:\n",
    "        raise ValueError(f\"Metric '{metric_name}' is not defined in the criteria.\")\n",
    "\n",
    "    criteria = METRIC_CRITERIA[metric_name]\n",
    "    high_threshold = criteria[\"high_threshold\"]\n",
    "    low_threshold = criteria[\"low_threshold\"]\n",
    "    reverse = criteria.get(\"reverse\", False)\n",
    "\n",
    "    # For metrics where a lower score is better (like Word Mover's Distance)\n",
    "    if reverse:\n",
    "        if score <= high_threshold:\n",
    "            return \"High\"\n",
    "        elif score >= low_threshold:\n",
    "            return \"Low\"\n",
    "    else:\n",
    "        if score >= high_threshold:\n",
    "            return \"High\"\n",
    "        elif score <= low_threshold:\n",
    "            return \"Low\"\n",
    "\n",
    "    return \"Medium\"\n",
    "\n",
    "\n",
    "def categorize_scores(scores):\n",
    "    \"\"\"\n",
    "    Categorize a dictionary of scores based on their metric names.\n",
    "\n",
    "    Args:\n",
    "        scores (dict): A dictionary where keys are metric names and values are scores.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the same keys but with \"High\", \"Medium\", or \"Low\" as values.\n",
    "    \"\"\"\n",
    "    categorized_scores = {}\n",
    "    for metric_name, score in scores.items():\n",
    "        try:\n",
    "            categorized_scores[metric_name] = evaluate_score(metric_name, score)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            categorized_scores[metric_name] = \"Unknown\"\n",
    "    return categorized_scores\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    example_scores = {\n",
    "        \"bert_score_precision\": 0.88,\n",
    "        \"soft_similarity\": 0.45,\n",
    "        \"word_movers_distance\": 4.2,\n",
    "        \"nli_entailment_score\": 0.2,\n",
    "        \"jaccard_similarity\": 0.6,\n",
    "        \"deberta_entailment_score\": 0.75,\n",
    "    }\n",
    "\n",
    "    categorized = categorize_scores(example_scores)\n",
    "    display(\"Categorized Scores:\", categorized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing OpenAI / Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, here's one for you:\\n\\nWhy couldn't the bicycle find its way home?\\n\\nBecause it lost its bearings!\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "opeanai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = opeanai_api_key\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"tell me a joke.\"},\n",
    "    ],\n",
    "    temperature=0.2,\n",
    "    max_tokens=200,  # Ensure sufficient tokens for response\n",
    ")\n",
    "\n",
    "response_text = response.choices[0].message.content\n",
    "response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Responsibilities Edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:47:48,608 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:47:50,700 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:47:50,704 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:47:50,706 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': 1, 'optimized_text': 'Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.'}\n",
      "Results updated: \n",
      "{'resp_id': 1, 'optimized_text': 'Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:47:50,709 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': 1, 'optimized_text': 'Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'resp_id': 1,\n",
       " 'optimized_text': 'Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editing import edit_text_for_dp, edit_text_for_semantic_entailment\n",
    "\n",
    "resp_text = \"Provided strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\"\n",
    "\n",
    "reqs_text1 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "reqs_text2 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "reqs_text3 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "api_key = get_openai_api_key()\n",
    "client = OpenAI(api_key=api_key)  # Instantiate openai api chat completion class\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "gpt4t = \"gpt-4-turbo\"\n",
    "\n",
    "\n",
    "revised_text = edit_text_for_semantic_entailment(\n",
    "    client=client, \n",
    "    text_id=1, \n",
    "    candidate_text=resp_text, \n",
    "    model_id=gpt4t, \n",
    "    reference_text=reqs_text3)\n",
    "\n",
    "revised_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 15:35:47,560 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Formatted Prompt in edit_text_for_dp:\n",
      "\n",
      "You are a skilled professional at writing resumes. Please perform the following tasks:\n",
      "\n",
      "1. Analyze the **source text** at a high level.\n",
      "\n",
      "2. Apply the \"\"source text's** dependency structure to the **target text**. Ensure that the original meaning of the **target text** is mostly preserved.\n",
      "\n",
      "**Source Text:**\n",
      "\"Provided strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\"\n",
      "\n",
      "**Target Text:**\n",
      "\"Leveraged strategic insights to optimize the service partner ecosystem in Asia Pacific for a major global IT vendor, enhancing local implementation outcomes.\"\n",
      "\n",
      "**Return Format:**\n",
      "Please return the result in JSON format as follows:\n",
      "\n",
      "{\n",
      "  \"optimized_text\": \"Edited version of the target text\"\n",
      "}\n",
      "\n",
      "Do not include any additional text or explanations.\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 15:35:49,943 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 15:35:49,947 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 15:35:49,949 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': '1', 'optimized_text': 'Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.'}\n",
      "Results updated: \n",
      "{'resp_id': '1', 'optimized_text': 'Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 15:35:49,950 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': '1', 'optimized_text': 'Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "source text:\n",
      "Provided strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\n",
      "\n",
      "target text:\n",
      "Leveraged strategic insights to optimize the service partner ecosystem in Asia Pacific for a major global IT vendor, enhancing local implementation outcomes.\n",
      "\n",
      "Optimized Text Result: \n",
      "Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editing import edit_text_for_dp, edit_text_for_semantic_entailment\n",
    "\n",
    "# Sample Source and Target Texts\n",
    "source_text = \"Provided strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\"\n",
    "target_text = \"Leveraged strategic insights to optimize the service partner ecosystem in Asia Pacific for a major global IT vendor, enhancing local implementation outcomes.\"\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "api_key = get_openai_api_key()\n",
    "client = OpenAI(api_key=api_key)  # Instantiate openai api chat completion class\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4\"\n",
    "gpt4t = \"gpt-4-turbo\"\n",
    "\n",
    "# Call the edit_text_for_dp function with the sample texts\n",
    "try:\n",
    "    result = edit_text_for_dp(\n",
    "        client=client,  # Your OpenAI client instance\n",
    "        text_id=\"1\",  # Just an example ID\n",
    "        target_text=target_text,\n",
    "        source_text=source_text,\n",
    "        model_id=\"gpt-4\",  # Example model ID\n",
    "        max_tokens=1056  # Example max token limit\n",
    "    )\n",
    "    print(f\"\\nsource text:\\n{source_text}\")\n",
    "    print(f\"\\ntarget text:\\n{target_text}\")\n",
    "    print(f\"\\nOptimized Text Result: \\n{result['optimized_text']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during testing: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longer sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just Semantic & Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:11:38,282 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:11:44,505 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:11:44,508 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:11:44,511 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': 'ad95243c-4c69-4bf1-97cb-065b8672f538', 'optimized_text': 'Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.'}\n",
      "Results updated: \n",
      "{'resp_id': 'ad95243c-4c69-4bf1-97cb-065b8672f538', 'optimized_text': 'Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:11:44,512 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': 'ad95243c-4c69-4bf1-97cb-065b8672f538', 'optimized_text': 'Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidate Text:\n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n",
      "\n",
      "Reference Text:\n",
      "Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editing import edit_text_for_dp, edit_text_for_semantic_entailment\n",
    "\n",
    "\n",
    "text_1 = \"MBA or graduate degree in a management, technical, or engineering field\"\n",
    "text_2 = \"Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling\"\n",
    "text_3 = \"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\"\n",
    "text_4 = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "text_5 = \"Experience with performing market analysis and developing competitive intelligence\"\n",
    "text_6 = \"Ability to manage executive stakeholders and communicate with a highly technical management team\"\n",
    "text_7 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "text_8 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "text_9 = \"English proficiency is a requirement for all roles unless stated otherwise in the job posting\"\n",
    "text_10 = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "text_11 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "text_12 = \"Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\"\n",
    "\n",
    "original_text = \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
    "\n",
    "\n",
    "candidate = original_text\n",
    "reference = text_12\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "api_key = get_openai_api_key()\n",
    "client = OpenAI(api_key=api_key)  # Instantiate openai api chat completion class\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "\n",
    "revised = edit_text_for_semantic_entailment(\n",
    "    client=client,  \n",
    "    candidate_text=candidate, \n",
    "    reference_text=reference,\n",
    "    model_id=gpt4t, \n",
    "    temperature=0.6)\n",
    "\n",
    "revised_text = revised[\"optimized_text\"]\n",
    "\n",
    "print(f\"\\nCandidate Text:\\n{candidate}\")\n",
    "print(f\"\\nReference Text:\\n{reference}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:03,008 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:07,445 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:07,448 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:07,451 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': '5d15e5aa-5394-4ec8-9610-242f1f147be9', 'optimized_text': 'Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.'}\n",
      "Results updated: \n",
      "{'resp_id': '5d15e5aa-5394-4ec8-9610-242f1f147be9', 'optimized_text': 'Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:07,452 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': '5d15e5aa-5394-4ec8-9610-242f1f147be9', 'optimized_text': 'Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Formatted Prompt in edit_text_for_dp:\n",
      "\n",
      "You are a skilled professional at writing resumes. Please perform the following tasks:\n",
      "\n",
      "1. Analyze the **source text** at a high level.\n",
      "\n",
      "2. Apply the \"\"source text's** dependency structure to the **target text**. Ensure that the original meaning of the **target text** is mostly preserved.\n",
      "\n",
      "**Source Text:**\n",
      "\"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
      "\n",
      "**Target Text:**\n",
      "\"Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "\n",
      "**Return Format:**\n",
      "Please return the result in JSON format as follows:\n",
      "\n",
      "{\n",
      "  \"optimized_text\": \"Edited version of the target text\"\n",
      "}\n",
      "\n",
      "Do not include any additional text or explanations.\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:11,194 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:11,196 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:11,199 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': '8e0249d3-37f4-4818-aa8d-97751838b6f3', 'optimized_text': 'Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.'}\n",
      "Results updated: \n",
      "{'resp_id': '8e0249d3-37f4-4818-aa8d-97751838b6f3', 'optimized_text': 'Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:11,200 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': '8e0249d3-37f4-4818-aa8d-97751838b6f3', 'optimized_text': 'Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidate Text:\n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n",
      "\n",
      "Reference Text:\n",
      "Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editing import edit_text_for_dp, edit_text_for_semantic_entailment\n",
    "\n",
    "# Texts\n",
    "text_1 = \"MBA or graduate degree in a management, technical, or engineering field\"\n",
    "text_2 = \"Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling\"\n",
    "text_3 = \"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\"\n",
    "text_4 = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "text_5 = \"Experience with performing market analysis and developing competitive intelligence\"\n",
    "text_6 = \"Ability to manage executive stakeholders and communicate with a highly technical management team\"\n",
    "text_7 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "text_8 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "text_9 = \"English proficiency is a requirement for all roles unless stated otherwise in the job posting\"\n",
    "text_10 = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "text_11 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "text_12 = \"Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\"\n",
    "\n",
    "original_text = \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "api_key = get_openai_api_key()\n",
    "client = OpenAI(api_key=api_key)  # Instantiate openai api chat completion class\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "\n",
    "\n",
    "# Step 1: Align Semantic & Entailment\n",
    "candidate = original_text\n",
    "reference = text_12\n",
    "\n",
    "revised = edit_text_for_semantic_entailment(\n",
    "    client=client,  \n",
    "    candidate_text=candidate, \n",
    "    reference_text=reference,\n",
    "    model_id=gpt4t, \n",
    "    temperature=0.6)\n",
    "\n",
    "revised_text = revised[\"optimized_text\"]\n",
    "\n",
    "# Step 2: Align Original Sentence's DP\n",
    "target = revised_text\n",
    "source = original_text\n",
    "\n",
    "final = edit_text_for_dp(client=client,\n",
    "                         target_text=target, \n",
    "                         source_text=source, \n",
    "                         model_id=gpt4t, \n",
    "                         temperature=0.8)\n",
    "\n",
    "final_text = final['optimized_text']\n",
    "\n",
    "\n",
    "# Print\n",
    "print(f\"\\nCandidate Text:\\n{candidate}\")\n",
    "print(f\"\\nReference Text:\\n{reference}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{final_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Dependency Parsing Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:02,631 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:05,966 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:05,968 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:05,971 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': '4e77b7d4-2e89-418d-9bd1-725b2fb51f3f', 'optimized_text': 'Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.'}\n",
      "Results updated: \n",
      "{'resp_id': '4e77b7d4-2e89-418d-9bd1-725b2fb51f3f', 'optimized_text': 'Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:05,972 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': '4e77b7d4-2e89-418d-9bd1-725b2fb51f3f', 'optimized_text': 'Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Formatted Prompt in edit_text_for_dp:\n",
      "\n",
      "You are a skilled professional at writing resumes. Please perform the following tasks:\n",
      "\n",
      "1. Analyze the **source text** at a high level.\n",
      "\n",
      "2. Apply the \"\"source text's** dependency structure to the **target text**. Ensure that the original meaning of the **target text** is mostly preserved.\n",
      "\n",
      "**Source Text:**\n",
      "\"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
      "\n",
      "**Target Text:**\n",
      "\"Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.\"\n",
      "\n",
      "**Return Format:**\n",
      "Please return the result in JSON format as follows:\n",
      "\n",
      "{\n",
      "  \"optimized_text\": \"Edited version of the target text\"\n",
      "}\n",
      "\n",
      "Do not include any additional text or explanations.\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:08,998 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:09,000 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:09,002 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': '5bce9c3d-ff64-44b6-9ddd-52c2a8769499', 'optimized_text': 'Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.'}\n",
      "Results updated: \n",
      "{'resp_id': '5bce9c3d-ff64-44b6-9ddd-52c2a8769499', 'optimized_text': 'Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:09,003 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': '5bce9c3d-ff64-44b6-9ddd-52c2a8769499', 'optimized_text': 'Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidate Text:\n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n",
      "\n",
      "Reference Text:\n",
      "Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editing import edit_text_for_semantic_entailment_llama3\n",
    "\n",
    "# Texts\n",
    "text_1 = \"MBA or graduate degree in a management, technical, or engineering field\"\n",
    "text_2 = \"Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling\"\n",
    "text_3 = \"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\"\n",
    "text_4 = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "text_5 = \"Experience with performing market analysis and developing competitive intelligence\"\n",
    "text_6 = \"Ability to manage executive stakeholders and communicate with a highly technical management team\"\n",
    "text_7 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "text_8 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "text_9 = \"English proficiency is a requirement for all roles unless stated otherwise in the job posting\"\n",
    "text_10 = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "text_11 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "text_12 = \"Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\"\n",
    "\n",
    "original_text = \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "api_key = get_openai_api_key()\n",
    "client = OpenAI(api_key=api_key)  # Instantiate openai api chat completion class\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "\n",
    "\n",
    "# Step 1: Align Semantic & Entailment\n",
    "candidate = original_text\n",
    "reference = text_12\n",
    "\n",
    "revised = edit_text_for_semantic_entailment(\n",
    "    client=client,  \n",
    "    candidate_text=candidate, \n",
    "    reference_text=reference,\n",
    "    model_id=gpt4t, \n",
    "    temperature=0.6)\n",
    "\n",
    "revised_text = revised[\"optimized_text\"]\n",
    "\n",
    "# Step 2: Align Original Sentence's DP\n",
    "target = revised_text\n",
    "source = original_text\n",
    "\n",
    "final = edit_text_for_dp(client=client,\n",
    "                         target_text=target, \n",
    "                         source_text=source, \n",
    "                         model_id=gpt4t, \n",
    "                         temperature=0.8)\n",
    "\n",
    "final_text = final['optimized_text']\n",
    "\n",
    "\n",
    "# Print\n",
    "print(f\"\\nCandidate Text:\\n{candidate}\")\n",
    "print(f\"\\nReference Text:\\n{reference}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{final_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:30,270 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:33,487 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:33,490 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:33,492 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': '035476c0-63f1-4357-86c8-200f9aec746d', 'optimized_text': 'Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.'}\n",
      "Results updated: \n",
      "{'text_id': '035476c0-63f1-4357-86c8-200f9aec746d', 'optimized_text': 'Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:33,495 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': '035476c0-63f1-4357-86c8-200f9aec746d', 'optimized_text': 'Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:36,996 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:36,998 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:37,002 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': '70fa76ac-430b-4cd2-bfde-5ebb770d900f', 'optimized_text': 'Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.'}\n",
      "Results updated: \n",
      "{'text_id': '70fa76ac-430b-4cd2-bfde-5ebb770d900f', 'optimized_text': 'Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:37,003 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': '70fa76ac-430b-4cd2-bfde-5ebb770d900f', 'optimized_text': 'Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:40,340 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:40,343 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:40,346 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': 'a4ef3843-19d5-4667-9869-4d5c3850a206', 'optimized_text': 'Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.'}\n",
      "Results updated: \n",
      "{'text_id': 'a4ef3843-19d5-4667-9869-4d5c3850a206', 'optimized_text': 'Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:40,347 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': 'a4ef3843-19d5-4667-9869-4d5c3850a206', 'optimized_text': 'Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text:\n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n",
      "\n",
      "Compared to Text:\n",
      "Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n",
      "\n",
      "Optimized Text Result: \n",
      "Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.\n",
      "\n",
      "Optimized Text Result: \n",
      "Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editor import TextEditor\n",
    "\n",
    "# Texts\n",
    "text_1 = \"MBA or graduate degree in a management, technical, or engineering field\"\n",
    "text_2 = \"Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling\"\n",
    "text_3 = \"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\"\n",
    "text_4 = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "text_5 = \"Experience with performing market analysis and developing competitive intelligence\"\n",
    "text_6 = \"Ability to manage executive stakeholders and communicate with a highly technical management team\"\n",
    "text_7 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "text_8 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "text_9 = \"English proficiency is a requirement for all roles unless stated otherwise in the job posting\"\n",
    "text_10 = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "text_11 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "text_12 = \"Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\"\n",
    "\n",
    "original = \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
    "requirement = text_12\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "\n",
    "# Instantiate TextEditor class\n",
    "text_editor = TextEditor(model=\"openai\", model_id=gpt4, max_tokens=512)\n",
    "\n",
    "\n",
    "# Step 1: Align Semantic \n",
    "candidate = original\n",
    "reference = requirement\n",
    "\n",
    "revised = text_editor.edit_for_semantics(\n",
    "    candidate_text=candidate, \n",
    "    reference_text=reference,\n",
    "    temperature=0.4)\n",
    "\n",
    "revised_text_1 = revised[\"optimized_text\"]\n",
    "\n",
    "# Step 2: Allign Entailment\n",
    "premise = revised_text_1\n",
    "hypothesis = requirement\n",
    "\n",
    "revised = text_editor.edit_for_entailment(\n",
    "                         premise_text=premise, \n",
    "                         hypothesis_text=hypothesis,\n",
    "                         temperature=0.6)\n",
    "\n",
    "revised_text_2 = revised['optimized_text']\n",
    "\n",
    "# Step 3: Align Original Sentence's DP\n",
    "target = revised_text_2\n",
    "source = original\n",
    "\n",
    "revised = text_editor.edit_for_dp(\n",
    "                         target_text=target, \n",
    "                         source_text=source, \n",
    "                         temperature=0.8)\n",
    "\n",
    "revised_text_3 = revised['optimized_text']\n",
    "\n",
    "\n",
    "# Print\n",
    "print(f\"\\nOriginal Text:\\n{original}\")\n",
    "print(f\"\\nCompared to Text:\\n{requirement}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_1}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_2}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:27,773 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:29,101 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:29,104 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:29,105 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': '0d79593a-af95-4a68-99f6-2d309ae3cc2f', 'optimized_text': 'Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.'}\n",
      "Results updated: \n",
      "{'text_id': '0d79593a-af95-4a68-99f6-2d309ae3cc2f', 'optimized_text': 'Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:29,106 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': '0d79593a-af95-4a68-99f6-2d309ae3cc2f', 'optimized_text': 'Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:30,206 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:30,209 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:30,212 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': '5c78fdb4-854a-40d2-bc17-f62213610763', 'optimized_text': 'Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.'}\n",
      "Results updated: \n",
      "{'text_id': '5c78fdb4-854a-40d2-bc17-f62213610763', 'optimized_text': 'Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:30,213 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': '5c78fdb4-854a-40d2-bc17-f62213610763', 'optimized_text': 'Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:31,466 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:31,469 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:31,471 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': 'ebaba7f3-a403-4566-8ac7-f1f4dc4e31c6', 'optimized_text': 'Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.'}\n",
      "Results updated: \n",
      "{'text_id': 'ebaba7f3-a403-4566-8ac7-f1f4dc4e31c6', 'optimized_text': 'Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:31,473 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': 'ebaba7f3-a403-4566-8ac7-f1f4dc4e31c6', 'optimized_text': 'Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text:\n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n",
      "\n",
      "Compared to Text:\n",
      "Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n",
      "\n",
      "Optimized Text Result: \n",
      "Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.\n",
      "\n",
      "Optimized Text Result: \n",
      "Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editor import TextEditor\n",
    "\n",
    "# Texts\n",
    "text_1 = \"MBA or graduate degree in a management, technical, or engineering field\"\n",
    "text_2 = \"Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling\"\n",
    "text_3 = \"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\"\n",
    "text_4 = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "text_5 = \"Experience with performing market analysis and developing competitive intelligence\"\n",
    "text_6 = \"Ability to manage executive stakeholders and communicate with a highly technical management team\"\n",
    "text_7 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "text_8 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "text_9 = \"English proficiency is a requirement for all roles unless stated otherwise in the job posting\"\n",
    "text_10 = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "text_11 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "text_12 = \"Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\"\n",
    "\n",
    "original = \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
    "requirement = text_12\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "\n",
    "# Instantiate TextEditor class\n",
    "text_editor = TextEditor(model=\"openai\", model_id=gpt3, max_tokens=512)\n",
    "\n",
    "\n",
    "# Step 1: Align Semantic \n",
    "candidate = original\n",
    "reference = requirement\n",
    "\n",
    "revised = text_editor.edit_for_semantics(\n",
    "    candidate_text=candidate, \n",
    "    reference_text=reference,\n",
    "    temperature=0.5)\n",
    "\n",
    "revised_text_1 = revised[\"optimized_text\"]\n",
    "\n",
    "# Step 2: Allign Entailment\n",
    "premise = revised_text_1\n",
    "hypothesis = requirement\n",
    "\n",
    "revised = text_editor.edit_for_entailment(\n",
    "                         premise_text=premise, \n",
    "                         hypothesis_text=hypothesis,\n",
    "                         temperature=0.6)\n",
    "\n",
    "revised_text_2 = revised['optimized_text']\n",
    "\n",
    "# Step 3: Align Original Sentence's DP\n",
    "target = revised_text_2\n",
    "source = original\n",
    "\n",
    "revised = text_editor.edit_for_dp(\n",
    "                         target_text=target, \n",
    "                         source_text=source, \n",
    "                         temperature=0.9)\n",
    "\n",
    "revised_text_3 = revised['optimized_text']\n",
    "\n",
    "\n",
    "# Print\n",
    "print(f\"\\nOriginal Text:\\n{original}\")\n",
    "print(f\"\\nCompared to Text:\\n{requirement}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_1}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_2}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:00,148 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:19,460 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:19,462 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': 'cfeae29e-475e-42d0-92ca-8817b5869f3f', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), and industry trends. Developed actionable recommendations through reports, blogs, presentations, and custom studies.'}\n",
      "Results updated: \n",
      "{'text_id': 'cfeae29e-475e-42d0-92ca-8817b5869f3f', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), and industry trends. Developed actionable recommendations through reports, blogs, presentations, and custom studies.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:19,463 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': 'cfeae29e-475e-42d0-92ca-8817b5869f3f', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), and industry trends. Developed actionable recommendations through reports, blogs, presentations, and custom studies.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:35,382 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:35,383 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': '50978914-33be-4817-98c4-c06fae43c0eb', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies.'}\n",
      "Results updated: \n",
      "{'text_id': '50978914-33be-4817-98c4-c06fae43c0eb', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:35,385 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': '50978914-33be-4817-98c4-c06fae43c0eb', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:59,118 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:59,120 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': '0cd881f0-9b4d-4925-ac27-fdf9cd19550a', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies, authored in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.'}\n",
      "Results updated: \n",
      "{'text_id': '0cd881f0-9b4d-4925-ac27-fdf9cd19550a', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies, authored in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:59,121 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': '0cd881f0-9b4d-4925-ac27-fdf9cd19550a', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies, authored in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text:\n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n",
      "\n",
      "Compared to Text:\n",
      "Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n",
      "\n",
      "Optimized Text Result: \n",
      "Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), and industry trends. Developed actionable recommendations through reports, blogs, presentations, and custom studies.\n",
      "\n",
      "Optimized Text Result: \n",
      "Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies.\n",
      "\n",
      "Optimized Text Result: \n",
      "Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies, authored in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editor import TextEditor\n",
    "\n",
    "# Texts\n",
    "text_1 = \"MBA or graduate degree in a management, technical, or engineering field\"\n",
    "text_2 = \"Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling\"\n",
    "text_3 = \"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\"\n",
    "text_4 = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "text_5 = \"Experience with performing market analysis and developing competitive intelligence\"\n",
    "text_6 = \"Ability to manage executive stakeholders and communicate with a highly technical management team\"\n",
    "text_7 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "text_8 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "text_9 = \"English proficiency is a requirement for all roles unless stated otherwise in the job posting\"\n",
    "text_10 = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "text_11 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "text_12 = \"Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\"\n",
    "\n",
    "original = \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
    "requirement = text_12\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "\n",
    "# Instantiate TextEditor class\n",
    "text_editor = TextEditor(model=\"llama3\")\n",
    "\n",
    "\n",
    "# Step 1: Align Semantic \n",
    "candidate = original\n",
    "reference = requirement\n",
    "\n",
    "revised = text_editor.edit_for_semantics(\n",
    "    candidate_text=candidate, \n",
    "    reference_text=reference,\n",
    "    temperature=0.8)\n",
    "\n",
    "revised_text_1 = revised[\"optimized_text\"]\n",
    "\n",
    "# Step 2: Allign Entailment\n",
    "premise = revised_text_1\n",
    "hypothesis = requirement\n",
    "\n",
    "revised = text_editor.edit_for_entailment(\n",
    "                         premise_text=premise, \n",
    "                         hypothesis_text=hypothesis,\n",
    "                         temperature=0.6)\n",
    "\n",
    "revised_text_2 = revised['optimized_text']\n",
    "\n",
    "# Step 3: Align Original Sentence's DP\n",
    "target = revised_text_2\n",
    "source = original\n",
    "\n",
    "revised = text_editor.edit_for_dp(\n",
    "                         target_text=target, \n",
    "                         source_text=source, \n",
    "                         temperature=0.9)\n",
    "\n",
    "revised_text_3 = revised['optimized_text']\n",
    "\n",
    "\n",
    "# Print\n",
    "print(f\"\\nOriginal Text:\\n{original}\")\n",
    "print(f\"\\nCompared to Text:\\n{requirement}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_1}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_2}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEb Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     39\u001b[0m urls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://jobs.careers.microsoft.com/us/en/job/1771714/Head-of-Partner-Intelligence-and-Strategy?jobsource=linkedin\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 40\u001b[0m page_contents \u001b[38;5;241m=\u001b[39m \u001b[43mload_text_from_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43murls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(page_contents)\n",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m, in \u001b[0;36mload_text_from_pages\u001b[1;34m(urls)\u001b[0m\n\u001b[0;32m      5\u001b[0m content_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Start Playwright and launch the browser\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msync_playwright\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchromium\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheadless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Launch headless browser\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Create a new browser page\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\sync_api\\_context_manager.py:47\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_own_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m---> 47\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Error(\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"It looks like you are using Playwright Sync API inside the asyncio loop.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03mPlease use the Async API instead.\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m             )\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;66;03m# Create a new fiber for the protocol dispatcher. It will be pumping events\u001b[39;00m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;66;03m# until the end of times. We will pass control to that fiber every time we\u001b[39;00m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m# block while waiting for a response.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgreenlet_main\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mError\u001b[0m: It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead."
     ]
    }
   ],
   "source": [
    "from playwright.sync_api import sync_playwright\n",
    "\n",
    "def load_text_from_pages(urls):\n",
    "    # Initialize an empty dictionary to store the content\n",
    "    content_dict = {}\n",
    "\n",
    "    # Start Playwright and launch the browser\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)  # Launch headless browser\n",
    "        page = browser.new_page()  # Create a new browser page\n",
    "\n",
    "        # Loop through the list of URLs\n",
    "        for url in urls:\n",
    "            try:\n",
    "                # Navigate to the URL\n",
    "                page.goto(url)\n",
    "                \n",
    "                # Wait for the page to fully load\n",
    "                page.wait_for_load_state('networkidle')\n",
    "\n",
    "                # Get the text content of the entire page\n",
    "                content = page.inner_text('body')  # Extract text from <body>\n",
    "\n",
    "                # Store the content in the dictionary with the URL as the key\n",
    "                content_dict[url] = content\n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle errors (e.g., failed to load page)\n",
    "                print(f\"Failed to load {url}: {e}\")\n",
    "                content_dict[url] = None  # Store None for failed pages\n",
    "\n",
    "        # Close the browser after processing all URLs\n",
    "        browser.close()\n",
    "\n",
    "    # Return the dictionary with URL and content\n",
    "    return content_dict\n",
    "\n",
    "# Example usage:\n",
    "urls = [\"https://jobs.careers.microsoft.com/us/en/job/1771714/Head-of-Partner-Intelligence-and-Strategy?jobsource=linkedin\"]\n",
    "page_contents = load_text_from_pages(urls)\n",
    "print(page_contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Install Playwright using pip: pip install playwright\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Run the following command to install the browsers: playwright install\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplaywright\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msync_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sync_playwright\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msync_playwright\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Choose the browser (chromium, firefox, or webkit)\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchromium\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheadless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Create a new browser context\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\sync_api\\_context_manager.py:47\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_own_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m---> 47\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Error(\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"It looks like you are using Playwright Sync API inside the asyncio loop.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03mPlease use the Async API instead.\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m             )\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;66;03m# Create a new fiber for the protocol dispatcher. It will be pumping events\u001b[39;00m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;66;03m# until the end of times. We will pass control to that fiber every time we\u001b[39;00m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m# block while waiting for a response.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgreenlet_main\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mError\u001b[0m: It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead."
     ]
    }
   ],
   "source": [
    "# Install Playwright using pip: pip install playwright\n",
    "# Run the following command to install the browsers: playwright install\n",
    "\n",
    "from playwright.sync_api import sync_playwright\n",
    "\n",
    "with sync_playwright() as p:\n",
    "    # Choose the browser (chromium, firefox, or webkit)\n",
    "    browser = p.chromium.launch(headless=False)\n",
    "    \n",
    "    # Create a new browser context\n",
    "    context = browser.new_context()\n",
    "    \n",
    "    # Open a new page\n",
    "    page = context.new_page()\n",
    "    \n",
    "    # Navigate to a webpage\n",
    "    urls = \"https://jobs.careers.microsoft.com/us/en/job/1771714/Head-of-Partner-Intelligence-and-Strategy?jobsource=linkedin\"\n",
    "    page.goto(urls)\n",
    "    \n",
    "    # Extract the page title\n",
    "    title = page.title()\n",
    "    print(title)\n",
    "    \n",
    "    # Close the browser\n",
    "    browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 14:09:26,927 - asyncio - ERROR - Task exception was never retrieved\n",
      "future: <Task finished name='Task-2' coro=<Connection.run() done, defined at c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_connection.py:265> exception=NotImplementedError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_connection.py\", line 272, in run\n",
      "    await self._transport.connect()\n",
      "  File \"c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 133, in connect\n",
      "    raise exc\n",
      "  File \"c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 120, in connect\n",
      "    self._proc = await asyncio.create_subprocess_exec(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\subprocess.py\", line 223, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1708, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 503, in _make_subprocess_transport\n",
      "    raise NotImplementedError\n",
      "NotImplementedError\n",
      "2024-09-30 14:09:26,931 - asyncio - ERROR - Task exception was never retrieved\n",
      "future: <Task finished name='Task-4' coro=<Connection.run() done, defined at c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_connection.py:265> exception=NotImplementedError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Temp\\ipykernel_20304\\1154074729.py\", line 91, in <module>\n",
      "    asyncio.get_running_loop().run_until_complete(main())\n",
      "  File \"c:\\github\\job_bot\\env\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Temp\\ipykernel_20304\\1154074729.py\", line 85, in main\n",
      "    content_dict, failed_urls = await load_webpages_with_playwright(urls)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Temp\\ipykernel_20304\\1154074729.py\", line 50, in load_webpages_with_playwright\n",
      "    async with async_playwright() as p:\n",
      "  File \"c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py\", line 46, in __aenter__\n",
      "    playwright = AsyncPlaywright(next(iter(done)).result())\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 120, in connect\n",
      "    self._proc = await asyncio.create_subprocess_exec(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\subprocess.py\", line 223, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1708, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 503, in _make_subprocess_transport\n",
      "    raise NotImplementedError\n",
      "NotImplementedError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_connection.py\", line 272, in run\n",
      "    await self._transport.connect()\n",
      "  File \"c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 133, in connect\n",
      "    raise exc\n",
      "  File \"c:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 120, in connect\n",
      "    self._proc = await asyncio.create_subprocess_exec(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\subprocess.py\", line 223, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1708, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 503, in _make_subprocess_transport\n",
      "    raise NotImplementedError\n",
      "NotImplementedError\n",
      "C:\\Users\\xzhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:50: RuntimeWarning: coroutine 'load_webpages_with_playwright' was never awaited\n",
      "  return compile(source, filename, mode, flags,\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 91\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_running_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:  \u001b[38;5;66;03m# In case no loop is running (standard Python env)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[8], line 85\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m urls \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.metacareers.com/jobs/522232286825036/?rx_campaign=Linkedin1&rx_ch=connector&rx_group=126320&rx_job=a1KDp00000E28eGMAR&rx_medium=post&rx_r=none&rx_source=Linkedin&rx_ts=20240927T121201Z&rx_vp=slots&utm_campaign=Job\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m2Bboard&utm_medium=jobs&utm_source=LIpaid&rx_viewer=e3efacca649311ef917d17a1705b89ba0dc4e1e7a57f4231bbce94a604c83931\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m ]\n\u001b[1;32m---> 85\u001b[0m content_dict, failed_urls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m load_webpages_with_playwright(urls)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(content_dict, failed_urls)\n",
      "Cell \u001b[1;32mIn[8], line 50\u001b[0m, in \u001b[0;36mload_webpages_with_playwright\u001b[1;34m(urls)\u001b[0m\n\u001b[0;32m     48\u001b[0m failed_urls \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m async_playwright() \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m     51\u001b[0m     browser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m p\u001b[38;5;241m.\u001b[39mchromium\u001b[38;5;241m.\u001b[39mlaunch(headless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Headless False for debugging\u001b[39;00m\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:46\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__aenter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m     playwright_future\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m---> 46\u001b[0m playwright \u001b[38;5;241m=\u001b[39m AsyncPlaywright(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     47\u001b[0m playwright\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aexit__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_transport.py:120\u001b[0m, in \u001b[0;36mPipeTransport.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m     executable_path, entrypoint_path \u001b[38;5;241m=\u001b[39m compute_driver_executable()\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_subprocess_exec(\n\u001b[0;32m    121\u001b[0m         executable_path,\n\u001b[0;32m    122\u001b[0m         entrypoint_path,\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-driver\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    124\u001b[0m         stdin\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    125\u001b[0m         stdout\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    126\u001b[0m         stderr\u001b[38;5;241m=\u001b[39m_get_stderr_fileno(),\n\u001b[0;32m    127\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32768\u001b[39m,\n\u001b[0;32m    128\u001b[0m         env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m    129\u001b[0m         startupinfo\u001b[38;5;241m=\u001b[39mstartupinfo,\n\u001b[0;32m    130\u001b[0m     )\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\subprocess.py:223\u001b[0m, in \u001b[0;36mcreate_subprocess_exec\u001b[1;34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[0m\n\u001b[0;32m    221\u001b[0m protocol_factory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: SubprocessStreamProtocol(limit\u001b[38;5;241m=\u001b[39mlimit,\n\u001b[0;32m    222\u001b[0m                                                     loop\u001b[38;5;241m=\u001b[39mloop)\n\u001b[1;32m--> 223\u001b[0m transport, protocol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m loop\u001b[38;5;241m.\u001b[39msubprocess_exec(\n\u001b[0;32m    224\u001b[0m     protocol_factory,\n\u001b[0;32m    225\u001b[0m     program, \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    226\u001b[0m     stdin\u001b[38;5;241m=\u001b[39mstdin, stdout\u001b[38;5;241m=\u001b[39mstdout,\n\u001b[0;32m    227\u001b[0m     stderr\u001b[38;5;241m=\u001b[39mstderr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Process(transport, protocol, loop)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py:1708\u001b[0m, in \u001b[0;36mBaseEventLoop.subprocess_exec\u001b[1;34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_subprocess(debug_log, stdin, stdout, stderr)\n\u001b[1;32m-> 1708\u001b[0m transport \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_subprocess_transport(\n\u001b[0;32m   1709\u001b[0m     protocol, popen_args, \u001b[38;5;28;01mFalse\u001b[39;00m, stdin, stdout, stderr,\n\u001b[0;32m   1710\u001b[0m     bufsize, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debug \u001b[38;5;129;01mand\u001b[39;00m debug_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py:503\u001b[0m, in \u001b[0;36mBaseEventLoop._make_subprocess_transport\u001b[1;34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create subprocess transport.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m     asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_until_complete(main())\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:  \u001b[38;5;66;03m# In case no loop is running (standard Python env)\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[1;32mIn[8], line 85\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m     82\u001b[0m     urls \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.metacareers.com/jobs/522232286825036/?rx_campaign=Linkedin1&rx_ch=connector&rx_group=126320&rx_job=a1KDp00000E28eGMAR&rx_medium=post&rx_r=none&rx_source=Linkedin&rx_ts=20240927T121201Z&rx_vp=slots&utm_campaign=Job\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m2Bboard&utm_medium=jobs&utm_source=LIpaid&rx_viewer=e3efacca649311ef917d17a1705b89ba0dc4e1e7a57f4231bbce94a604c83931\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m     ]\n\u001b[1;32m---> 85\u001b[0m     content_dict, failed_urls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m load_webpages_with_playwright(urls)\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28mprint\u001b[39m(content_dict, failed_urls)\n",
      "Cell \u001b[1;32mIn[8], line 50\u001b[0m, in \u001b[0;36mload_webpages_with_playwright\u001b[1;34m(urls)\u001b[0m\n\u001b[0;32m     47\u001b[0m content_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     48\u001b[0m failed_urls \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m async_playwright() \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m     51\u001b[0m     browser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m p\u001b[38;5;241m.\u001b[39mchromium\u001b[38;5;241m.\u001b[39mlaunch(headless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Headless False for debugging\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m browser\u001b[38;5;241m.\u001b[39mnew_page()\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:46\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__aenter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m playwright_future\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     45\u001b[0m     playwright_future\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m---> 46\u001b[0m playwright \u001b[38;5;241m=\u001b[39m AsyncPlaywright(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     47\u001b[0m playwright\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aexit__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m playwright\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\_impl\\_transport.py:120\u001b[0m, in \u001b[0;36mPipeTransport.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m         startupinfo\u001b[38;5;241m.\u001b[39mwShowWindow \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mSW_HIDE\n\u001b[0;32m    119\u001b[0m     executable_path, entrypoint_path \u001b[38;5;241m=\u001b[39m compute_driver_executable()\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_subprocess_exec(\n\u001b[0;32m    121\u001b[0m         executable_path,\n\u001b[0;32m    122\u001b[0m         entrypoint_path,\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-driver\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    124\u001b[0m         stdin\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    125\u001b[0m         stdout\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    126\u001b[0m         stderr\u001b[38;5;241m=\u001b[39m_get_stderr_fileno(),\n\u001b[0;32m    127\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32768\u001b[39m,\n\u001b[0;32m    128\u001b[0m         env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m    129\u001b[0m         startupinfo\u001b[38;5;241m=\u001b[39mstartupinfo,\n\u001b[0;32m    130\u001b[0m     )\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_error_future\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\subprocess.py:223\u001b[0m, in \u001b[0;36mcreate_subprocess_exec\u001b[1;34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m loop \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[0;32m    221\u001b[0m protocol_factory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: SubprocessStreamProtocol(limit\u001b[38;5;241m=\u001b[39mlimit,\n\u001b[0;32m    222\u001b[0m                                                     loop\u001b[38;5;241m=\u001b[39mloop)\n\u001b[1;32m--> 223\u001b[0m transport, protocol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m loop\u001b[38;5;241m.\u001b[39msubprocess_exec(\n\u001b[0;32m    224\u001b[0m     protocol_factory,\n\u001b[0;32m    225\u001b[0m     program, \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    226\u001b[0m     stdin\u001b[38;5;241m=\u001b[39mstdin, stdout\u001b[38;5;241m=\u001b[39mstdout,\n\u001b[0;32m    227\u001b[0m     stderr\u001b[38;5;241m=\u001b[39mstderr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Process(transport, protocol, loop)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py:1708\u001b[0m, in \u001b[0;36mBaseEventLoop.subprocess_exec\u001b[1;34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1706\u001b[0m     debug_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecute program \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogram\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_subprocess(debug_log, stdin, stdout, stderr)\n\u001b[1;32m-> 1708\u001b[0m transport \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_subprocess_transport(\n\u001b[0;32m   1709\u001b[0m     protocol, popen_args, \u001b[38;5;28;01mFalse\u001b[39;00m, stdin, stdout, stderr,\n\u001b[0;32m   1710\u001b[0m     bufsize, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debug \u001b[38;5;129;01mand\u001b[39;00m debug_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1712\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, debug_log, transport)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py:503\u001b[0m, in \u001b[0;36mBaseEventLoop._make_subprocess_transport\u001b[1;34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_subprocess_transport\u001b[39m(\u001b[38;5;28mself\u001b[39m, protocol, args, shell,\n\u001b[0;32m    500\u001b[0m                                      stdin, stdout, stderr, bufsize,\n\u001b[0;32m    501\u001b[0m                                      extra\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    502\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create subprocess transport.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "import logging\n",
    "import logging_config\n",
    "import re\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Allow nested event loops (needed for Jupyter/IPython environments)\n",
    "nest_asyncio.apply()\n",
    "def clean_webpage_text(content):\n",
    "    \"\"\"\n",
    "    Clean the extracted text by removing JavaScript, URLs, scripts, and excessive whitespace.\n",
    "\n",
    "    This function performs the following cleaning steps:\n",
    "    - Removes JavaScript function calls.\n",
    "    - Removes URLs (e.g., tracking or other unwanted URLs).\n",
    "    - Removes script tags and their content.\n",
    "    - Replaces multiple spaces or newline characters with a single space or newline.\n",
    "    - Strips leading and trailing whitespace.\n",
    "\n",
    "    Args:\n",
    "        content (str): The text content to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned and processed text.\n",
    "    \"\"\"\n",
    "    # Remove JavaScript function calls (e.g., requireLazy([...]))\n",
    "    content = re.sub(r\"requireLazy\\([^)]+\\)\", \"\", content)\n",
    "\n",
    "    # Remove URLs (e.g., http, https)\n",
    "    content = re.sub(r\"https?:\\/\\/\\S+\", \"\", content)\n",
    "\n",
    "    # Remove <script> tags and their contents\n",
    "    content = re.sub(r\"<script[^>]*>[\\s\\S]*?</script>\", \"\", content)\n",
    "\n",
    "    # Remove excessive whitespace (more than one space)\n",
    "    content = re.sub(r\"\\s+\", \" \", content).strip()\n",
    "\n",
    "    # Replace double newlines with single newlines\n",
    "    content = content.replace(\"\\n\\n\", \"\\n\")\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "async def load_webpages_with_playwright(urls):\n",
    "    content_dict = {}\n",
    "    failed_urls = []\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=False)  # Headless False for debugging\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        for url in urls:\n",
    "            try:\n",
    "                logger.info(f\"Attempting to load content with Playwright for URL: {url}\")\n",
    "                await page.goto(url)\n",
    "                \n",
    "                # Wait for network to be idle\n",
    "                await page.wait_for_load_state('networkidle')\n",
    "\n",
    "                # Try using page.evaluate() to directly access the text content via JavaScript\n",
    "                content = await page.evaluate('document.body.innerText')\n",
    "                logger.debug(f\"Extracted content: {content}\")\n",
    "\n",
    "                if content and content.strip():\n",
    "                    clean_content = clean_webpage_text(content)\n",
    "                    content_dict[url] = clean_content\n",
    "                    logger.info(f\"Successfully processed content for {url}\")\n",
    "                else:\n",
    "                    raise ValueError(\"No content extracted.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error occurred while fetching content for {url}: {e}\")\n",
    "                failed_urls.append(url)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    return content_dict, failed_urls\n",
    "\n",
    "async def main():\n",
    "    urls = [\n",
    "        \"https://www.metacareers.com/jobs/522232286825036/?rx_campaign=Linkedin1&rx_ch=connector&rx_group=126320&rx_job=a1KDp00000E28eGMAR&rx_medium=post&rx_r=none&rx_source=Linkedin&rx_ts=20240927T121201Z&rx_vp=slots&utm_campaign=Job%2Bboard&utm_medium=jobs&utm_source=LIpaid&rx_viewer=e3efacca649311ef917d17a1705b89ba0dc4e1e7a57f4231bbce94a604c83931\"\n",
    "    ]\n",
    "    content_dict, failed_urls = await load_webpages_with_playwright(urls)\n",
    "    print(content_dict, failed_urls)\n",
    "\n",
    "# For environments like Jupyter, use get_event_loop instead of asyncio.run\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        asyncio.get_running_loop().run_until_complete(main())\n",
    "    except RuntimeError:  # In case no loop is running (standard Python env)\n",
    "        asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics. You may view our Equal Employment Opportunity notice\n",
      "here. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. We may use your information to maintain the safety and security of Meta, its employees, and others as required or permitted by law. You may view\n",
      "Meta Pay Transparency Policy, Equal Employment Opportunity is the Law notice, and\n",
      "Notice to Applicants for Employment and Employees by clicking on their corresponding links. Additionally, Meta participates in the\n",
      "E-Verify program in certain locations, as required by law.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from llama_index.readers.web import TrafilaturaWebReader\n",
    "\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "urls = [\"https://www.metacareers.com/jobs/522232286825036/?rx_campaign=Linkedin1&rx_ch=connector&rx_group=126320&rx_job=a1KDp00000E28eGMAR&rx_medium=post&rx_r=none&rx_source=Linkedin&rx_ts=20240927T121201Z&rx_vp=slots&utm_campaign=Job%2Bboard&utm_medium=jobs&utm_source=LIpaid&rx_viewer=e3efacca649311ef917d17a1705b89ba0dc4e1e7a57f4231bbce94a604c83931\"]\n",
    "\n",
    "reader = TrafilaturaWebReader()\n",
    "\n",
    "document=reader.load_data(urls=urls)\n",
    "\n",
    "for i, doc in enumerate(document):\n",
    "    # Check if the document has a 'text' attribute (as in Trafilatura Document object)\n",
    "    if hasattr(doc, \"text\"):\n",
    "        # If it's a Trafilatura Document, use its text attribute\n",
    "        page_text = (doc.text)\n",
    "        print(page_text)\n",
    "# display(document[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics. You may view our Equal Employment Opportunity notice\n",
      "here. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. We may use your information to maintain the safety and security of Meta, its employees, and others as required or permitted by law. You may view\n",
      "Meta Pay Transparency Policy, Equal Employment Opportunity is the Law notice, and\n",
      "Notice to Applicants for Employment and Employees by clicking on their corresponding links. Additionally, Meta participates in the\n",
      "E-Verify program in certain locations, as required by law.\n"
     ]
    }
   ],
   "source": [
    "import trafilatura\n",
    "\n",
    "def get_webpage_content(url):\n",
    "    download = trafilatura.fetch_url(url)\n",
    "\n",
    "    if download:\n",
    "        content = trafilatura.extract(download)\n",
    "        return content\n",
    "    else:\n",
    "        return \"failed\"\n",
    "    \n",
    "urls = \"https://www.metacareers.com/jobs/522232286825036/?rx_campaign=Linkedin1&rx_ch=connector&rx_group=126320&rx_job=a1KDp00000E28eGMAR&rx_medium=post&rx_r=none&rx_source=Linkedin&rx_ts=20240927T121201Z&rx_vp=slots&utm_campaign=Job%2Bboard&utm_medium=jobs&utm_source=LIpaid&rx_viewer=e3efacca649311ef917d17a1705b89ba0dc4e1e7a57f4231bbce94a604c83931\"\n",
    "\n",
    "content = get_webpage_content(urls)\n",
    "print(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m browser\n\u001b[0;32m     15\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.metacareers.com/jobs/522232286825036/?rx_campaign=Linkedin1&rx_ch=connector&rx_group=126320&rx_job=a1KDp00000E28eGMAR&rx_medium=post&rx_r=none&rx_source=Linkedin&rx_ts=20240927T121201Z&rx_vp=slots&utm_campaign=Job\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m2Bboard&utm_medium=jobs&utm_source=LIpaid&rx_viewer=e3efacca649311ef917d17a1705b89ba0dc4e1e7a57f4231bbce94a604c83931\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_synamic_webpage_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m content:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(content)\n",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m, in \u001b[0;36mfetch_synamic_webpage_content\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_synamic_webpage_content\u001b[39m(url):\n\u001b[1;32m----> 4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msync_playwright\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchromium\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheadless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\github\\job_bot\\env\\Lib\\site-packages\\playwright\\sync_api\\_context_manager.py:47\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_own_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m---> 47\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Error(\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"It looks like you are using Playwright Sync API inside the asyncio loop.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03mPlease use the Async API instead.\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m             )\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;66;03m# Create a new fiber for the protocol dispatcher. It will be pumping events\u001b[39;00m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;66;03m# until the end of times. We will pass control to that fiber every time we\u001b[39;00m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m# block while waiting for a response.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgreenlet_main\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mError\u001b[0m: It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead."
     ]
    }
   ],
   "source": [
    "from playwright.sync_api import sync_playwright\n",
    "\n",
    "def fetch_synamic_webpage_content(url):\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=False)\n",
    "        page = browser.new_page()\n",
    "\n",
    "        page.goto(url)\n",
    "\n",
    "        cotnent = page.content()\n",
    "\n",
    "        browser.close()\n",
    "        return browser\n",
    "\n",
    "url = \"https://www.metacareers.com/jobs/522232286825036/?rx_campaign=Linkedin1&rx_ch=connector&rx_group=126320&rx_job=a1KDp00000E28eGMAR&rx_medium=post&rx_r=none&rx_source=Linkedin&rx_ts=20240927T121201Z&rx_vp=slots&utm_campaign=Job%2Bboard&utm_medium=jobs&utm_source=LIpaid&rx_viewer=e3efacca649311ef917d17a1705b89ba0dc4e1e7a57f4231bbce94a604c83931\"\n",
    "content = fetch_synamic_webpage_content(url)\n",
    "\n",
    "if content:\n",
    "    print(content)\n",
    "else:\n",
    "    print(\"No content extracted.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from playwright.sync_api import Page, expect\n",
    "\n",
    "def test_has_title(page: Page):\n",
    "    page.goto(\"https://jobs.careers.microsoft.com/us/en/job/1771714/Head-of-Partner-Intelligence-and-Strategy?jobsource=linkedin\")\n",
    "\n",
    "    # Expect a title \"to contain\" a substring.\n",
    "    expect(page).to_have_title(re.compile(\"Playwright\"))\n",
    "\n",
    "def test_get_started_link(page: Page):\n",
    "    page.goto(\"https://jobs.careers.microsoft.com/us/en/job/1771714/Head-of-Partner-Intelligence-and-Strategy?jobsource=linkedin\")\n",
    "\n",
    "    # Click the get started link.\n",
    "    page.get_by_role(\"link\", name=\"Get started\").click()\n",
    "\n",
    "    # Expects page to have a heading with the name of Installation.\n",
    "    expect(page.get_by_role(\"heading\", name=\"Installation\")).to_be_visible()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup codes (to be delated later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back up just in case\n",
    "\n",
    "def call_llama3(self, prompt, temperature):\n",
    "    # def edit_text_for_semantic_entailment_llama3(\n",
    "    #     candidate_text,\n",
    "    #     reference_text,\n",
    "    #     text_id=None,\n",
    "    #     temperature=0.6,\n",
    "    # ):\n",
    "    \"\"\"\n",
    "\n",
    "    Edits/optimizes a text using Llama3 model based on another text (source text).\n",
    "\n",
    "    Args:\n",
    "        - text_id (int): (Optional) Identifier for the responsibility text.\n",
    "        Default to None (unique ids to be generated with UUID function)\n",
    "        - candidate_text (str): Original text to be transformed by the model\n",
    "        (i.e., the riginal responsibility text to be revised.)\n",
    "        - reference_text (str): Text that the candidate text is being compared to\n",
    "        (i.e., requirement text to optimize against.)\n",
    "        - max_tokens: default set to 1056\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains 'resp_id' and 'optimized_text' after revision.\n",
    "    \"\"\"\n",
    "    # Generate a unique text_id using UUID\n",
    "    if text_id is None:\n",
    "        text_id = str(uuid.uuid4())\n",
    "\n",
    "    # Create the prompt using a predefined template\n",
    "    try:\n",
    "        prompt = SEMANTIC_ALIGNMENT_PROMPT.format(\n",
    "            content_1=candidate_text, content_2=reference_text\n",
    "        )\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"Error formatting STRUCTURE_TRANSFER_PROMPT: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Call api function and get response; deserialize from pydantic obj to dict\n",
    "    response_pyd_obj = call_llama3(\n",
    "        prompt, expected_res_type=\"json\", temperature=temperature\n",
    "    )\n",
    "\n",
    "    # Ensure the response is in the correct format (Pydantic JSONResponse model)\n",
    "    if not isinstance(response_pyd_obj, JSONResponse):\n",
    "        logger.error(\"Received response is not in expected JSONResponse format.\")\n",
    "        raise ValueError(\n",
    "            \"Received response is not in expected JSONResponse format.\"\n",
    "        )\n",
    "\n",
    "    # Deserialize pydantic obj. (expect a dictionary)\n",
    "    response_dict = response_pyd_obj.model_dump()\n",
    "    # print(f\"final text: {response_dict}\")\n",
    "\n",
    "    # Validate the JSON structure using JSON Schema\n",
    "    try:\n",
    "        jsonschema.validate(instance=response_dict, schema=LLM_RES_JSON_SCHEMA)\n",
    "        logger.info(\"JSON schema validation passed.\")\n",
    "    except jsonschema.exceptions.ValidationError as e:\n",
    "        logger.error(f\"JSON schema validation failed: {e}\")\n",
    "        raise ValueError(f\"Invalid JSON format: {e}\")\n",
    "\n",
    "    # Combine w/t id, then return the combined dictionary\n",
    "    result = {\"resp_id\": text_id, **response_dict}  # ** to unpack a dictionary\n",
    "\n",
    "    logger.info(f\"Results updated: \\n{result}\")\n",
    "    return result\n",
    "\n",
    "def validate_response(self, response_dict):\n",
    "    \"\"\"Validate the API response dictionary using JSON Schema.\"\"\"\n",
    "    try:\n",
    "        jsonschema.validate(instance=response_dict, schema=LLM_RES_JSON_SCHEMA)\n",
    "        logger.info(\"JSON schema validation passed.\")\n",
    "    except jsonschema.exceptions.ValidationError as e:\n",
    "        logger.error(f\"JSON schema validation failed: {e}\")\n",
    "        raise ValueError(f\"Invalid JSON format: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_text_for_semantic_entailment(\n",
    "    client,\n",
    "    candidate_text,\n",
    "    reference_text,\n",
    "    text_id=None,\n",
    "    model_id=\"gpt-4-turbo\",\n",
    "    temperature=0.6,\n",
    "    max_tokens=1056,\n",
    "):\n",
    "\n",
    "    # Generate a unique text_id using UUID\n",
    "    if text_id is None:\n",
    "        text_id = str(uuid.uuid4())\n",
    "\n",
    "    # Create the prompt using a predefined template\n",
    "    try:\n",
    "        prompt = SEMANTIC_ALIGNMENT_PROMPT.format(\n",
    "            content_1=candidate_text, content_2=reference_text\n",
    "        )\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"Error formatting STRUCTURE_TRANSFER_PROMPT: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Call call openai api function and get response;\n",
    "    # deserialize from pydantic obj to dict\n",
    "    response_pyd_obj = call_openai_api(\n",
    "        client,\n",
    "        model_id,\n",
    "        prompt,\n",
    "        expected_res_type=\"json\",\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    # Ensure the response is a pyd  (Pydantic JSONResponse model)\n",
    "    if not isinstance(response_pyd_obj, JSONResponse):\n",
    "        logger.error(\"Received response is not in expected JSONResponse format.\")\n",
    "        raise ValueError(\"Received response is not in expected JSONResponse format.\")\n",
    "\n",
    "    # Deserialize pydantic obj. (expect a dictionary)\n",
    "    response_dict = response_pyd_obj.model_dump()\n",
    "    # print(f\"final text: {response_dict}\")\n",
    "\n",
    "    # Validate the JSON structure using JSON Schema\n",
    "    try:\n",
    "        jsonschema.validate(instance=response_dict, schema=LLM_RES_JSON_SCHEMA)\n",
    "        logger.info(\"JSON schema validation passed.\")\n",
    "    except jsonschema.exceptions.ValidationError as e:\n",
    "        logger.error(f\"JSON schema validation failed: {e}\")\n",
    "        raise ValueError(f\"Invalid JSON format: {e}\")\n",
    "\n",
    "    # Combine w/t id, then return the combined dictionary\n",
    "    result = {\"resp_id\": text_id, **response_dict}  # ** to unpack a dictionary\n",
    "\n",
    "    logger.info(f\"Results updated: \\n{result}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def edit_text_for_dp(\n",
    "    client,\n",
    "    target_text,\n",
    "    source_text,\n",
    "    text_id=None,\n",
    "    model_id=\"gpt-4-turbo\",\n",
    "    temperature=0.8,\n",
    "    max_tokens=1056,\n",
    "):\n",
    "    \"\"\"\n",
    "    Re-edit the target text to better align w/t source text's dependency parsing (DP),\n",
    "    leveraging the OpenAI API.\n",
    "\n",
    "    Example:\n",
    "    Re-edit revised responsibility to match with the original responsibility text's DP\n",
    "    to perserve the tone & style.\n",
    "\n",
    "    Args:\n",
    "        - client (OpenAI()): OpenAI API client instance.\n",
    "        - text_id (str): Identifier of the target text (defaulted to None - unique IDs\n",
    "        to be generated by UUID function)\n",
    "        (i.e., the responsibility bullet text.)\n",
    "        - target_text (str): The target text to be transformed (i.e.,\n",
    "        revised responsibility text).\n",
    "        - source_text (str): The source text from whose \"dependency parsing\"\n",
    "        to be modeled after (i.e., original responsibility text from resume).\n",
    "        - model_id (str): OpenAI model to use (default is 'gpt-4').\n",
    "        - temperature (float): defaulted to 0.8 (a higher temperature setting is\n",
    "        needed to give the model more flexibility/creativity).\n",
    "        - max_token: default to 1056\n",
    "\n",
    "        Note:\n",
    "        - resp is short for responsibility\n",
    "        - req is short for (job) requirement\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary in the format of {'resp_id': \"...\", 'optimized_text': \"...\"}.\n",
    "    \"\"\"\n",
    "    # Generate a unique text_id using UUID\n",
    "    if text_id is None:\n",
    "        text_id = str(uuid.uuid4())\n",
    "\n",
    "    # Define the JSON schema and instructions clearly in the prompt\n",
    "    try:\n",
    "        prompt = STRUCTURE_TRANSFER_PROMPT.format(\n",
    "            content_1=source_text, content_2=target_text\n",
    "        )\n",
    "        print(f\"DEBUG - Formatted Prompt in edit_text_for_dp:\\n{prompt}\")\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"Error formatting STRUCTURE_TRANSFER_PROMPT: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Call API function and get response; deserialize from pydantic obj to dict\n",
    "    response_pyd_obj = call_openai_api(\n",
    "        client,\n",
    "        model_id,\n",
    "        prompt,\n",
    "        expected_res_type=\"json\",\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    # Ensure the response is in the correct format (Pydantic JSONResponse model)\n",
    "    if not isinstance(response_pyd_obj, JSONResponse):\n",
    "        logger.error(\"Received response is not in expected JSONResponse format.\")\n",
    "        raise ValueError(\"Received response is not in expected JSONResponse format.\")\n",
    "\n",
    "    # Deserialize pydantic obj. (expect a dictionary)\n",
    "    response_dict = response_pyd_obj.model_dump()\n",
    "\n",
    "    # Validate the JSON structure using JSON Schema\n",
    "    try:\n",
    "        jsonschema.validate(instance=response_dict, schema=LLM_RES_JSON_SCHEMA)\n",
    "        logger.info(\"JSON schema validation passed.\")\n",
    "    except jsonschema.exceptions.ValidationError as e:\n",
    "        logger.error(f\"JSON schema validation failed: {e}\")\n",
    "        raise ValueError(f\"Invalid JSON format: {e}\")\n",
    "\n",
    "    # Combine w/t id, then return the combined dictionary\n",
    "    result = {\"resp_id\": text_id, **response_dict}  # ** to unpack a dictionary\n",
    "\n",
    "    logger.info(f\"Results updated: \\n{result}\")\n",
    "    return result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
