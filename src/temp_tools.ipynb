{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 years of experience in management consulting, product management and strategy, or analytics in a technology company.\n",
      "Experience working with and analyzing data, and managing multiple cross-functional programs or projects.\n",
      "Experience with performing market analysis and developing competitive intelligence.\n",
      "Ability to manage executive stakeholders and communicate with a highly technical management team.\n",
      "Ability to form and refine hypotheses, gather supporting data, and make recommendations.\n",
      "Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\n"
     ]
    }
   ],
   "source": [
    "texts = [\"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\", \n",
    "        \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\",\n",
    "        \"Experience with performing market analysis and developing competitive intelligence\",\n",
    "        \"Ability to manage executive stakeholders and communicate with a highly technical management team\",\n",
    "        \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\",\n",
    "        \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"]\n",
    "\n",
    "texts = \".\\n\".join(text for text in texts)\n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    bert_score_precision  soft_similarity  word_movers_distance  \\\n",
      "0               0.833088         0.294945             15.905974   \n",
      "1               0.833400         0.315879             15.811388   \n",
      "2               0.836929         0.377884             15.874508   \n",
      "3               0.845272         0.264874             15.524175   \n",
      "4               0.831945         0.243683             15.716234   \n",
      "5               0.832256         0.294245             15.524175   \n",
      "6               0.830909         0.337065             15.905974   \n",
      "7               0.838344         0.215926             15.099669   \n",
      "8               0.832782         0.277579             15.524175   \n",
      "9               0.856216         0.354124             15.620499   \n",
      "10              0.827047         0.433190             15.459625   \n",
      "11              0.835676         0.253837             15.748016   \n",
      "12              0.846321         0.363054             15.491933   \n",
      "13              0.852224         0.191506             15.524175   \n",
      "14              0.816647         0.157734             15.620499   \n",
      "15              0.869484         0.262018             15.459625   \n",
      "16              0.823150         0.330459             15.716234   \n",
      "17              0.837198         0.360644             15.524175   \n",
      "18              0.820165         0.344930             15.716234   \n",
      "19              0.822814         0.287575             15.556349   \n",
      "20              0.821459         0.282235             15.620499   \n",
      "21              0.829722         0.210947             15.652476   \n",
      "22              0.842820         0.352504             15.748016   \n",
      "23              0.824793         0.309893             15.684387   \n",
      "24              0.865685         0.395633             15.427249   \n",
      "25              0.828627         0.275297             15.524175   \n",
      "26              0.850405         0.346306             15.132746   \n",
      "\n",
      "    nli_entailment_score  jaccard_similarity  \n",
      "0               0.049976            0.042424  \n",
      "1               0.042624            0.036364  \n",
      "2               0.004728            0.058140  \n",
      "3               0.048242            0.043478  \n",
      "4               0.003041            0.037500  \n",
      "5               0.002506            0.036810  \n",
      "6               0.014546            0.080247  \n",
      "7               0.009681            0.061350  \n",
      "8               0.005850            0.057325  \n",
      "9               0.030971            0.038710  \n",
      "10              0.026829            0.056604  \n",
      "11              0.001266            0.037500  \n",
      "12              0.039242            0.052980  \n",
      "13              0.070042            0.026667  \n",
      "14              0.051734            0.032680  \n",
      "15              0.171467            0.026667  \n",
      "16              0.087002            0.019231  \n",
      "17              0.019296            0.026316  \n",
      "18              0.008592            0.019231  \n",
      "19              0.010191            0.032051  \n",
      "20              0.009039            0.048485  \n",
      "21              0.010838            0.025316  \n",
      "22              0.022875            0.049689  \n",
      "23              0.005869            0.025157  \n",
      "24              0.001645            0.038462  \n",
      "25              0.004120            0.025806  \n",
      "26              0.055380            0.045455  \n",
      "MBA or graduate degree in a management, technical, or engineering field Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling 11 years of experience in management consulting, product management and strategy, or analytics in a technology company Experience working with and analyzing data, and managing multiple cross-functional programs or projects Experience with performing market analysis and developing competitive intelligence Ability to manage executive stakeholders and communicate with a highly technical management team Ability to form and refine hypotheses, gather supporting data, and make recommendations Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems English proficiency is a requirement for all roles unless stated otherwise in the job posting Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "\n",
    "f_path = r\"C:\\github\\job_bot\\data\\output_sim_matrix.csv\"\n",
    "df = pd.read_csv(f_path)\n",
    "df[\"Similarity Metrices\"] = df['Similarity Metrices'].apply(ast.literal_eval)\n",
    "\n",
    "metrics_df = pd.json_normalize(df['Similarity Metrices'])\n",
    "print(metrics_df)\n",
    "combined_df = pd.concat([df[[\"Responsibility\"]], metrics_df], axis=1)\n",
    "combined_df['Responsibility'] = combined_df['Responsibility'].str.replace('\\n', ' ')\n",
    "\n",
    "    # Ensure full display of DataFrame content\n",
    "pd.set_option(\"display.max_colwidth\", 100)  # Display full column content\n",
    "pd.set_option(\"display.max_rows\", None)  # Display all rows\n",
    "\n",
    "# Export the combined DataFrame to a CSV file\n",
    "# combined_df.to_csv(\"output_sim_matrix_cleaned.csv\", index=False)\n",
    "print(df.Requirements[1])\n",
    "# This will create a CSV file that you can open directly in Excel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\xzhan\\.cache\\huggingface\\hub\\models--roberta-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "nli_model_name = \"roberta-large-mnli\"\n",
    "\n",
    "# Re-download the model and tokenizer\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name, force_download=True)\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name, force_download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2819904685020447\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "premise = \"The candidate should have experience in developing machine learning models using Python.\"\n",
    "\n",
    "hypothesis = \"I have developed machine learning models using Python for data analysis projects.\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "nli_score = textsimilarity.nli_entailment_score(premise, hypothesis)\n",
    "\n",
    "print(nli_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Entailment Score: 0.614780604839325\n"
     ]
    }
   ],
   "source": [
    "premise = \"The company has launched a new machine learning model.\"\n",
    "hypothesis = \"The company is working on AI projects.\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "nli_score = textsimilarity.nli_entailment_score(hypothesis, premise)\n",
    "print(f\"NLI Entailment Score: {nli_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Entailment Score: 0.0010548133868724108\n"
     ]
    }
   ],
   "source": [
    "hypothesis = \"The company has launched a new machine learning model.\"\n",
    "premise = \"The company is working on AI projects.\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "nli_score = textsimilarity.nli_entailment_score(hypothesis, premise)\n",
    "print(f\"NLI Entailment Score: {nli_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Entailment Score: 0.11766134947538376\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "premise = \"She graduated with a degree in computer science and has 5 years of experience in software development.\"\n",
    "hypothesis = \"She is prepared for a software engineering role.\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "nli_score = textsimilarity.deberta_entailment_score(hypothesis, premise)\n",
    "print(f\"NLI Entailment Score: {nli_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment Score: 0.0016027865931391716\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "hypothesis = \"Enhanced data quality and consistency by integrating thorough financial analysis, standardizing methodologies, and conducting in-depth vendor engagements.\"\n",
    "premise = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "deberta_score = textsimilarity.deberta_entailment_score(hypothesis, premise)\n",
    "print(f\"Entailment Score: {deberta_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment Score: 0.4864349961280823\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "premise = \"Enhanced data quality and consistency by integrating thorough financial analysis, standardizing methodologies, and conducting in-depth vendor engagements.\"\n",
    "hypothesis = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "deberta_score = textsimilarity.deberta_entailment_score(hypothesis, premise)\n",
    "print(f\"Entailment Score: {deberta_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment Score: 0.025084542110562325\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "premise = \"Collaborated with the engineering services research team to pioneer the engineering services tracker, authored influential publications on market forecasts, the impact of COVID-19 on services, and trends in M&A within the engineering services industry.\"\n",
    "hypothesis = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "deberta_score = textsimilarity.deberta_entailment_score(hypothesis, premise)\n",
    "print(f\"Entailment Score: {deberta_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Categorized Scores:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'bert_score_precision': 'High',\n",
       " 'soft_similarity': 'Medium',\n",
       " 'word_movers_distance': 'High',\n",
       " 'nli_entailment_score': 'Low',\n",
       " 'jaccard_similarity': 'High',\n",
       " 'deberta_entailment_score': 'Medium'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# metrics_evaluator.py\n",
    "\n",
    "# Define the metric criteria in a dictionary\n",
    "METRIC_CRITERIA = {\n",
    "    # \"BERTScore Precision\"\n",
    "    \"bert_score_precision\": {\n",
    "        \"range\": (0, 1),\n",
    "        \"high_threshold\": 0.85,\n",
    "        \"low_threshold\": 0.70,\n",
    "    },\n",
    "    # \"Soft Similarity (SBERT)\"\n",
    "    \"soft_similarity\": {\n",
    "        \"range\": (-1, 1),\n",
    "        \"high_threshold\": 0.7,\n",
    "        \"low_threshold\": 0.4,\n",
    "    },\n",
    "    # \"Word Mover's Distance\"\n",
    "    \"word_movers_distance\": {\n",
    "        \"range\": (0, float(\"inf\")),\n",
    "        \"high_threshold\": 5,  # High score is considered \"Low\" for this metric\n",
    "        \"low_threshold\": 15,  # Low score is considered \"High\" for this metric\n",
    "        \"reverse\": True,  # Indicates smaller scores are better\n",
    "    },\n",
    "    # \"NLI Entailment Score\"\n",
    "    \"nli_entailment_score\": {\n",
    "        \"range\": (0, 1),\n",
    "        \"high_threshold\": 0.7,\n",
    "        \"low_threshold\": 0.3,\n",
    "    },\n",
    "    # \"Jaccard Similarity\"\n",
    "    \"jaccard_similarity\": {\n",
    "        \"range\": (0, 1),\n",
    "        \"high_threshold\": 0.5,\n",
    "        \"low_threshold\": 0.2,\n",
    "    },\n",
    "    # \"DeBERTa Entailment Score\"\n",
    "    \"deberta_entailment_score\": {\n",
    "        \"range\": (0, 1),\n",
    "        \"high_threshold\": 0.8,\n",
    "        \"low_threshold\": 0.4,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def evaluate_score(metric_name, score):\n",
    "    \"\"\"\n",
    "    Evaluate the score for a given metric name.\n",
    "\n",
    "    Args:\n",
    "        metric_name (str): The name of the metric.\n",
    "        score (float): The score to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        str: The category of the score (\"High\", \"Medium\", \"Low\").\n",
    "    \"\"\"\n",
    "    if metric_name not in METRIC_CRITERIA:\n",
    "        raise ValueError(f\"Metric '{metric_name}' is not defined in the criteria.\")\n",
    "\n",
    "    criteria = METRIC_CRITERIA[metric_name]\n",
    "    high_threshold = criteria[\"high_threshold\"]\n",
    "    low_threshold = criteria[\"low_threshold\"]\n",
    "    reverse = criteria.get(\"reverse\", False)\n",
    "\n",
    "    # For metrics where a lower score is better (like Word Mover's Distance)\n",
    "    if reverse:\n",
    "        if score <= high_threshold:\n",
    "            return \"High\"\n",
    "        elif score >= low_threshold:\n",
    "            return \"Low\"\n",
    "    else:\n",
    "        if score >= high_threshold:\n",
    "            return \"High\"\n",
    "        elif score <= low_threshold:\n",
    "            return \"Low\"\n",
    "\n",
    "    return \"Medium\"\n",
    "\n",
    "\n",
    "def categorize_scores(scores):\n",
    "    \"\"\"\n",
    "    Categorize a dictionary of scores based on their metric names.\n",
    "\n",
    "    Args:\n",
    "        scores (dict): A dictionary where keys are metric names and values are scores.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the same keys but with \"High\", \"Medium\", or \"Low\" as values.\n",
    "    \"\"\"\n",
    "    categorized_scores = {}\n",
    "    for metric_name, score in scores.items():\n",
    "        try:\n",
    "            categorized_scores[metric_name] = evaluate_score(metric_name, score)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            categorized_scores[metric_name] = \"Unknown\"\n",
    "    return categorized_scores\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    example_scores = {\n",
    "        \"bert_score_precision\": 0.88,\n",
    "        \"soft_similarity\": 0.45,\n",
    "        \"word_movers_distance\": 4.2,\n",
    "        \"nli_entailment_score\": 0.2,\n",
    "        \"jaccard_similarity\": 0.6,\n",
    "        \"deberta_entailment_score\": 0.75,\n",
    "    }\n",
    "\n",
    "    categorized = categorize_scores(example_scores)\n",
    "    display(\"Categorized Scores:\", categorized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, here's one for you:\\n\\nWhy couldn't the bicycle find its way home?\\n\\nBecause it lost its bearings!\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "opeanai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = opeanai_api_key\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"tell me a joke.\"},\n",
    "    ],\n",
    "    temperature=0.2,\n",
    "    max_tokens=200,  # Ensure sufficient tokens for response\n",
    ")\n",
    "\n",
    "response_text = response.choices[0].message.content\n",
    "response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Responsibilities Edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:47:48,608 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:47:50,700 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:47:50,704 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:47:50,706 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': 1, 'optimized_text': 'Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.'}\n",
      "Results updated: \n",
      "{'resp_id': 1, 'optimized_text': 'Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:47:50,709 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': 1, 'optimized_text': 'Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'resp_id': 1,\n",
       " 'optimized_text': 'Utilized first-party and third-party market data to provide strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific, which enhanced local implementation outcomes and informed product roadmaps.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editing import edit_text_for_dp, edit_text_for_semantic_entailment\n",
    "\n",
    "resp_text = \"Provided strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\"\n",
    "\n",
    "reqs_text1 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "reqs_text2 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "reqs_text3 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "api_key = get_openai_api_key()\n",
    "client = OpenAI(api_key=api_key)  # Instantiate openai api chat completion class\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "gpt4t = \"gpt-4-turbo\"\n",
    "\n",
    "\n",
    "revised_text = edit_text_for_semantic_entailment(\n",
    "    client=client, \n",
    "    text_id=1, \n",
    "    candidate_text=resp_text, \n",
    "    model_id=gpt4t, \n",
    "    reference_text=reqs_text3)\n",
    "\n",
    "revised_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 15:35:47,560 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Formatted Prompt in edit_text_for_dp:\n",
      "\n",
      "You are a skilled professional at writing resumes. Please perform the following tasks:\n",
      "\n",
      "1. Analyze the **source text** at a high level.\n",
      "\n",
      "2. Apply the \"\"source text's** dependency structure to the **target text**. Ensure that the original meaning of the **target text** is mostly preserved.\n",
      "\n",
      "**Source Text:**\n",
      "\"Provided strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\"\n",
      "\n",
      "**Target Text:**\n",
      "\"Leveraged strategic insights to optimize the service partner ecosystem in Asia Pacific for a major global IT vendor, enhancing local implementation outcomes.\"\n",
      "\n",
      "**Return Format:**\n",
      "Please return the result in JSON format as follows:\n",
      "\n",
      "{\n",
      "  \"optimized_text\": \"Edited version of the target text\"\n",
      "}\n",
      "\n",
      "Do not include any additional text or explanations.\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 15:35:49,943 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 15:35:49,947 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 15:35:49,949 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': '1', 'optimized_text': 'Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.'}\n",
      "Results updated: \n",
      "{'resp_id': '1', 'optimized_text': 'Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 15:35:49,950 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': '1', 'optimized_text': 'Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "source text:\n",
      "Provided strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\n",
      "\n",
      "target text:\n",
      "Leveraged strategic insights to optimize the service partner ecosystem in Asia Pacific for a major global IT vendor, enhancing local implementation outcomes.\n",
      "\n",
      "Optimized Text Result: \n",
      "Leveraged strategic insights for a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editing import edit_text_for_dp, edit_text_for_semantic_entailment\n",
    "\n",
    "# Sample Source and Target Texts\n",
    "source_text = \"Provided strategic insights to a major global IT vendor, optimizing their service partner ecosystem in Asia Pacific for improved local implementation outcomes.\"\n",
    "target_text = \"Leveraged strategic insights to optimize the service partner ecosystem in Asia Pacific for a major global IT vendor, enhancing local implementation outcomes.\"\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "api_key = get_openai_api_key()\n",
    "client = OpenAI(api_key=api_key)  # Instantiate openai api chat completion class\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4\"\n",
    "gpt4t = \"gpt-4-turbo\"\n",
    "\n",
    "# Call the edit_text_for_dp function with the sample texts\n",
    "try:\n",
    "    result = edit_text_for_dp(\n",
    "        client=client,  # Your OpenAI client instance\n",
    "        text_id=\"1\",  # Just an example ID\n",
    "        target_text=target_text,\n",
    "        source_text=source_text,\n",
    "        model_id=\"gpt-4\",  # Example model ID\n",
    "        max_tokens=1056  # Example max token limit\n",
    "    )\n",
    "    print(f\"\\nsource text:\\n{source_text}\")\n",
    "    print(f\"\\ntarget text:\\n{target_text}\")\n",
    "    print(f\"\\nOptimized Text Result: \\n{result['optimized_text']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during testing: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longer sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just Semantic & Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:11:38,282 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:11:44,505 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:11:44,508 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:11:44,511 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': 'ad95243c-4c69-4bf1-97cb-065b8672f538', 'optimized_text': 'Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.'}\n",
      "Results updated: \n",
      "{'resp_id': 'ad95243c-4c69-4bf1-97cb-065b8672f538', 'optimized_text': 'Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:11:44,512 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': 'ad95243c-4c69-4bf1-97cb-065b8672f538', 'optimized_text': 'Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidate Text:\n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n",
      "\n",
      "Reference Text:\n",
      "Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored various analytical content including reports, blogs, and presentations, focusing on go-to-market strategies, deal signing, renewal processes, and buyer behavior studies. Conducted in-depth research on technology adoptions such as cloud, AI, ML, and digital trends, enhancing industry understanding and identifying data gaps. Utilized external vendors and advanced tooling to support research efforts. Established regular distribution methods for sharing insights and actionable recommendations, such as newsletters, dashboards, and executive reviews.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editing import edit_text_for_dp, edit_text_for_semantic_entailment\n",
    "\n",
    "\n",
    "text_1 = \"MBA or graduate degree in a management, technical, or engineering field\"\n",
    "text_2 = \"Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling\"\n",
    "text_3 = \"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\"\n",
    "text_4 = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "text_5 = \"Experience with performing market analysis and developing competitive intelligence\"\n",
    "text_6 = \"Ability to manage executive stakeholders and communicate with a highly technical management team\"\n",
    "text_7 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "text_8 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "text_9 = \"English proficiency is a requirement for all roles unless stated otherwise in the job posting\"\n",
    "text_10 = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "text_11 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "text_12 = \"Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\"\n",
    "\n",
    "original_text = \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
    "\n",
    "\n",
    "candidate = original_text\n",
    "reference = text_12\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "api_key = get_openai_api_key()\n",
    "client = OpenAI(api_key=api_key)  # Instantiate openai api chat completion class\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "\n",
    "revised = edit_text_for_semantic_entailment(\n",
    "    client=client,  \n",
    "    candidate_text=candidate, \n",
    "    reference_text=reference,\n",
    "    model_id=gpt4t, \n",
    "    temperature=0.6)\n",
    "\n",
    "revised_text = revised[\"optimized_text\"]\n",
    "\n",
    "print(f\"\\nCandidate Text:\\n{candidate}\")\n",
    "print(f\"\\nReference Text:\\n{reference}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:03,008 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:07,445 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:07,448 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:07,451 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': '5d15e5aa-5394-4ec8-9610-242f1f147be9', 'optimized_text': 'Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.'}\n",
      "Results updated: \n",
      "{'resp_id': '5d15e5aa-5394-4ec8-9610-242f1f147be9', 'optimized_text': 'Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:07,452 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': '5d15e5aa-5394-4ec8-9610-242f1f147be9', 'optimized_text': 'Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Formatted Prompt in edit_text_for_dp:\n",
      "\n",
      "You are a skilled professional at writing resumes. Please perform the following tasks:\n",
      "\n",
      "1. Analyze the **source text** at a high level.\n",
      "\n",
      "2. Apply the \"\"source text's** dependency structure to the **target text**. Ensure that the original meaning of the **target text** is mostly preserved.\n",
      "\n",
      "**Source Text:**\n",
      "\"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
      "\n",
      "**Target Text:**\n",
      "\"Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "\n",
      "**Return Format:**\n",
      "Please return the result in JSON format as follows:\n",
      "\n",
      "{\n",
      "  \"optimized_text\": \"Edited version of the target text\"\n",
      "}\n",
      "\n",
      "Do not include any additional text or explanations.\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:11,194 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:11,196 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:11,199 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': '8e0249d3-37f4-4818-aa8d-97751838b6f3', 'optimized_text': 'Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.'}\n",
      "Results updated: \n",
      "{'resp_id': '8e0249d3-37f4-4818-aa8d-97751838b6f3', 'optimized_text': 'Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:27:11,200 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': '8e0249d3-37f4-4818-aa8d-97751838b6f3', 'optimized_text': 'Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidate Text:\n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n",
      "\n",
      "Reference Text:\n",
      "Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored reports, blogs, presentations, and custom research focusing on go-to-market strategy, deal signing analysis, renewal analysis, and buyer studies. Additionally, investigated technology adoptions (such as cloud, AI, ML, digital technologies) and industry trends to identify data gaps and suggest actionable insights. Engaged third-party vendors and utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis. Investigated data gaps, suggested actionable insights, engaged third-party vendors, utilized advanced tooling to augment research, and established regular dissemination channels such as newsletters, dashboards, and executive reviews to ensure effective communication and implementation of findings.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editing import edit_text_for_dp, edit_text_for_semantic_entailment\n",
    "\n",
    "# Texts\n",
    "text_1 = \"MBA or graduate degree in a management, technical, or engineering field\"\n",
    "text_2 = \"Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling\"\n",
    "text_3 = \"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\"\n",
    "text_4 = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "text_5 = \"Experience with performing market analysis and developing competitive intelligence\"\n",
    "text_6 = \"Ability to manage executive stakeholders and communicate with a highly technical management team\"\n",
    "text_7 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "text_8 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "text_9 = \"English proficiency is a requirement for all roles unless stated otherwise in the job posting\"\n",
    "text_10 = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "text_11 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "text_12 = \"Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\"\n",
    "\n",
    "original_text = \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "api_key = get_openai_api_key()\n",
    "client = OpenAI(api_key=api_key)  # Instantiate openai api chat completion class\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "\n",
    "\n",
    "# Step 1: Align Semantic & Entailment\n",
    "candidate = original_text\n",
    "reference = text_12\n",
    "\n",
    "revised = edit_text_for_semantic_entailment(\n",
    "    client=client,  \n",
    "    candidate_text=candidate, \n",
    "    reference_text=reference,\n",
    "    model_id=gpt4t, \n",
    "    temperature=0.6)\n",
    "\n",
    "revised_text = revised[\"optimized_text\"]\n",
    "\n",
    "# Step 2: Align Original Sentence's DP\n",
    "target = revised_text\n",
    "source = original_text\n",
    "\n",
    "final = edit_text_for_dp(client=client,\n",
    "                         target_text=target, \n",
    "                         source_text=source, \n",
    "                         model_id=gpt4t, \n",
    "                         temperature=0.8)\n",
    "\n",
    "final_text = final['optimized_text']\n",
    "\n",
    "\n",
    "# Print\n",
    "print(f\"\\nCandidate Text:\\n{candidate}\")\n",
    "print(f\"\\nReference Text:\\n{reference}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{final_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Dependency Parsing Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:02,631 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:05,966 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:05,968 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:05,971 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': '4e77b7d4-2e89-418d-9bd1-725b2fb51f3f', 'optimized_text': 'Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.'}\n",
      "Results updated: \n",
      "{'resp_id': '4e77b7d4-2e89-418d-9bd1-725b2fb51f3f', 'optimized_text': 'Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:05,972 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': '4e77b7d4-2e89-418d-9bd1-725b2fb51f3f', 'optimized_text': 'Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Formatted Prompt in edit_text_for_dp:\n",
      "\n",
      "You are a skilled professional at writing resumes. Please perform the following tasks:\n",
      "\n",
      "1. Analyze the **source text** at a high level.\n",
      "\n",
      "2. Apply the \"\"source text's** dependency structure to the **target text**. Ensure that the original meaning of the **target text** is mostly preserved.\n",
      "\n",
      "**Source Text:**\n",
      "\"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
      "\n",
      "**Target Text:**\n",
      "\"Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.\"\n",
      "\n",
      "**Return Format:**\n",
      "Please return the result in JSON format as follows:\n",
      "\n",
      "{\n",
      "  \"optimized_text\": \"Edited version of the target text\"\n",
      "}\n",
      "\n",
      "Do not include any additional text or explanations.\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:08,998 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:09,000 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:09,002 - evaluation_optimization.resume_editing - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editing:Results updated: \n",
      "{'resp_id': '5bce9c3d-ff64-44b6-9ddd-52c2a8769499', 'optimized_text': 'Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.'}\n",
      "Results updated: \n",
      "{'resp_id': '5bce9c3d-ff64-44b6-9ddd-52c2a8769499', 'optimized_text': 'Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 22:03:09,003 - evaluation_optimization.resume_editing - INFO - Results updated: \n",
      "{'resp_id': '5bce9c3d-ff64-44b6-9ddd-52c2a8769499', 'optimized_text': 'Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidate Text:\n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n",
      "\n",
      "Reference Text:\n",
      "Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored comprehensive reports, blogs, and presentations, and conducted custom research on go-to-market strategies, deal signing, and renewal analysis. Performed buyer studies and analyzed technology adoption trends (including cloud, AI, ML, and digital transformations) across various industries. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, which supported the creation of actionable insights distributed through newsletters, dashboards, and executive reviews.\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored comprehensive reports, blogs, presentations, & custom researches in go-to-market strategies, deal signing analysis, renewal analysis, buyer studies, and technology adoptions (cloud, AI, ML, digital transformations), and industry trend analysis. This work involved identifying data gaps and utilizing external vendors and tools to enhance research, aiding in the creation of actionable insights that were distributed across newsletters, dashboards, and executive reviews.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editing import edit_text_for_semantic_entailment_llama3\n",
    "\n",
    "# Texts\n",
    "text_1 = \"MBA or graduate degree in a management, technical, or engineering field\"\n",
    "text_2 = \"Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling\"\n",
    "text_3 = \"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\"\n",
    "text_4 = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "text_5 = \"Experience with performing market analysis and developing competitive intelligence\"\n",
    "text_6 = \"Ability to manage executive stakeholders and communicate with a highly technical management team\"\n",
    "text_7 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "text_8 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "text_9 = \"English proficiency is a requirement for all roles unless stated otherwise in the job posting\"\n",
    "text_10 = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "text_11 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "text_12 = \"Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\"\n",
    "\n",
    "original_text = \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "api_key = get_openai_api_key()\n",
    "client = OpenAI(api_key=api_key)  # Instantiate openai api chat completion class\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "\n",
    "\n",
    "# Step 1: Align Semantic & Entailment\n",
    "candidate = original_text\n",
    "reference = text_12\n",
    "\n",
    "revised = edit_text_for_semantic_entailment(\n",
    "    client=client,  \n",
    "    candidate_text=candidate, \n",
    "    reference_text=reference,\n",
    "    model_id=gpt4t, \n",
    "    temperature=0.6)\n",
    "\n",
    "revised_text = revised[\"optimized_text\"]\n",
    "\n",
    "# Step 2: Align Original Sentence's DP\n",
    "target = revised_text\n",
    "source = original_text\n",
    "\n",
    "final = edit_text_for_dp(client=client,\n",
    "                         target_text=target, \n",
    "                         source_text=source, \n",
    "                         model_id=gpt4t, \n",
    "                         temperature=0.8)\n",
    "\n",
    "final_text = final['optimized_text']\n",
    "\n",
    "\n",
    "# Print\n",
    "print(f\"\\nCandidate Text:\\n{candidate}\")\n",
    "print(f\"\\nReference Text:\\n{reference}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{final_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:30,270 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:33,487 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:33,490 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:33,492 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': '035476c0-63f1-4357-86c8-200f9aec746d', 'optimized_text': 'Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.'}\n",
      "Results updated: \n",
      "{'text_id': '035476c0-63f1-4357-86c8-200f9aec746d', 'optimized_text': 'Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:33,495 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': '035476c0-63f1-4357-86c8-200f9aec746d', 'optimized_text': 'Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:36,996 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:36,998 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:37,002 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': '70fa76ac-430b-4cd2-bfde-5ebb770d900f', 'optimized_text': 'Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.'}\n",
      "Results updated: \n",
      "{'text_id': '70fa76ac-430b-4cd2-bfde-5ebb770d900f', 'optimized_text': 'Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:37,003 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': '70fa76ac-430b-4cd2-bfde-5ebb770d900f', 'optimized_text': 'Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:40,340 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:40,343 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:40,346 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': 'a4ef3843-19d5-4667-9869-4d5c3850a206', 'optimized_text': 'Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.'}\n",
      "Results updated: \n",
      "{'text_id': 'a4ef3843-19d5-4667-9869-4d5c3850a206', 'optimized_text': 'Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:53:40,347 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': 'a4ef3843-19d5-4667-9869-4d5c3850a206', 'optimized_text': 'Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text:\n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n",
      "\n",
      "Compared to Text:\n",
      "Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n",
      "\n",
      "Optimized Text Result: \n",
      "Conducted original research and authored reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis. This includes buyer studies and technology adoption trends (cloud, AI, ML, digital, etc.), ensuring continuous updates and actionable insights through various distribution methods such as newsletters and dashboards.\n",
      "\n",
      "Optimized Text Result: \n",
      "Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored detailed reports, blogs, and presentations on go-to-market strategies, deal signing, and renewal analysis, focusing on buyer studies and technology adoption trends such as cloud, AI, ML, and digital innovations. Identified gaps in existing data and engaged in original research to fill these gaps, often incorporating third-party vendors and advanced tooling. Ensured the provision of continuous updates and actionable insights, establishing regular cadences for research distribution through various methods including newsletters, dashboards, and executive reviews.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editor import TextEditor\n",
    "\n",
    "# Texts\n",
    "text_1 = \"MBA or graduate degree in a management, technical, or engineering field\"\n",
    "text_2 = \"Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling\"\n",
    "text_3 = \"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\"\n",
    "text_4 = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "text_5 = \"Experience with performing market analysis and developing competitive intelligence\"\n",
    "text_6 = \"Ability to manage executive stakeholders and communicate with a highly technical management team\"\n",
    "text_7 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "text_8 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "text_9 = \"English proficiency is a requirement for all roles unless stated otherwise in the job posting\"\n",
    "text_10 = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "text_11 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "text_12 = \"Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\"\n",
    "\n",
    "original = \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
    "requirement = text_12\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "\n",
    "# Instantiate TextEditor class\n",
    "text_editor = TextEditor(model=\"openai\", model_id=gpt4, max_tokens=512)\n",
    "\n",
    "\n",
    "# Step 1: Align Semantic \n",
    "candidate = original\n",
    "reference = requirement\n",
    "\n",
    "revised = text_editor.edit_for_semantics(\n",
    "    candidate_text=candidate, \n",
    "    reference_text=reference,\n",
    "    temperature=0.4)\n",
    "\n",
    "revised_text_1 = revised[\"optimized_text\"]\n",
    "\n",
    "# Step 2: Allign Entailment\n",
    "premise = revised_text_1\n",
    "hypothesis = requirement\n",
    "\n",
    "revised = text_editor.edit_for_entailment(\n",
    "                         premise_text=premise, \n",
    "                         hypothesis_text=hypothesis,\n",
    "                         temperature=0.6)\n",
    "\n",
    "revised_text_2 = revised['optimized_text']\n",
    "\n",
    "# Step 3: Align Original Sentence's DP\n",
    "target = revised_text_2\n",
    "source = original\n",
    "\n",
    "revised = text_editor.edit_for_dp(\n",
    "                         target_text=target, \n",
    "                         source_text=source, \n",
    "                         temperature=0.8)\n",
    "\n",
    "revised_text_3 = revised['optimized_text']\n",
    "\n",
    "\n",
    "# Print\n",
    "print(f\"\\nOriginal Text:\\n{original}\")\n",
    "print(f\"\\nCompared to Text:\\n{requirement}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_1}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_2}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:27,773 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:29,101 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:29,104 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:29,105 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': '0d79593a-af95-4a68-99f6-2d309ae3cc2f', 'optimized_text': 'Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.'}\n",
      "Results updated: \n",
      "{'text_id': '0d79593a-af95-4a68-99f6-2d309ae3cc2f', 'optimized_text': 'Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:29,106 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': '0d79593a-af95-4a68-99f6-2d309ae3cc2f', 'optimized_text': 'Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:30,206 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:30,209 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:30,212 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': '5c78fdb4-854a-40d2-bc17-f62213610763', 'optimized_text': 'Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.'}\n",
      "Results updated: \n",
      "{'text_id': '5c78fdb4-854a-40d2-bc17-f62213610763', 'optimized_text': 'Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:30,213 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': '5c78fdb4-854a-40d2-bc17-f62213610763', 'optimized_text': 'Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:31,466 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.llm_data_utils:Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\"\n",
      "}\n",
      "Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:31,469 - utils.llm_data_utils - INFO - Raw LLM Response: {\n",
      "  \"optimized_text\": \"Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:31,471 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': 'ebaba7f3-a403-4566-8ac7-f1f4dc4e31c6', 'optimized_text': 'Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.'}\n",
      "Results updated: \n",
      "{'text_id': 'ebaba7f3-a403-4566-8ac7-f1f4dc4e31c6', 'optimized_text': 'Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:00:31,473 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': 'ebaba7f3-a403-4566-8ac7-f1f4dc4e31c6', 'optimized_text': 'Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text:\n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n",
      "\n",
      "Compared to Text:\n",
      "Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n",
      "\n",
      "Optimized Text Result: \n",
      "Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), and industry trends.\n",
      "\n",
      "Optimized Text Result: \n",
      "Produced reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\n",
      "\n",
      "Optimized Text Result: \n",
      "Authored reports, articles, and presentations on go-to-market strategy, deal analysis, renewal analysis, buyer behavior, technology adoption (cloud, AI, ML, digital, etc.), industry trends, and original research to fill gaps in existing data.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editor import TextEditor\n",
    "\n",
    "# Texts\n",
    "text_1 = \"MBA or graduate degree in a management, technical, or engineering field\"\n",
    "text_2 = \"Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling\"\n",
    "text_3 = \"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\"\n",
    "text_4 = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "text_5 = \"Experience with performing market analysis and developing competitive intelligence\"\n",
    "text_6 = \"Ability to manage executive stakeholders and communicate with a highly technical management team\"\n",
    "text_7 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "text_8 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "text_9 = \"English proficiency is a requirement for all roles unless stated otherwise in the job posting\"\n",
    "text_10 = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "text_11 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "text_12 = \"Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\"\n",
    "\n",
    "original = \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
    "requirement = text_12\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "\n",
    "# Instantiate TextEditor class\n",
    "text_editor = TextEditor(model=\"openai\", model_id=gpt3, max_tokens=512)\n",
    "\n",
    "\n",
    "# Step 1: Align Semantic \n",
    "candidate = original\n",
    "reference = requirement\n",
    "\n",
    "revised = text_editor.edit_for_semantics(\n",
    "    candidate_text=candidate, \n",
    "    reference_text=reference,\n",
    "    temperature=0.5)\n",
    "\n",
    "revised_text_1 = revised[\"optimized_text\"]\n",
    "\n",
    "# Step 2: Allign Entailment\n",
    "premise = revised_text_1\n",
    "hypothesis = requirement\n",
    "\n",
    "revised = text_editor.edit_for_entailment(\n",
    "                         premise_text=premise, \n",
    "                         hypothesis_text=hypothesis,\n",
    "                         temperature=0.6)\n",
    "\n",
    "revised_text_2 = revised['optimized_text']\n",
    "\n",
    "# Step 3: Align Original Sentence's DP\n",
    "target = revised_text_2\n",
    "source = original\n",
    "\n",
    "revised = text_editor.edit_for_dp(\n",
    "                         target_text=target, \n",
    "                         source_text=source, \n",
    "                         temperature=0.9)\n",
    "\n",
    "revised_text_3 = revised['optimized_text']\n",
    "\n",
    "\n",
    "# Print\n",
    "print(f\"\\nOriginal Text:\\n{original}\")\n",
    "print(f\"\\nCompared to Text:\\n{requirement}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_1}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_2}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:OpenAI API key successfully loaded.\n",
      "OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:00,148 - root - INFO - OpenAI API key successfully loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:19,460 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:19,462 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': 'cfeae29e-475e-42d0-92ca-8817b5869f3f', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), and industry trends. Developed actionable recommendations through reports, blogs, presentations, and custom studies.'}\n",
      "Results updated: \n",
      "{'text_id': 'cfeae29e-475e-42d0-92ca-8817b5869f3f', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), and industry trends. Developed actionable recommendations through reports, blogs, presentations, and custom studies.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:19,463 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': 'cfeae29e-475e-42d0-92ca-8817b5869f3f', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), and industry trends. Developed actionable recommendations through reports, blogs, presentations, and custom studies.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:35,382 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:35,383 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': '50978914-33be-4817-98c4-c06fae43c0eb', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies.'}\n",
      "Results updated: \n",
      "{'text_id': '50978914-33be-4817-98c4-c06fae43c0eb', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:35,385 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': '50978914-33be-4817-98c4-c06fae43c0eb', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:59,118 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:JSON schema validation passed.\n",
      "JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:59,120 - evaluation_optimization.resume_editor - INFO - JSON schema validation passed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:evaluation_optimization.resume_editor:Results updated: \n",
      "{'text_id': '0cd881f0-9b4d-4925-ac27-fdf9cd19550a', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies, authored in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.'}\n",
      "Results updated: \n",
      "{'text_id': '0cd881f0-9b4d-4925-ac27-fdf9cd19550a', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies, authored in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 16:21:59,121 - evaluation_optimization.resume_editor - INFO - Results updated: \n",
      "{'text_id': '0cd881f0-9b4d-4925-ac27-fdf9cd19550a', 'optimized_text': 'Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies, authored in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text:\n",
      "Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n",
      "\n",
      "Compared to Text:\n",
      "Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n",
      "\n",
      "Optimized Text Result: \n",
      "Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), and industry trends. Developed actionable recommendations through reports, blogs, presentations, and custom studies.\n",
      "\n",
      "Optimized Text Result: \n",
      "Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies.\n",
      "\n",
      "Optimized Text Result: \n",
      "Conducted original research and analysis on go-to-market strategies, deal signing, renewal rates, buyer behavior, technology adoption (cloud, AI, ML, digital), industry trends, and identified gaps to inform actionable recommendations through reports, blogs, presentations, and custom studies, authored in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from utils.llm_data_utils import get_openai_api_key\n",
    "from evaluation_optimization.resume_editor import TextEditor\n",
    "\n",
    "# Texts\n",
    "text_1 = \"MBA or graduate degree in a management, technical, or engineering field\"\n",
    "text_2 = \"Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling\"\n",
    "text_3 = \"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\"\n",
    "text_4 = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "text_5 = \"Experience with performing market analysis and developing competitive intelligence\"\n",
    "text_6 = \"Ability to manage executive stakeholders and communicate with a highly technical management team\"\n",
    "text_7 = \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\"\n",
    "text_8 = \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"\n",
    "text_9 = \"English proficiency is a requirement for all roles unless stated otherwise in the job posting\"\n",
    "text_10 = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "text_11 = \"Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps\"\n",
    "text_12 = \"Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\"\n",
    "\n",
    "original = \"Authored reports, blogs, presentations, & custom researches in go-to-market strategy, deal signing analysis, renewal analysis, buyer studies, technology adoptions (cloud, AI, ML, digital, etc.), and industry trend analysis.\"\n",
    "requirement = text_12\n",
    "\n",
    "# Instantiate API object (do this outside the function to reduce overhead)\n",
    "gpt3 = \"gpt-3.5-turbo\"\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "\n",
    "# Instantiate TextEditor class\n",
    "text_editor = TextEditor(model=\"llama3\")\n",
    "\n",
    "\n",
    "# Step 1: Align Semantic \n",
    "candidate = original\n",
    "reference = requirement\n",
    "\n",
    "revised = text_editor.edit_for_semantics(\n",
    "    candidate_text=candidate, \n",
    "    reference_text=reference,\n",
    "    temperature=0.8)\n",
    "\n",
    "revised_text_1 = revised[\"optimized_text\"]\n",
    "\n",
    "# Step 2: Allign Entailment\n",
    "premise = revised_text_1\n",
    "hypothesis = requirement\n",
    "\n",
    "revised = text_editor.edit_for_entailment(\n",
    "                         premise_text=premise, \n",
    "                         hypothesis_text=hypothesis,\n",
    "                         temperature=0.6)\n",
    "\n",
    "revised_text_2 = revised['optimized_text']\n",
    "\n",
    "# Step 3: Align Original Sentence's DP\n",
    "target = revised_text_2\n",
    "source = original\n",
    "\n",
    "revised = text_editor.edit_for_dp(\n",
    "                         target_text=target, \n",
    "                         source_text=source, \n",
    "                         temperature=0.9)\n",
    "\n",
    "revised_text_3 = revised['optimized_text']\n",
    "\n",
    "\n",
    "# Print\n",
    "print(f\"\\nOriginal Text:\\n{original}\")\n",
    "print(f\"\\nCompared to Text:\\n{requirement}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_1}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_2}\")\n",
    "print(f\"\\nOptimized Text Result: \\n{revised_text_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup codes (to be delated later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back up just in case\n",
    "\n",
    "def call_llama3(self, prompt, temperature):\n",
    "    # def edit_text_for_semantic_entailment_llama3(\n",
    "    #     candidate_text,\n",
    "    #     reference_text,\n",
    "    #     text_id=None,\n",
    "    #     temperature=0.6,\n",
    "    # ):\n",
    "    \"\"\"\n",
    "\n",
    "    Edits/optimizes a text using Llama3 model based on another text (source text).\n",
    "\n",
    "    Args:\n",
    "        - text_id (int): (Optional) Identifier for the responsibility text.\n",
    "        Default to None (unique ids to be generated with UUID function)\n",
    "        - candidate_text (str): Original text to be transformed by the model\n",
    "        (i.e., the riginal responsibility text to be revised.)\n",
    "        - reference_text (str): Text that the candidate text is being compared to\n",
    "        (i.e., requirement text to optimize against.)\n",
    "        - max_tokens: default set to 1056\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains 'resp_id' and 'optimized_text' after revision.\n",
    "    \"\"\"\n",
    "    # Generate a unique text_id using UUID\n",
    "    if text_id is None:\n",
    "        text_id = str(uuid.uuid4())\n",
    "\n",
    "    # Create the prompt using a predefined template\n",
    "    try:\n",
    "        prompt = SEMANTIC_ALIGNMENT_PROMPT.format(\n",
    "            content_1=candidate_text, content_2=reference_text\n",
    "        )\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"Error formatting STRUCTURE_TRANSFER_PROMPT: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Call api function and get response; deserialize from pydantic obj to dict\n",
    "    response_pyd_obj = call_llama3(\n",
    "        prompt, expected_res_type=\"json\", temperature=temperature\n",
    "    )\n",
    "\n",
    "    # Ensure the response is in the correct format (Pydantic JSONResponse model)\n",
    "    if not isinstance(response_pyd_obj, JSONResponse):\n",
    "        logger.error(\"Received response is not in expected JSONResponse format.\")\n",
    "        raise ValueError(\n",
    "            \"Received response is not in expected JSONResponse format.\"\n",
    "        )\n",
    "\n",
    "    # Deserialize pydantic obj. (expect a dictionary)\n",
    "    response_dict = response_pyd_obj.model_dump()\n",
    "    # print(f\"final text: {response_dict}\")\n",
    "\n",
    "    # Validate the JSON structure using JSON Schema\n",
    "    try:\n",
    "        jsonschema.validate(instance=response_dict, schema=LLM_RES_JSON_SCHEMA)\n",
    "        logger.info(\"JSON schema validation passed.\")\n",
    "    except jsonschema.exceptions.ValidationError as e:\n",
    "        logger.error(f\"JSON schema validation failed: {e}\")\n",
    "        raise ValueError(f\"Invalid JSON format: {e}\")\n",
    "\n",
    "    # Combine w/t id, then return the combined dictionary\n",
    "    result = {\"resp_id\": text_id, **response_dict}  # ** to unpack a dictionary\n",
    "\n",
    "    logger.info(f\"Results updated: \\n{result}\")\n",
    "    return result\n",
    "\n",
    "def validate_response(self, response_dict):\n",
    "    \"\"\"Validate the API response dictionary using JSON Schema.\"\"\"\n",
    "    try:\n",
    "        jsonschema.validate(instance=response_dict, schema=LLM_RES_JSON_SCHEMA)\n",
    "        logger.info(\"JSON schema validation passed.\")\n",
    "    except jsonschema.exceptions.ValidationError as e:\n",
    "        logger.error(f\"JSON schema validation failed: {e}\")\n",
    "        raise ValueError(f\"Invalid JSON format: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_text_for_semantic_entailment(\n",
    "    client,\n",
    "    candidate_text,\n",
    "    reference_text,\n",
    "    text_id=None,\n",
    "    model_id=\"gpt-4-turbo\",\n",
    "    temperature=0.6,\n",
    "    max_tokens=1056,\n",
    "):\n",
    "\n",
    "    # Generate a unique text_id using UUID\n",
    "    if text_id is None:\n",
    "        text_id = str(uuid.uuid4())\n",
    "\n",
    "    # Create the prompt using a predefined template\n",
    "    try:\n",
    "        prompt = SEMANTIC_ALIGNMENT_PROMPT.format(\n",
    "            content_1=candidate_text, content_2=reference_text\n",
    "        )\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"Error formatting STRUCTURE_TRANSFER_PROMPT: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Call call openai api function and get response;\n",
    "    # deserialize from pydantic obj to dict\n",
    "    response_pyd_obj = call_openai_api(\n",
    "        client,\n",
    "        model_id,\n",
    "        prompt,\n",
    "        expected_res_type=\"json\",\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    # Ensure the response is a pyd  (Pydantic JSONResponse model)\n",
    "    if not isinstance(response_pyd_obj, JSONResponse):\n",
    "        logger.error(\"Received response is not in expected JSONResponse format.\")\n",
    "        raise ValueError(\"Received response is not in expected JSONResponse format.\")\n",
    "\n",
    "    # Deserialize pydantic obj. (expect a dictionary)\n",
    "    response_dict = response_pyd_obj.model_dump()\n",
    "    # print(f\"final text: {response_dict}\")\n",
    "\n",
    "    # Validate the JSON structure using JSON Schema\n",
    "    try:\n",
    "        jsonschema.validate(instance=response_dict, schema=LLM_RES_JSON_SCHEMA)\n",
    "        logger.info(\"JSON schema validation passed.\")\n",
    "    except jsonschema.exceptions.ValidationError as e:\n",
    "        logger.error(f\"JSON schema validation failed: {e}\")\n",
    "        raise ValueError(f\"Invalid JSON format: {e}\")\n",
    "\n",
    "    # Combine w/t id, then return the combined dictionary\n",
    "    result = {\"resp_id\": text_id, **response_dict}  # ** to unpack a dictionary\n",
    "\n",
    "    logger.info(f\"Results updated: \\n{result}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def edit_text_for_dp(\n",
    "    client,\n",
    "    target_text,\n",
    "    source_text,\n",
    "    text_id=None,\n",
    "    model_id=\"gpt-4-turbo\",\n",
    "    temperature=0.8,\n",
    "    max_tokens=1056,\n",
    "):\n",
    "    \"\"\"\n",
    "    Re-edit the target text to better align w/t source text's dependency parsing (DP),\n",
    "    leveraging the OpenAI API.\n",
    "\n",
    "    Example:\n",
    "    Re-edit revised responsibility to match with the original responsibility text's DP\n",
    "    to perserve the tone & style.\n",
    "\n",
    "    Args:\n",
    "        - client (OpenAI()): OpenAI API client instance.\n",
    "        - text_id (str): Identifier of the target text (defaulted to None - unique IDs\n",
    "        to be generated by UUID function)\n",
    "        (i.e., the responsibility bullet text.)\n",
    "        - target_text (str): The target text to be transformed (i.e.,\n",
    "        revised responsibility text).\n",
    "        - source_text (str): The source text from whose \"dependency parsing\"\n",
    "        to be modeled after (i.e., original responsibility text from resume).\n",
    "        - model_id (str): OpenAI model to use (default is 'gpt-4').\n",
    "        - temperature (float): defaulted to 0.8 (a higher temperature setting is\n",
    "        needed to give the model more flexibility/creativity).\n",
    "        - max_token: default to 1056\n",
    "\n",
    "        Note:\n",
    "        - resp is short for responsibility\n",
    "        - req is short for (job) requirement\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary in the format of {'resp_id': \"...\", 'optimized_text': \"...\"}.\n",
    "    \"\"\"\n",
    "    # Generate a unique text_id using UUID\n",
    "    if text_id is None:\n",
    "        text_id = str(uuid.uuid4())\n",
    "\n",
    "    # Define the JSON schema and instructions clearly in the prompt\n",
    "    try:\n",
    "        prompt = STRUCTURE_TRANSFER_PROMPT.format(\n",
    "            content_1=source_text, content_2=target_text\n",
    "        )\n",
    "        print(f\"DEBUG - Formatted Prompt in edit_text_for_dp:\\n{prompt}\")\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"Error formatting STRUCTURE_TRANSFER_PROMPT: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Call API function and get response; deserialize from pydantic obj to dict\n",
    "    response_pyd_obj = call_openai_api(\n",
    "        client,\n",
    "        model_id,\n",
    "        prompt,\n",
    "        expected_res_type=\"json\",\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    # Ensure the response is in the correct format (Pydantic JSONResponse model)\n",
    "    if not isinstance(response_pyd_obj, JSONResponse):\n",
    "        logger.error(\"Received response is not in expected JSONResponse format.\")\n",
    "        raise ValueError(\"Received response is not in expected JSONResponse format.\")\n",
    "\n",
    "    # Deserialize pydantic obj. (expect a dictionary)\n",
    "    response_dict = response_pyd_obj.model_dump()\n",
    "\n",
    "    # Validate the JSON structure using JSON Schema\n",
    "    try:\n",
    "        jsonschema.validate(instance=response_dict, schema=LLM_RES_JSON_SCHEMA)\n",
    "        logger.info(\"JSON schema validation passed.\")\n",
    "    except jsonschema.exceptions.ValidationError as e:\n",
    "        logger.error(f\"JSON schema validation failed: {e}\")\n",
    "        raise ValueError(f\"Invalid JSON format: {e}\")\n",
    "\n",
    "    # Combine w/t id, then return the combined dictionary\n",
    "    result = {\"resp_id\": text_id, **response_dict}  # ** to unpack a dictionary\n",
    "\n",
    "    logger.info(f\"Results updated: \\n{result}\")\n",
    "    return result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
