{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 years of experience in management consulting, product management and strategy, or analytics in a technology company.\n",
      "Experience working with and analyzing data, and managing multiple cross-functional programs or projects.\n",
      "Experience with performing market analysis and developing competitive intelligence.\n",
      "Ability to manage executive stakeholders and communicate with a highly technical management team.\n",
      "Ability to form and refine hypotheses, gather supporting data, and make recommendations.\n",
      "Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\n"
     ]
    }
   ],
   "source": [
    "texts = [\"11 years of experience in management consulting, product management and strategy, or analytics in a technology company\", \n",
    "        \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\",\n",
    "        \"Experience with performing market analysis and developing competitive intelligence\",\n",
    "        \"Ability to manage executive stakeholders and communicate with a highly technical management team\",\n",
    "        \"Ability to form and refine hypotheses, gather supporting data, and make recommendations\",\n",
    "        \"Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems\"]\n",
    "\n",
    "texts = \".\\n\".join(text for text in texts)\n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    bert_score_precision  soft_similarity  word_movers_distance  \\\n",
      "0               0.833088         0.294945             15.905974   \n",
      "1               0.833400         0.315879             15.811388   \n",
      "2               0.836929         0.377884             15.874508   \n",
      "3               0.845272         0.264874             15.524175   \n",
      "4               0.831945         0.243683             15.716234   \n",
      "5               0.832256         0.294245             15.524175   \n",
      "6               0.830909         0.337065             15.905974   \n",
      "7               0.838344         0.215926             15.099669   \n",
      "8               0.832782         0.277579             15.524175   \n",
      "9               0.856216         0.354124             15.620499   \n",
      "10              0.827047         0.433190             15.459625   \n",
      "11              0.835676         0.253837             15.748016   \n",
      "12              0.846321         0.363054             15.491933   \n",
      "13              0.852224         0.191506             15.524175   \n",
      "14              0.816647         0.157734             15.620499   \n",
      "15              0.869484         0.262018             15.459625   \n",
      "16              0.823150         0.330459             15.716234   \n",
      "17              0.837198         0.360644             15.524175   \n",
      "18              0.820165         0.344930             15.716234   \n",
      "19              0.822814         0.287575             15.556349   \n",
      "20              0.821459         0.282235             15.620499   \n",
      "21              0.829722         0.210947             15.652476   \n",
      "22              0.842820         0.352504             15.748016   \n",
      "23              0.824793         0.309893             15.684387   \n",
      "24              0.865685         0.395633             15.427249   \n",
      "25              0.828627         0.275297             15.524175   \n",
      "26              0.850405         0.346306             15.132746   \n",
      "\n",
      "    nli_entailment_score  jaccard_similarity  \n",
      "0               0.049976            0.042424  \n",
      "1               0.042624            0.036364  \n",
      "2               0.004728            0.058140  \n",
      "3               0.048242            0.043478  \n",
      "4               0.003041            0.037500  \n",
      "5               0.002506            0.036810  \n",
      "6               0.014546            0.080247  \n",
      "7               0.009681            0.061350  \n",
      "8               0.005850            0.057325  \n",
      "9               0.030971            0.038710  \n",
      "10              0.026829            0.056604  \n",
      "11              0.001266            0.037500  \n",
      "12              0.039242            0.052980  \n",
      "13              0.070042            0.026667  \n",
      "14              0.051734            0.032680  \n",
      "15              0.171467            0.026667  \n",
      "16              0.087002            0.019231  \n",
      "17              0.019296            0.026316  \n",
      "18              0.008592            0.019231  \n",
      "19              0.010191            0.032051  \n",
      "20              0.009039            0.048485  \n",
      "21              0.010838            0.025316  \n",
      "22              0.022875            0.049689  \n",
      "23              0.005869            0.025157  \n",
      "24              0.001645            0.038462  \n",
      "25              0.004120            0.025806  \n",
      "26              0.055380            0.045455  \n",
      "MBA or graduate degree in a management, technical, or engineering field Knowledge of the Machine Learning and Artificial Intelligence market landscape, ideally with a focus on developer tooling 11 years of experience in management consulting, product management and strategy, or analytics in a technology company Experience working with and analyzing data, and managing multiple cross-functional programs or projects Experience with performing market analysis and developing competitive intelligence Ability to manage executive stakeholders and communicate with a highly technical management team Ability to form and refine hypotheses, gather supporting data, and make recommendations Excellent problem solving and analysis skills, including opportunity identification, market segmentation, and framing of complex/ambiguous problems English proficiency is a requirement for all roles unless stated otherwise in the job posting Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google Leverage first party and third party market data to build assets and programs that surface valuable insights to our business stakeholders and help inform product roadmaps Identify gaps in the existing data and engage in original research to fill these gaps, utilizing third party vendors and tooling where appropriate. Create ongoing cadences to enable research distribution and actionable recommendations (e.g. newsletters, dashboards, exec reviews, etc.)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "\n",
    "f_path = r\"C:\\github\\job_bot\\data\\output_sim_matrix.csv\"\n",
    "df = pd.read_csv(f_path)\n",
    "df[\"Similarity Metrices\"] = df['Similarity Metrices'].apply(ast.literal_eval)\n",
    "\n",
    "metrics_df = pd.json_normalize(df['Similarity Metrices'])\n",
    "print(metrics_df)\n",
    "combined_df = pd.concat([df[[\"Responsibility\"]], metrics_df], axis=1)\n",
    "combined_df['Responsibility'] = combined_df['Responsibility'].str.replace('\\n', ' ')\n",
    "\n",
    "    # Ensure full display of DataFrame content\n",
    "pd.set_option(\"display.max_colwidth\", 100)  # Display full column content\n",
    "pd.set_option(\"display.max_rows\", None)  # Display all rows\n",
    "\n",
    "# Export the combined DataFrame to a CSV file\n",
    "# combined_df.to_csv(\"output_sim_matrix_cleaned.csv\", index=False)\n",
    "print(df.Requirements[1])\n",
    "# This will create a CSV file that you can open directly in Excel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\xzhan\\.cache\\huggingface\\hub\\models--roberta-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "nli_model_name = \"roberta-large-mnli\"\n",
    "\n",
    "# Re-download the model and tokenizer\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name, force_download=True)\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name, force_download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2819904685020447\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "premise = \"The candidate should have experience in developing machine learning models using Python.\"\n",
    "\n",
    "hypothesis = \"I have developed machine learning models using Python for data analysis projects.\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "nli_score = textsimilarity.nli_entailment_score(premise, hypothesis)\n",
    "\n",
    "print(nli_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Entailment Score: 0.614780604839325\n"
     ]
    }
   ],
   "source": [
    "premise = \"The company has launched a new machine learning model.\"\n",
    "hypothesis = \"The company is working on AI projects.\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "nli_score = textsimilarity.nli_entailment_score(hypothesis, premise)\n",
    "print(f\"NLI Entailment Score: {nli_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Entailment Score: 0.0010548133868724108\n"
     ]
    }
   ],
   "source": [
    "hypothesis = \"The company has launched a new machine learning model.\"\n",
    "premise = \"The company is working on AI projects.\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "nli_score = textsimilarity.nli_entailment_score(hypothesis, premise)\n",
    "print(f\"NLI Entailment Score: {nli_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Entailment Score: 0.11766134947538376\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "premise = \"She graduated with a degree in computer science and has 5 years of experience in software development.\"\n",
    "hypothesis = \"She is prepared for a software engineering role.\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "nli_score = textsimilarity.deberta_entailment_score(hypothesis, premise)\n",
    "print(f\"NLI Entailment Score: {nli_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment Score: 0.0016027865931391716\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "hypothesis = \"Enhanced data quality and consistency by integrating thorough financial analysis, standardizing methodologies, and conducting in-depth vendor engagements.\"\n",
    "premise = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "deberta_score = textsimilarity.deberta_entailment_score(hypothesis, premise)\n",
    "print(f\"Entailment Score: {deberta_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\job_bot\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment Score: 0.4864349961280823\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "premise = \"Enhanced data quality and consistency by integrating thorough financial analysis, standardizing methodologies, and conducting in-depth vendor engagements.\"\n",
    "hypothesis = \"Experience working with and analyzing data, and managing multiple cross-functional programs or projects\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "deberta_score = textsimilarity.deberta_entailment_score(hypothesis, premise)\n",
    "print(f\"Entailment Score: {deberta_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment Score: 0.025084542110562325\n"
     ]
    }
   ],
   "source": [
    "from matching.text_similarity_finder import AsymmetricTextSimilarity\n",
    "\n",
    "premise = \"Collaborated with the engineering services research team to pioneer the engineering services tracker, authored influential publications on market forecasts, the impact of COVID-19 on services, and trends in M&A within the engineering services industry.\"\n",
    "hypothesis = \"Work across Program Management teams and our partners (engineering, UX, Customer Experience, TPM, Marketing, Developer Relations, etc.) to help shape the future of AI at Google\"\n",
    "\n",
    "textsimilarity = AsymmetricTextSimilarity()\n",
    "deberta_score = textsimilarity.deberta_entailment_score(hypothesis, premise)\n",
    "print(f\"Entailment Score: {deberta_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Categorized Scores:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'bert_score_precision': 'High',\n",
       " 'soft_similarity': 'Medium',\n",
       " 'word_movers_distance': 'High',\n",
       " 'nli_entailment_score': 'Low',\n",
       " 'jaccard_similarity': 'High',\n",
       " 'deberta_entailment_score': 'Medium'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# metrics_evaluator.py\n",
    "\n",
    "# Define the metric criteria in a dictionary\n",
    "METRIC_CRITERIA = {\n",
    "    # \"BERTScore Precision\"\n",
    "    \"bert_score_precision\": {\n",
    "        \"range\": (0, 1),\n",
    "        \"high_threshold\": 0.85,\n",
    "        \"low_threshold\": 0.70,\n",
    "    },\n",
    "    # \"Soft Similarity (SBERT)\"\n",
    "    \"soft_similarity\": {\n",
    "        \"range\": (-1, 1),\n",
    "        \"high_threshold\": 0.7,\n",
    "        \"low_threshold\": 0.4,\n",
    "    },\n",
    "    # \"Word Mover's Distance\"\n",
    "    \"word_movers_distance\": {\n",
    "        \"range\": (0, float(\"inf\")),\n",
    "        \"high_threshold\": 5,  # High score is considered \"Low\" for this metric\n",
    "        \"low_threshold\": 15,  # Low score is considered \"High\" for this metric\n",
    "        \"reverse\": True,  # Indicates smaller scores are better\n",
    "    },\n",
    "    # \"NLI Entailment Score\"\n",
    "    \"nli_entailment_score\": {\n",
    "        \"range\": (0, 1),\n",
    "        \"high_threshold\": 0.7,\n",
    "        \"low_threshold\": 0.3,\n",
    "    },\n",
    "    # \"Jaccard Similarity\"\n",
    "    \"jaccard_similarity\": {\n",
    "        \"range\": (0, 1),\n",
    "        \"high_threshold\": 0.5,\n",
    "        \"low_threshold\": 0.2,\n",
    "    },\n",
    "    # \"DeBERTa Entailment Score\"\n",
    "    \"deberta_entailment_score\": {\n",
    "        \"range\": (0, 1),\n",
    "        \"high_threshold\": 0.8,\n",
    "        \"low_threshold\": 0.4,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def evaluate_score(metric_name, score):\n",
    "    \"\"\"\n",
    "    Evaluate the score for a given metric name.\n",
    "\n",
    "    Args:\n",
    "        metric_name (str): The name of the metric.\n",
    "        score (float): The score to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        str: The category of the score (\"High\", \"Medium\", \"Low\").\n",
    "    \"\"\"\n",
    "    if metric_name not in METRIC_CRITERIA:\n",
    "        raise ValueError(f\"Metric '{metric_name}' is not defined in the criteria.\")\n",
    "\n",
    "    criteria = METRIC_CRITERIA[metric_name]\n",
    "    high_threshold = criteria[\"high_threshold\"]\n",
    "    low_threshold = criteria[\"low_threshold\"]\n",
    "    reverse = criteria.get(\"reverse\", False)\n",
    "\n",
    "    # For metrics where a lower score is better (like Word Mover's Distance)\n",
    "    if reverse:\n",
    "        if score <= high_threshold:\n",
    "            return \"High\"\n",
    "        elif score >= low_threshold:\n",
    "            return \"Low\"\n",
    "    else:\n",
    "        if score >= high_threshold:\n",
    "            return \"High\"\n",
    "        elif score <= low_threshold:\n",
    "            return \"Low\"\n",
    "\n",
    "    return \"Medium\"\n",
    "\n",
    "\n",
    "def categorize_scores(scores):\n",
    "    \"\"\"\n",
    "    Categorize a dictionary of scores based on their metric names.\n",
    "\n",
    "    Args:\n",
    "        scores (dict): A dictionary where keys are metric names and values are scores.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the same keys but with \"High\", \"Medium\", or \"Low\" as values.\n",
    "    \"\"\"\n",
    "    categorized_scores = {}\n",
    "    for metric_name, score in scores.items():\n",
    "        try:\n",
    "            categorized_scores[metric_name] = evaluate_score(metric_name, score)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            categorized_scores[metric_name] = \"Unknown\"\n",
    "    return categorized_scores\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    example_scores = {\n",
    "        \"bert_score_precision\": 0.88,\n",
    "        \"soft_similarity\": 0.45,\n",
    "        \"word_movers_distance\": 4.2,\n",
    "        \"nli_entailment_score\": 0.2,\n",
    "        \"jaccard_similarity\": 0.6,\n",
    "        \"deberta_entailment_score\": 0.75,\n",
    "    }\n",
    "\n",
    "    categorized = categorize_scores(example_scores)\n",
    "    display(\"Categorized Scores:\", categorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_494b4 th {\n",
       "  background-color: lightblue;\n",
       "  color: black;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_494b4 thead th {\n",
       "  background-color: lightblue;\n",
       "  color: black;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_494b4 tbody th {\n",
       "  background-color: lightblue;\n",
       "  color: black;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_494b4_row0_col0, #T_494b4_row0_col1, #T_494b4_row0_col2, #T_494b4_row0_col3 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_494b4_row1_col0, #T_494b4_row1_col1, #T_494b4_row1_col3, #T_494b4_row2_col0, #T_494b4_row2_col1, #T_494b4_row2_col2, #T_494b4_row2_col3, #T_494b4_row3_col0, #T_494b4_row3_col1, #T_494b4_row3_col3, #T_494b4_row4_col0, #T_494b4_row4_col1, #T_494b4_row4_col3, #T_494b4_row5_col0, #T_494b4_row5_col1, #T_494b4_row5_col3, #T_494b4_row6_col0, #T_494b4_row6_col1, #T_494b4_row6_col3, #T_494b4_row7_col0, #T_494b4_row7_col1, #T_494b4_row7_col3 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_494b4_row1_col2, #T_494b4_row5_col2, #T_494b4_row6_col2 {\n",
       "  background-color: #3e51c5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_494b4_row3_col2, #T_494b4_row4_col2 {\n",
       "  background-color: #3d50c3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_494b4_row7_col2 {\n",
       "  background-color: #4055c8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_494b4\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_494b4_level0_col0\" class=\"col_heading level0 col0\" >BERTScore Precision</th>\n",
       "      <th id=\"T_494b4_level0_col1\" class=\"col_heading level0 col1\" >Soft Similarity</th>\n",
       "      <th id=\"T_494b4_level0_col2\" class=\"col_heading level0 col2\" >Word Mover's Distance</th>\n",
       "      <th id=\"T_494b4_level0_col3\" class=\"col_heading level0 col3\" >Deberta Entailment Scoree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_494b4_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_494b4_row0_col0\" class=\"data row0 col0\" >31200.0%</td>\n",
       "      <td id=\"T_494b4_row0_col1\" class=\"data row0 col1\" >31200.0%</td>\n",
       "      <td id=\"T_494b4_row0_col2\" class=\"data row0 col2\" >31200.0%</td>\n",
       "      <td id=\"T_494b4_row0_col3\" class=\"data row0 col3\" >31200.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_494b4_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_494b4_row1_col0\" class=\"data row1 col0\" >83.3%</td>\n",
       "      <td id=\"T_494b4_row1_col1\" class=\"data row1 col1\" >26.1%</td>\n",
       "      <td id=\"T_494b4_row1_col2\" class=\"data row1 col2\" >507.1%</td>\n",
       "      <td id=\"T_494b4_row1_col3\" class=\"data row1 col3\" >7.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_494b4_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_494b4_row2_col0\" class=\"data row2 col0\" >1.7%</td>\n",
       "      <td id=\"T_494b4_row2_col1\" class=\"data row2 col1\" >10.6%</td>\n",
       "      <td id=\"T_494b4_row2_col2\" class=\"data row2 col2\" >89.9%</td>\n",
       "      <td id=\"T_494b4_row2_col3\" class=\"data row2 col3\" >14.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_494b4_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_494b4_row3_col0\" class=\"data row3 col0\" >78.3%</td>\n",
       "      <td id=\"T_494b4_row3_col1\" class=\"data row3 col1\" >3.2%</td>\n",
       "      <td id=\"T_494b4_row3_col2\" class=\"data row3 col2\" >360.6%</td>\n",
       "      <td id=\"T_494b4_row3_col3\" class=\"data row3 col3\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_494b4_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_494b4_row4_col0\" class=\"data row4 col0\" >82.2%</td>\n",
       "      <td id=\"T_494b4_row4_col1\" class=\"data row4 col1\" >19.2%</td>\n",
       "      <td id=\"T_494b4_row4_col2\" class=\"data row4 col2\" >435.9%</td>\n",
       "      <td id=\"T_494b4_row4_col3\" class=\"data row4 col3\" >0.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_494b4_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_494b4_row5_col0\" class=\"data row5 col0\" >83.2%</td>\n",
       "      <td id=\"T_494b4_row5_col1\" class=\"data row5 col1\" >26.1%</td>\n",
       "      <td id=\"T_494b4_row5_col2\" class=\"data row5 col2\" >489.9%</td>\n",
       "      <td id=\"T_494b4_row5_col3\" class=\"data row5 col3\" >2.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_494b4_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_494b4_row6_col0\" class=\"data row6 col0\" >84.2%</td>\n",
       "      <td id=\"T_494b4_row6_col1\" class=\"data row6 col1\" >32.5%</td>\n",
       "      <td id=\"T_494b4_row6_col2\" class=\"data row6 col2\" >556.8%</td>\n",
       "      <td id=\"T_494b4_row6_col3\" class=\"data row6 col3\" >7.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_494b4_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_494b4_row7_col0\" class=\"data row7 col0\" >88.4%</td>\n",
       "      <td id=\"T_494b4_row7_col1\" class=\"data row7 col1\" >55.9%</td>\n",
       "      <td id=\"T_494b4_row7_col2\" class=\"data row7 col2\" >818.5%</td>\n",
       "      <td id=\"T_494b4_row7_col3\" class=\"data row7 col3\" >88.1%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2d32ab02a10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dataframe_styler import DataFrameStyler\n",
    "\n",
    "\n",
    "f_path = r\"C:\\github\\job_bot\\data\\output_seg_by_seg_sim_matrix_v2_excel_version.xlsx\"\n",
    "df = pd.read_excel(f_path)\n",
    "\n",
    "stats_df = df.describe()\n",
    "styled_df = DataFrameStyler(stats_df)\n",
    "styled_df.style_percentage_columns_with_gradient()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
